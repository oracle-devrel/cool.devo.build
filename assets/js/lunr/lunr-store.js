var store = [{
        "title": "Deploying A Multi-Cluster Verrazzano On Oracle Container Engine for Kubernetes (OKE) Part 1",
        "excerpt":" In the previous article, we were introduced to Verrazzano and took it for a quick spin on an Oracle Container Engine for Kubernetes (OKE). As promised, in this article, we’re going to deploy a multi-cluster Verrazzano on OKE. And just to make things a little more interesting, we’ll also do that using different Oracle Cloud Infrastructure (OCI) regions. But first, we’ll make a small digression into WebLogic and Kubernetes to set the stage for how we’ll be handling this in each of the next two tutorials. Key topics covered in this tutorial: An introduction to WebLogic and Kubernetes A discussion about infrastructure Creating Verrazzano clusters For additional information, see: Signing Up for Oracle Cloud Infrastructure Getting started with Terraform Getting started From WebLogic to Kubernetes to Verrazzano To better frame our understanding of what Kubernetes is and how it fits in with Verrazzano, let’s take a step back and apply some simple analogies to its components. A good touchstone for us is the WebLogic space, and we can draw from a lot of familiar concepts to help with our understanding here. WebLogic and Kubernetes analogy In WebLogic, a cluster consists of an Admin Server and a group of Managed Servers. In this set up, the Admin Server handles the administration, deployment, and other less silky but nevertheless important tasks, while Managed Servers are utilized for deploying and running the applications, as well as responding to requests. This allows you to run your applications either across your entire cluster or on specific Managed Servers. NOTE: You could always run your applications on the single Admin Server (in a way that’s somewhat equivalent to taints and tolerations of the master nodes), but it’s not recommended. Under this set up, if your application is deployed to the cluster and a Managed Server in the cluster fails (JVM, host, reboot, etc.), other Managed Servers in the cluster will automatically handle the job. But, what if the Managed Server where your singleton service is running fails? WebLogic has you covered with Automatic Service Migration (ASM). For a more detailed read on ASM, check out the WebLogic ASM guide. Now that we have a better sense of the basic cluster infrastructure, let’s start connecting this back to Kubernetes. What’s the best equivalent in Kubernetes? Essentially, ASM is a bit like a ReplicaSet. Initially, applications on Kubernetes were stateless until the addition of StatefulSets, but now you can also run stateful applications across the entire cluster. Geographically distributed clusters What if, for the purpose of high availability, you needed to run your Kubernetes applications in geographically distributed clusters. You could try your luck with kubefed, although it’s currently still in beta and has admittedly been experiencing some growing pains. Or, you could try deploying the same applications to different clusters, implement a kind of global health check, and then use an intelligent load balancer to switch the traffic from one cluster to another. All these approaches are valid, but still fairly limited, error-prone, and risky. Enter Verrazzano multi-clustering. How did Verrazzano make our lives a whole lot better? It took the concept of Admin and Managed Servers in WebLogic and applied it to Kubernetes clusters: Verrazzano multi-cluster Where you previously had a single Admin Server for WebLogic, you now have a single Admin cluster based on Kubernetes for Verrazzano. And where your applications were deployed on managed servers, your Verrazzano workloads are deployed on managed Kubernetes clusters, possibly closer to your users. Infrastructure Planning In order to achieve this, Verrazzano-managed clusters (i.e., Kubernetes clusters administered and managed by the Verrazzano container platform) need to be able to communicate with the Verrazzano Admin cluster and vice-versa. In WebLogic, the Managed Servers would usually be part of the same network (unless you were using stretch clusters) and this administration would be fairly straightforward. Our ultimate goal though, is to deploy the different Verrazzano clusters in different cloud regions on OCI, so we need to start thinking about our plan for networking and security. NOTE: You can also use Verrazzano to manage clusters deployed in other clouds or on-premises, but the networking and security configurations would vary (VPN/FastConnect etc). Below is a map of OCI regions to help us pick a set of regions: Map of OCI regions We’ll use our newly-minted Singapore region for the Admin cluster and then Mumbai, Tokyo, and Sydney as managed clusters in a star architecture: Verrazzano Clusters spread across OCI Asia Pacific regions Networking Infrastructure Remote Peering with different regions We need the clusters to communicate securely using the OCI Backbone, so this means we need to set up DRGs in each region, attach them to their respective VCNs, and then use remote peering. Since the VCNs and the clusters will eventually be connected, we also need to ensure that their respective IP address ranges (VCN, pod, and service) do not overlap. Creating the Verrazzano clusters We’re going to the use terraform-oci-oke module to create our clusters. We could create them individually by cloning the module 4 times and then changing the region parameters, but there’s now a much simpler way to do it. You’ll be pleased to know that one of the things we improved in the 4.0 release of the module is reusability. We’ll take advantage of this! Create a new Terraform project Create project Let’s create a new Terraform project and define our variables as follows: # Copyright 2017, 2021 Oracle Corporation and/or affiliates. All rights reserved. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl # OCI Provider parameters variable \"api_fingerprint\" { default = \"\" description = \"Fingerprint of the API private key to use with OCI API.\" type = string } variable \"apiprivatekey_path\" { default = \"\" description = \"The path to the OCI API private key.\" type = string } variable \"verrazzano_regions\" { # List of regions: https://docs.cloud.oracle.com/iaas/Content/General/Concepts/regions.htm#ServiceAvailabilityAcrossRegions description = \"A map Verrazzano regions.\" type = map(string) } variable \"tenancy_id\" { description = \"The tenancy id of the OCI Cloud Account in which to create the resources.\" type = string } variable \"user_id\" { description = \"The id of the user that terraform will use to create the resources.\" type = string default = \"\" } # General OCI parameters variable \"compartment_id\" { description = \"The compartment id where to create all resources.\" type = string } variable \"label_prefix\" { default = \"none\" description = \"A string that will be prepended to all resources.\" type = string } Define regions In terraform.tfvars, along with the identity parameters, let’s define our regions: verrazzano_regions = { home = \"your-tenancy-home-region\" #replace with your tenancy's home region admin = \"ap-singapore-1\" syd = \"ap-sydney-1\" mum = \"ap-mumbai-1\" tok = \"ap-tokyo-1\" } Define providers In provider.tf, let’s define the providers for the different regions using aliases: provider \"oci\" { fingerprint = var.api_fingerprint privatekeypath = var.apiprivatekey_path region = var.verrazzano_regions[\"admin\"] tenancyocid = var.tenancyid userocid = var.userid alias = \"admin\" } provider \"oci\" { fingerprint = var.api_fingerprint privatekeypath = var.apiprivatekey_path region = var.verrazzano_regions[\"home\"] tenancyocid = var.tenancyid userocid = var.userid alias = \"home\" } provider \"oci\" { fingerprint = var.api_fingerprint privatekeypath = var.apiprivatekey_path region = var.verrazzano_regions[\"syd\"] tenancyocid = var.tenancyid userocid = var.userid alias = \"syd\" } provider \"oci\" { fingerprint = var.api_fingerprint privatekeypath = var.apiprivatekey_path region = var.verrazzano_regions[\"mum\"] tenancyocid = var.tenancyid userocid = var.userid alias = \"mum\" } provider \"oci\" { fingerprint = var.api_fingerprint privatekeypath = var.apiprivatekey_path region = var.verrazzano_regions[\"tok\"] tenancyocid = var.tenancyid userocid = var.userid alias = \"tok\" } Create clusters In main.tf, we’ll create the different clusters (note that some of the parameters here have the same values, and you could use the default ones, but it’s definitely possible to configure these by regions too): module \"vadmin\" { source = \"oracle-terraform-modules/oke/oci\" version = \"4.0.1\" homeregion = var.verrazzanoregions[\"home\"] region = var.verrazzano_regions[\"admin\"] tenancyid = var.tenancyid # general oci parameters compartmentid = var.compartmentid label_prefix = \"v8o\" # ssh keys sshprivatekeypath = \"~/.ssh/idrsa\" sshpublickeypath = \"~/.ssh/idrsa.pub\" # networking create_drg = true internetgatewayroute_rules = [] natgatewayroute_rules = [ { destination = \"10.1.0.0/16\" destinationtype = \"CIDRBLOCK\" networkentityid = \"drg\" description = \"To Sydney\" }, { destination = \"10.2.0.0/16\" destinationtype = \"CIDRBLOCK\" networkentityid = \"drg\" description = \"To Mumbai\" }, { destination = \"10.3.0.0/16\" destinationtype = \"CIDRBLOCK\" networkentityid = \"drg\" description = \"To Tokyo\" }, ] vcn_cidrs = [\"10.0.0.0/16\"] vcndnslabel = \"admin\" vcn_name = \"admin\" # bastion host createbastionhost = true upgrade_bastion = false # operator host create_operator = true enableoperatorinstance_principal = true upgrade_operator = false # oke cluster options cluster_name = \"admin\" controlplanetype = \"private\" controlplaneallowed_cidrs = [\"0.0.0.0/0\"] kubernetes_version = \"v1.20.11\" pods_cidr = \"10.244.0.0/16\" services_cidr = \"10.96.0.0/16\" # node pools node_pools = { np1 = { shape = \"VM.Standard.E4.Flex\", ocpus = 2, memory = 32, nodepoolsize = 2, bootvolumesize = 150, label = { app = \"frontend\", pool = \"np1\" } } } nodepoolname_prefix = \"np-admin\" # oke load balancers load_balancers = \"both\" preferredloadbalancer = \"public\" publiclballowed_cidrs = [\"0.0.0.0/0\"] publiclballowed_ports = [80, 443] # freeform_tags freeform_tags = { vcn = { verrazzano = \"admin\" } bastion = { access = \"public\", role = \"bastion\", security = \"high\" verrazzano = \"admin\" } operator = { access = \"restricted\", role = \"operator\", security = \"high\" verrazzano = \"admin\" } } providers = { oci = oci.admin oci.home = oci.home } } module \"vsyd\" { source = \"oracle-terraform-modules/oke/oci\" version = \"4.0.1\" homeregion = var.verrazzanoregions[\"home\"] region = var.verrazzano_regions[\"syd\"] tenancyid = var.tenancyid # general oci parameters compartmentid = var.compartmentid label_prefix = \"v8o\" # ssh keys sshprivatekeypath = \"~/.ssh/idrsa\" sshpublickeypath = \"~/.ssh/idrsa.pub\" # networking create_drg = true internetgatewayroute_rules = [] natgatewayroute_rules = [ { destination = \"10.0.0.0/16\" destinationtype = \"CIDRBLOCK\" networkentityid = \"drg\" description = \"To Admin\" } ] vcn_cidrs = [\"10.1.0.0/16\"] vcndnslabel = \"syd\" vcn_name = \"syd\" # bastion host createbastionhost = false upgrade_bastion = false # operator host create_operator = false enableoperatorinstance_principal = true upgrade_operator = false # oke cluster options cluster_name = \"syd\" controlplanetype = \"private\" controlplaneallowed_cidrs = [\"0.0.0.0/0\"] kubernetes_version = \"v1.20.11\" pods_cidr = \"10.245.0.0/16\" services_cidr = \"10.97.0.0/16\" # node pools node_pools = { np1 = { shape = \"VM.Standard.E4.Flex\", ocpus = 2, memory = 32, nodepoolsize = 2, bootvolumesize = 150 } } # oke load balancers load_balancers = \"both\" preferredloadbalancer = \"public\" publiclballowed_cidrs = [\"0.0.0.0/0\"] publiclballowed_ports = [80, 443] # freeform_tags freeform_tags = { vcn = { verrazzano = \"syd\" } bastion = { access = \"public\", role = \"bastion\", security = \"high\" verrazzano = \"syd\" } operator = { access = \"restricted\", role = \"operator\", security = \"high\" verrazzano = \"syd\" } } providers = { oci = oci.syd oci.home = oci.home } } module \"vmum\" { source = \"oracle-terraform-modules/oke/oci\" version = \"4.0.1\" homeregion = var.verrazzanoregions[\"home\"] region = var.verrazzano_regions[\"mum\"] tenancyid = var.tenancyid # general oci parameters compartmentid = var.compartmentid label_prefix = \"v8o\" # ssh keys sshprivatekeypath = \"~/.ssh/idrsa\" sshpublickeypath = \"~/.ssh/idrsa.pub\" # networking create_drg = true internetgatewayroute_rules = [] natgatewayroute_rules = [ { destination = \"10.0.0.0/16\" destinationtype = \"CIDRBLOCK\" networkentityid = \"drg\" description = \"To Admin\" } ] vcn_cidrs = [\"10.2.0.0/16\"] vcndnslabel = \"mum\" vcn_name = \"mum\" # bastion host createbastionhost = false upgrade_bastion = false # operator host create_operator = false enableoperatorinstance_principal = true upgrade_operator = false # oke cluster options cluster_name = \"mum\" controlplanetype = \"private\" controlplaneallowed_cidrs = [\"0.0.0.0/0\"] kubernetes_version = \"v1.20.11\" pods_cidr = \"10.246.0.0/16\" services_cidr = \"10.98.0.0/16\" # node pools node_pools = { np1 = { shape = \"VM.Standard.E4.Flex\", ocpus = 2, memory = 32, nodepoolsize = 2, bootvolumesize = 150 } } # oke load balancers load_balancers = \"both\" preferredloadbalancer = \"public\" publiclballowed_cidrs = [\"0.0.0.0/0\"] publiclballowed_ports = [80, 443] # freeform_tags freeform_tags = { vcn = { verrazzano = \"mum\" } bastion = { access = \"public\", role = \"bastion\", security = \"high\" verrazzano = \"mum\" } operator = { access = \"restricted\", role = \"operator\", security = \"high\" verrazzano = \"mum\" } } providers = { oci = oci.mum oci.home = oci.home } } module \"vtok\" { source = \"oracle-terraform-modules/oke/oci\" version = \"4.0.1\" homeregion = var.verrazzanoregions[\"home\"] region = var.verrazzano_regions[\"tok\"] tenancyid = var.tenancyid # general oci parameters compartmentid = var.compartmentid label_prefix = \"v8o\" # ssh keys sshprivatekeypath = \"~/.ssh/idrsa\" sshpublickeypath = \"~/.ssh/idrsa.pub\" # networking create_drg = true internetgatewayroute_rules = [] natgatewayroute_rules = [ { destination = \"10.0.0.0/16\" destinationtype = \"CIDRBLOCK\" networkentityid = \"drg\" description = \"To Admin\" } ] vcn_cidrs = [\"10.3.0.0/16\"] vcndnslabel = \"tok\" vcn_name = \"tok\" # bastion host createbastionhost = false upgrade_bastion = false # operator host create_operator = false enableoperatorinstance_principal = true upgrade_operator = false # oke cluster options cluster_name = \"tok\" controlplanetype = \"private\" controlplaneallowed_cidrs = [\"0.0.0.0/0\"] kubernetes_version = \"v1.20.11\" pods_cidr = \"10.247.0.0/16\" services_cidr = \"10.99.0.0/16\" # node pools node_pools = { np1 = { shape = \"VM.Standard.E4.Flex\", ocpus = 2, memory = 32, nodepoolsize = 2, bootvolumesize = 150 } } # oke load balancers load_balancers = \"both\" preferredloadbalancer = \"public\" publiclballowed_cidrs = [\"0.0.0.0/0\"] publiclballowed_ports = [80, 443] # freeform_tags freeform_tags = { vcn = { verrazzano = \"tok\" } bastion = { access = \"public\", role = \"bastion\", security = \"high\" verrazzano = \"tok\" } operator = { access = \"restricted\", role = \"operator\", security = \"high\" verrazzano = \"tok\" } } providers = { oci = oci.tok oci.home = oci.home } } Display operators For convenience, let’s display the operator host in each region: output \"sshtoadmin_operator\" { description = \"convenient command to ssh to the Admin operator host\" value = module.vadmin.sshtooperator } output \"sshtoau_operator\" { description = \"convenient command to ssh to the Sydney operator host\" value = module.vsyd.sshtooperator } output \"sshtoin_operator\" { description = \"convenient command to ssh to the Mumbai operator host\" value = module.vmum.sshtooperator } output \"sshtojp_operator\" { description = \"convenient command to ssh to the Tokyo operator host\" value = module.vtok.sshtooperator } Create OKE clusters Run terraform init and then terraform plan. The output of plan should indicate the following: Plan: 292 to add, 0 to change, 0 to destroy.Changes to Outputs: + sshtoadmin_operator = (known after apply) + sshtoauoperator = \"ssh -i ~/.ssh/idrsa -J opc@ opc@\" + sshtoinoperator = \"ssh -i ~/.ssh/idrsa -J opc@ opc@\" + sshtojpoperator = \"ssh -i ~/.ssh/idrsa -J opc@ opc@\" Run terraform apply and then terraform relax. Shortly after, you should see the following: Simultaneous creation of 4 OKE clusters in different regions This means that our four OKE Clusters are being simultaneously created in 4 different OCI regions. In about 15 minutes, you’ll have all four clusters created: NOTE: The ssh commands to the various operator hosts will also be printed. Establish connections Create remote peering connections Navigate to the DRGs in each managed cluster’s region (Mumbai, Tokyo, and Sydney). Select Remote Peering Attachment and create a Remote Peering Connection (call it rpctoadmin). In the Admin region (Singapore in our selected region), create 3 Remote Peering Connections: 3 RPCs in the Admin region Now, we need to peer them. Peer the connections Select rpctosyd. Open a new tab in your browser and access the OCI Console. Change the region to Sydney. Navigate to the DRG and then to rpctosyd page. Copy the RPC’s OCID (not the DRG), switch to the Admin tab, and then select Establish Connection: Establishing RPC Establish connection Once you’ve provided the RPC ID and the region as above, select Establish Connection to perform the peering. Repeat the same procedure for the Tokyo and Mumbai regions until all the managed cluster regions are peered with the Admin region. When the peering is performed and completed, you will see its status will change to “Pending” and eventually “Peered”: RPCs in Pending state RPCs in Peered state Configure At this point, our VCNs are peered but there are three more things we need to do: Configure routing tables so that the Verrazzano managed clusters can communicate to the Admin cluster and vice-versa Configure NSGs for the control plane CIDRs to accept requests from Admin VCN Merge the kubeconfigs In the first step, we’re asked to configure the routing table. Previously, you would have had to manually configure rules, but now, they’re automagically done for you! “How,” you ask? Well, one of the features we’ve added is the ability to configure and update routing tables. Let’s explore this in a little more detail. In your main.tf, take a look in the the Admin cluster module. Usually, the natgatewayroute_rules parameter is an empty list: natgatewayroute_rules = [] However, in our Admin module definition, notice that we had already changed this to: natgatewayroute_rules = [ { destination = \"10.1.0.0/16\" destinationtype = \"CIDRBLOCK\" networkentityid = \"drg\" description = \"To Sydney\" }, { destination = \"10.2.0.0/16\" destinationtype = \"CIDRBLOCK\" networkentityid = \"drg\" description = \"To Mumbai\" }, { destination = \"10.3.0.0/16\" destinationtype = \"CIDRBLOCK\" networkentityid = \"drg\" description = \"To Tokyo\" }, ] Similarly, in the managed cluster definitions, we had also set the routing rules to reach the Admin cluster in Singapore: natgatewayroute_rules = [ { destination = \"10.0.0.0/16\" destinationtype = \"CIDRBLOCK\" networkentityid = \"drg\" description = \"To Admin\" } ] NOTE: You can always update these rules later. Example - updating routing rules: Let’s say you add another managed region in Hyderabad (VCN CIDR: 10.4.0.0). There’s a couple of things you’ll need to do to have this region recognized. Add new rule In the routing rules for Admin, you’ll need to add an additional entry to route traffic to this new region: natgatewayroute_rules = [ { destination = \"10.4.0.0/16\" destinationtype = \"CIDRBLOCK\" networkentityid = \"drg\" description = \"To Hyderabad\" } ] Update After updating the custom rules, run terraform apply again and the routing rules in the Admin region will be updated. Check rules Navigate to the Network Visualizer page to check your connectivity and routing rules: Network connectivity across regions TCP requests Next, in each region-managed VCN’s control plane NSG, add an ingress to accept TCP requests from source CIDR 10.0.0.0/16 (Admin) and destination port 6443. This is so that the Admin cluster can be able to communicate with the Managed Cluster’s control plane. Additional ingress security rule in each managed cluster's control plane NSG Operational Convenience For convenience, we’d like to be able to execute most of our operations from the Admin operator host. To do this, we first need to obtain the kubeconfig of each cluster and then merge them together on the Admin operator. At the moment, this step isn’t quite so convenient itself, since you’ll have to perform it manually, but we’re working to improve this in the future. Obtain kubeconfigs Navigate to each managed cluster’s page and select Access cluster. Copy the second command which allows you get the kubeconfig for that cluster: oci ce cluster create-kubeconfig --cluster-id ocid1.cluster.... --file $HOME/.kube/configsyd --region ap-sydney-1 --token-version 2.0.0 --kube-endpoint PRIVATE_ENDPOINT oci ce cluster create-kubeconfig --cluster-id ocid1.cluster.... --file $HOME/.kube/configmum --region ap-mumbai-1 --token-version 2.0.0 --kube-endpoint PRIVATE_ENDPOINT oci ce cluster create-kubeconfig --cluster-id ocid1.cluster.... --file $HOME/.kube/configtok --region ap-tokyo-1 --token-version 2.0.0 --kube-endpoint PRIVATE_ENDPOINT NOTE: You also have to rename the file so it won’t overwrite the existing config for the Admin region. In our example above, that would be configsyd, configmum, and configtok. Run the commands to get the managed cluster’s respective kubeconfigs. You should have four kubeconfigs: $ ls -al .kube total 16 drwxrwxr-x. 2 opc opc 71 Nov 10 11:40 . drwx------. 4 opc opc 159 Nov 10 11:15 .. -rw--w----. 1 opc opc 2398 Nov 10 11:15 config -rw-rw-r--. 1 opc opc 2364 Nov 10 11:40 configmum -rw-rw-r--. 1 opc opc 2364 Nov 10 11:40 configsyd -rw-rw-r--. 1 opc opc 2362 Nov 10 11:40 configtok We can check access to the clusters from the Admin operator host: cd .kubefor cluster in config configsyd configmum configtok; do KUBECONFIG=$CLUSTER kubectl get nodes done This will return us the list of nodes in each cluster: List of nodes in each cluster Rename cluster content Again for convenience, another thing we’ll want to do is rename each cluster’s context. That way, we’ll know which region we’re dealing with. In this exercise, we want 1 context to equate to a Verrazzano cluster. Let’s rename all the kubeconfig files first: config -&gt; admin configmum -&gt; mumbai configsyd -&gt; sydney configtok -&gt; tokyo Now, let’s rename their respective contexts: for cluster in admin sydney mumbai tokyo; do current=$(KUBECONFIG=$cluster kubectl config current-context) KUBECONFIG=$cluster kubectl config rename-context $current $cluster done Merge We’re now ready to merge! To merge the files, run: KUBECONFIG=./admin:./sydney:./mumbai:./tokyo kubectl config view --flatten &gt; ./config List contexts To get a list of the contexts, run: kubectl config get-contexts This will return the following: CURRENT NAME CLUSTER AUTHINFO NAMESPACE * admin cluster-cillzxw34tq user-cillzxw34tqmumbai mumbai cluster-cuvo2ifxe2a user-cuvo2ifxe2asydney sydney cluster-cmgb37morjq user-cmgb37morjqtokyo tokyo cluster-coxskjynjra user-coxskjynjra Now, this is all rather verbose. Fortunately, there’s a way to get nice, succinct output through kubectx. But generating clean output isn’t all this tool can do. It can also be used to rename the contexts as well. In the next section, we’ll explore this alternative a little further. Install and run kubectx First, let’s install kubectx: wget https://github.com/ahmetb/kubectx/releases/download/v0.9.4/kubectx chmod +x kubectx sudo mv kubectx /usr/local/bin Now, run kubectx: Using kubectx The current context (i.e., the current Verrazzano cluster), is highlighted in yellow. We can also easily change contexts in order to perform Verrazzano installations and other operations. For example: Changing context to Sydney What’s next We made it through a lot of material in this article. Let’s review everything we covered: Learned how to set up OKE, networking connectivity, and routing. Discovered operational convenience to run multi-cluster Verrazzano in different regions. Got started with kubectx With this, I would like to thank my colleague and friend Shaun Levey for his ever perceptive insights into the intricacies of OCI Networking. Our exploration continues in Part 2, where we’ll take a look at how to install Verrazzano in a multi-cluster configuration. To explore more information about development with Oracle products: Oracle Developers Portal Oracle Cloud Infrastructure By Ali Mukadam","categories": ["cloudapps","opensource"],
        "tags": ["open-source","oke","kubernetes","verrazzano","terraform","devops"],
        "url": "/tutorials/multi-cluster-verrazzano-oke/1-deploy-multi-cluster-verrazzano-oke",
        "teaser": ""
      },{
        "title": "Why Infrastructure as Code?",
        "excerpt":" Terraform 101 Terraform. Infrastructure-as-Code (IaC). Automation. DevOps. DevSecOps. So many buzz words floating around. What’s the big deal anyway? And which one is right for me? There are lots of different ways to manage IT resources and cloud infrastructure. But what makes Terraform so great? And why should you care about IaC? IaC might not be for everyone, but it’s for almost everyone. This tutorial will cover many of the common ways to manage IT resources, explore the upsides and downsides of each, and get you a little more familiar with IaC. For additional information, see: Signing Up for Oracle Cloud Infrastructure About the User Interfaces of the Oracle Integration Oracle Cloud Infrastructure (OCI) Console Working with the Command Line Interface (CLI) IaC tools So many Ways to manage Let’s start with the basics. When there are IT resources to manage, what are some of the common options? GUI (aka Oracle Cloud Infrastructure (OCI) Console) Command Line Interface (CLI) API IaC tools GUIs In many applications, most of us start with the GUI. If you work within the Oracle Cloud Infrastructure (OCI), you’re probably already familiar with the OCI Console. Overall, GUIs are typically pretty fancy, fun, and easy to use. But while they can be entertaining and great for initially learning a system or platform, they’re not always the most scalable or efficient. Also, GUIs require time and manual user intervention, unless you’re using something to automate the pointing-and-clicking, such as Selenium (a fairly niche edge case). Things only “work” when someone’s there to click a button. It’s difficult (read: impossible) to rapidly manage resources using a GUI. Just the time it takes to point, click, and wait for the browser to update isn’t really ideal when you’re trying to handle a critical situation. On occasion, another downside to consider with GUIs is how difficult it can be to roll back a system should things go sideways. Many of us know all too well how rolling back a change can involve lots of pointing-and-clicking just to undo the changes that were previously made. In the end, there’s more time and human involvement spent in the UI than actually fixing the problem. It’s definitely not the best all-around solution. For the most part, GUIs can be great for learning and playing around, but definitely not ideal for maintaining anything beyond a lab or sandbox environment. CLIs CLIs are a little bit better than GUIs for helping you manage critical issues. Instead of pointing-and-clicking, commands are directly issued into the application. When you get down to it, CLIs are really just a text user interface. Most CLIs allow for running “headless”, meaning they don’t require continuous user input (all input can be provided at runtime). One benefit is that it’s easier to use CLIs with automated workflows, so it’s definitely a step in the right direction. By themselves though, it’s impossible to embed any sort of logic when using CLIs, unless there’s some sort of scripting (shell scripts, Ruby, Python, etc.) involved. This means that it’s great to send a single command (or even a series of commands), but it can be difficult to maintain a high level of assurance without more involved scripting logic being used with the CLI. APIs Now we’re talking! Who doesn’t love APIs? At the end of the day, almost everything interacts with the underlying APIs. Most GUIs, CLIs, and even IaC tools interact with APIs to do what they do. Whether you use curl, Postman, Paw, or other tools, you’re typically interacting with a single API endpoint. There’s still an inherent lack of logic though, unless you use some sort of scripting or application to add it. We’ve all written small scripts or apps that interact with APIs to achieve certain outcomes. This is terrific, but it’s still pretty manually intensive to create. For many of us, it can still take a bit of time to custom-build a script or app for each type of management need. While this gives an extreme amount of customizability (the sky’s the limit with what you can do), it’s really not practical or scalable in the long run. All cloud providers present an API for managing the platform and OCI is no exception. OCI provides a versatile API interface for both developers and users to utilize when interacting with the system. But the beauty here lies in OCI’s flexibility, and the API is only one of serveral options open to you. There are other tools such as Upbound that offer a great twist for having a single interface managing multiple backend interfaces. IaC tools When you really need to tailor your environment and maximize control over how it performs, IaC tools are what you need. IaC tools are designed from the ground-up to manage infrastructure resources using code. Resources are defined within the code, with the tool itself providing the necessary structure and logic to quickly and easily build a definition of what you need or want an environment to be. The basic “scaffolding” (logic elements, API interactions, etc.) are all abstracted, allowing you to focus on describing the resources you need or want to exist in the environment. By far, this is one of the easiest and fastest ways to build and maintain cloud environments. But this isn’t just limited to the cloud, on-premesis resources can often be managed with IaC tools as well. Are you using multiple cloud providers? Even more reason to utilize IaC in managing your IT infrastructure. With IaC, it’s common to use Git on the back-end. Git allows you to get a deep history of changes and its robust version control system enables easy and rapid rollbacks. Additionally, if you use Git for storing the code definitions, you’re able to use standard processes and tools to monitor/approve/manage changes before they’re made. Whether using policy-as-code (such as Open Policy Agent, with implementations such as Policy-as-Code on OCI using Open Policy Agent) or a manual pull request (PR)/merge request (MR) review process, you can have a really solid review/approval/compliance mechanism (not to mention yet another audit trail that’s separate from the cloud/platform itself). Infrastructure management One of the most common and popular IaC tools is HashiCorp Terraform. Other tools, such as Pulumi, are also available, but the common goal of each is the programmatic management of infrastructure. Configuration management Sometimes infrastructure isn’t your major concern and what you really need are tools that predominantly operate in the realm of configuration management, such as Ansible or Chef/Cinc. These are all options for managing infrastructure as well, but you may be better off with tools that focus exclusively on infrastructure management. For OCI’s part, it’s certainly no exception in supporting many of these different platforms. IaC and Terraform In the rest of this series, we’re going to target using Terraform to manage OCI infrastructure. Why Terraform? It’s fairly mature at this point, widely adopted, and has support for a wide variety of cloud platforms, including OCI. Additionally, there are plenty of resources available and Terraform enjoys strong user support, making it an ideal tool. Terraform is mainly targeted at managing infrastructure and not so much the configuration management side of things. But it’s super powerful when combined with a traditional configuration management tool like Ansible. What’s Next By now, you should be a little more familiar with IaC and ready to get started with Terraform! Until your next lesson, happy coding! Take a look at the next lesson in the Terraform 101 series and go through a very quick experience in using Terraform. From there, you’ll dive into several aspects of how Terraform works. To explore more information about development with Oracle products: Oracle Developers Portal Oracle Cloud Infrastructure By Tim Clegg","categories": ["iac","opensource"],
        "tags": ["open-source","terraform","iac","devops","get-started"],
        "url": "/tutorials/tf-101/1-why-iac",
        "teaser": ""
      },{
        "title": "Deploying A Multi-Cluster Verrazzano On Oracle Container Engine for Kubernetes (OKE) Part 2",
        "excerpt":" In Part 1, we discussed setting up the network infrastructure for a multi-cluster Verrazzano deployment. Earlier, we focused on deploying Verrazzano on Oracle Container Engine (OKE). In this article, we will configure the clusters so they behave as a kind of global cluster. Below is the multi-clustering process depicted graphically: Verrazzano multi-cluster deployment and registration process Prerequisites To successfully complete this tutorial, you will need to have: An Oracle Cloud Infrastructure Free Tier account. Start for Free A MacOS, Linux, or Windows computer with ssh support installed Git [Terraform 1.0.0] or later Completed section 2 of the series Getting started First, a little refresher. Recall that a Verrazzano multi-cluster has: 1 Admin cluster, and 1 or more managed clusters Where each Verrazzano cluster is also a Kubernetes cluster: Verrazzano multi-cluster architecture Also, remember we have the following setup: Remote peering with different regions And that we chose the Admin server to be in the Singapore OCI region. Next, We’ll install Verrazzano with the dev/prod profile on the Admin cluster and with managed-cluster profile on the managed clusters. Quick note on using kubectx In the commands below, we’ll use kubectx to set the Kubernetes context such that a context is equivalent to a Kubernetes cluster. Strictly speaking, that’s not really true, but it does serve our purposes here. In our example, since we have 1 Admin server and 3 managed servers in 4 different regions, we have 4 different contexts: Verifying your Kubernetes context To ensure we’re always using the correct context, we execute the kubectx &lt;context-name&gt; before every command. Installing Verrazzano as Admin Installing Verrazzano as the Admin cluster is straightforward, just follow the steps in the quickstart guide. During the set up process you can choose between the dev/prod profile. First things first Before you start, make sure that your context is pointing to “admin” on the operator host: Verifying your Kubernetes context If it’s pointing to one of the other clusters, change it as follows: kubectx admin Install Verrazzanno Begin the deployment: kubectl apply -f https://github.com/verrazzano/verrazzano/releases/download/v1.0.3/operator.yaml Wait for the deployment to finish: kubectl -n verrazzano-install rollout status deployment/verrazzano-platform-operator Confirm that the operator pods are working correctly: kubectl -n verrazzano-install get pods NAME READY STATUS RESTARTS AGE verrazzano-platform-operator-54cf56884f-46zzk 1/1 Running 0 91s Install Verrazzano: kubectl apply -f - &lt;&lt;EOF apiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: admin spec: profile: dev EOF Wait until the installation is complete: kubectl wait \\ --timeout=20m \\ --for=condition=InstallComplete \\ verrazzano/admin This will take a while. In the meantime, let’s install Verrazzano on the managed clusters. Installing Verrazzano on managed clusters Change the context to one of the managed clusters and install the operator again: kubectx sydney kubectl apply -f https://github.com/verrazzano/verrazzano/releases/download/v1.0.3/operator.yamlkubectl -n verrazzano-install rollout status deployment/verrazzano-platform-operator Repeat the previous command for each of the remaining managed clusters. NOTE: Before running in each managed cluster, ensure that you’ve changed your context with kubectx &lt;contextname&gt; as noted above. Using the same procedure as the Admin region, verify that the Verrazzano operator has been successfully installed. Using the managed profile, install Verrazzano for each cluster by changing the context and name accordingly: apiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: sydney spec: profile: managed-cluster Verifying the Admin cluster and managed clusters While the managed clusters are being installed, let’s see if we can access the various consoles. First, make sure that you can log in into the Verrazzano and Rancher consoles. Change the context again and verify: kubectx sydney kubectl wait \\ --timeout=20m \\ --for=condition=InstallComplete \\ verrazzano/sydney Repeat the verification for each managed cluster. Registering the managed clusters Verify the the CA certificate type for each managed cluster: kubectx sydney kubectl -n verrazzano-system get secret system-tls -o jsonpath='' If this value is empty, it’s actually a good thing. This means that your managed cluster is using certificates signed by a well-known certificate authority and you can generate a secret containing the CA certificate in YAML format. If it’s not empty, then the certificate is self-signed and needs to be extracted. Refer to the workflow at the beginning of this article. kubectx sydney CA_CERT=$(kubectl \\ get secret system-tls \\ -n verrazzano-system \\ -o jsonpath=\"\" | base64 --decode) kubectl create secret generic \"ca-secret-sydney\" \\ -n verrazzano-mc \\ --from-literal=cacrt=\"$CA_CERT\" \\ --dry-run=client -o yaml &gt; managedsydney.yaml Repeat the above for the 2 other regions, replacing the region/context and filenames accordingly. Create 3 secrets on the Admin cluster that contains the CA certificate for each managed cluster: kubectx adminkubectl apply -f managedsydney.yaml kubectl apply -f managedmumbai.yaml kubectl apply -f managedtokyo.yaml Get the cluster name for the Admin Cluster: kubectl config get contexts Cluster names Get the API Server address for the Admin server: kubectx adminexport CLUSTERNAME=\"cluster-cillzxw34tq\"APISERVER=$(kubectl config view -o jsonpath=\"\") Create a ConfigMap that contains the Admin cluster’s API server address: kubectx adminkubectl apply -f &lt;&lt;EOF - apiVersion: v1 kind: ConfigMap metadata: name: verrazzano-admin-cluster namespace: verrazzano-mc data: server: \"${API_SERVER}\" EOF Create the VerrazzanoManagedCluster object for each managed cluster: kubectx admin kubectl apply -f &lt;&lt;EOF - apiVersion: clusters.verrazzano.io/v1alpha1 kind: VerrazzanoManagedCluster metadata: name: sydney namespace: verrazzano-mc spec: description: \"Sydney VerrazzanoManagedCluster object\" caSecret: ca-secret-sydney EOFkubectl apply -f &lt;&lt;EOF - apiVersion: clusters.verrazzano.io/v1alpha1 kind: VerrazzanoManagedCluster metadata: name: mumbai namespace: verrazzano-mc spec: description: \"Mumbai VerrazzanoManagedCluster object\" caSecret: ca-secret-mumbai EOFkubectl apply -f &lt;&lt;EOF - apiVersion: clusters.verrazzano.io/v1alpha1 kind: VerrazzanoManagedCluster metadata: name: tokyo namespace: verrazzano-mc spec: description: \"Tokyo VerrazzanoManagedCluster object\" caSecret: ca-secret-tokyo EOF Wait for the VerrazzanoManagedCluster resource to reach the Ready status: kubectx adminkubectl wait --for=condition=Ready \\ vmc sydney -n verrazzano-mckubectl wait --for=condition=Ready \\ vmc sydney -n verrazzano-mckubectl wait --for=condition=Ready \\ vmc sydney -n verrazzano-mc Export a YAML file created to register the managed cluster: kubectx adminkubectl get secret verrazzano-cluster-sydney-manifest \\ -n verrazzano-mc \\ -o jsonpath= | base64 --decode &gt; registersydney.yamlkubectl get secret verrazzano-cluster-mumbai-manifest \\ -n verrazzano-mc \\ -o jsonpath= | base64 --decode &gt; registermumbai.yamlkubectl get secret verrazzano-cluster-tokyo-manifest \\ -n verrazzano-mc \\ -o jsonpath= | base64 --decode &gt; registertokyo.yaml On each managed cluster, apply the registration file: kubectx sydney kubectl apply -f registersydney.yamlkubectx mumbai kubectl apply -f registermumbai.yamlkubectx tokyo kubectl apply -f registertokyo.yaml Verify whether the registration completed successfully: kubectx admin kubectl get vmc sydney -n verrazzano-mc -o yaml kubectl get vmc mumbai -n verrazzano-mc -o yaml kubectl get vmc tokyo -n verrazzano-mc -o yaml Additional verifications Verrazzano console Navigate to the Verrazzano console and log in. You should be able to see all 3 clusters: Managed clusters in Verrazzano Rancher console Similarly, on the Rancher console, you should be able to see all 4 clusters: Admin and managed clusters in Rancher NOTE: “local” is the Admin cluster whereas the others are the managed clusters. Conclusion This concludes the exercise of connecting OKE clusters deployed in different regions into a multi-cluster Verrazzano deployment. Keep in mind that in this series we never configured things like DNS, Certificates, or the Ingress Controller. Our goal was just to get the multi-cluster configuration going. In a future article, we’ll come back to this topic and look at those other things as well. To explore more information about development with Oracle products: Oracle Developers Portal Oracle Cloud Infrastructure By Ali Mukadam","categories": ["cloudapps","opensource"],
        "tags": ["open-source","oke","kubernetes","verrazzano","terraform","devops"],
        "url": "/tutorials/multi-cluster-verrazzano-oke/2-deploy-multi-cluster-verrazzano-oke",
        "teaser": ""
      },{
        "title": "Experiencing Terraform",
        "excerpt":" 1 Introduction Terraform 101 This next part in the series will give you all you need to begin harnessing the power of infrastructure-as-code (IaC) in your environment. Even if this is your first time using Terraform or just looking to get reacquainted, this will be the place for you. In this article, we’ll cover the basics of how Terraform works and then explore an actual working example. During the journey, we’ll point out several resources essential to your future work with Terraform and managing your Oracle Cloud Infrastructure (OCI) environment. After going through this tutorial, you’ll be able to better understand why IaC is amazing and how it has gained so much traction. You’ll also learn how to harness IaC to improve the efficiency of managing your environment. Key topics covered in this tutorial: Creating Terraform code files Examining Terraform’s resource configuration plans Using Terraform to create a Virtual Cloud Network (VCN) and subnet Organizing your Terraform code An introduction to OCI Cloud Shell For additional information, see: Signing Up for Oracle Cloud Infrastructure Getting started with Terraform Getting started with OCI Cloud Shell Begin &raquo; 2 Prerequisites To successfully complete this tutorial, you will need the following: An Oracle Cloud Infrastructure Free Tier account. Start for Free. A MacOS, Linux, or Windows computer with ssh support installed. OCI Cloud Shell (It provides a great platform for quickly working with Terraform as well as a host of other OCI interfaces and tools.) &laquo; Back Continue &raquo; 3 Getting started Terraform is incredibly easy to use. In this section, we’ll learn how to: Create a VCN Create a subnet in the VCN NOTE: All commands will be used within OCI Cloud Shell. If you haven’t started it up already, now’s the time to open your own Cloud Shell session! Set up the OCI provider Create a new directory for the project and then navigate into it: mkdir experiencing-tf cd experiencing-tf NOTE: The experiencing-tf directory will contain both our Terraform files and our Terraform state. This will be our project directory. Using your favorite editor (nano, vi, etc.), add the following to provider.tf: terraform { required_version = \"&gt;= 1.0.0\" } provider \"oci\" { region = var.region tenancyocid = var.tenancyocid } &laquo; Back Continue &raquo; 4 Create a VCN Now that we have our environment set up, let’s create a VCN. Create a new file, vcn.tf, with the following content: resource ocicorevcn \"tf_101\" { cidr_block = \"192.168.1.0/24\" compartmentid = var.tenancyocid display_name = \"tf-101\" dns_label = \"tf101\" } The above tells Terraform that we want a VCN with a name of tf-101, using a CIDR block of 192.168.1.0/24, deployed into the root (tenancy) compartment. NOTE: To keep things simple, this example uses the tenancy (root) compartment, which is often times locked down in many tenancies. However, if you’re using a tenancy with limited permissions (i.e., one in which you can’t deploy to the root compartment), you’ll need to replace each instance of var.tenancy_ocid above with your own compartment OCID. For example, compartmentid = \"&lt;yourOCID&gt;\". &laquo; Back Continue &raquo; 5 Set up a subnet Next we’ll create a subnet within our VCN. To do this, add the following to a new file called subnets.tf: resource ocicoresubnet \"vlan1\" { cidr_block = \"192.168.1.0/24\" compartmentid = var.tenancyocid display_name = \"vlan1\" dns_label = \"vlan1\" prohibitpubliciponvnic = true vcnid = ocicorevcn.tf101.id } This will tell Terraform to manage a subnet that lives within the VCN we’ve previously defined, using the entire CIDR space. We’ve also prohibited the use of public IPs in this subnet and have given it the wonderfully original name, vlan1. Define resource definitions in Terraform Up to this point, we’ve referenced a couple of variables in our resource definitions: var.region and var.tenancy_ocid. Now, we need to go ahead and define these using Terraform code. To do so, edit variables.tf and add the following content: variable \"tenancy_ocid\" { type = string } variable \"region\" { type = string } &laquo; Back Continue &raquo; 6 Set up an output Now that we’ve defined our inputs, let’s go ahead and set up the status of the VCN as our output. Edit outputs.tf and add the following content: output \"vcn_state\" { description = \"The state of the VCN.\" value = ocicorevcn.tf_101.state } Save the file and exit the editor. Configure related environment variables Right out of the box, the OCI Cloud Shell session comes prepopulated with lots of great environment settings that make our job a whole lot easier. To be able to access them in our new workspace though, we’ll need to put them in a format that Terraform can easily use. The following commands will set up a few environment variables that Terraform will be using. In a terminal window, enter the following: declare -x TFVARtenancyocid=echo $OCITENANCY declare -x TFVARregion=echo $OCI_REGION &laquo; Back Continue &raquo; 7 Initialize Terraform Now it’s time to see this all work! Initialize Terraform by running: terraform init Terraform echoes something similar to the following: Initializing the backend... Initializing provider plugins... - Finding latest version of hashicorp/oci... - Installing hashicorp/oci v4.45.0... - Installed hashicorp/oci v4.45.0 (unauthenticated) Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run \"terraform init\" in the future. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. At this point, Terraform is ready for us to give it directions on what OCI resources we want it to manage. In Terraform terms, this is called a plan. A closer look at Terraform’s proposed plan Let’s look at the plan that Terraform proposes. The terraform plan command provides a preview of the actions that Terraform will take to configure resources according to our configuration file. In a console, run: terraform plan Terraform echoes something similar to the following: Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # ocicoresubnet.vlan1 will be created + resource \"ocicoresubnet\" \"vlan1\" { + availability_domain = (known after apply) + cidr_block = \"192.168.1.0/24\" + compartment_id = \"ocid1.tenancy.oc1..&lt;sanitized&gt;\" + defined_tags = (known after apply) + dhcpoptionsid = (known after apply) + display_name = \"vlan1\" + dns_label = \"vlan1\" + freeform_tags = (known after apply) + id = (known after apply) + ipv6cidr_block = (known after apply) + ipv6virtualrouterip = (known after apply) + prohibitinternetingress = (known after apply) + prohibitpubliciponvnic = true + routetableid = (known after apply) + securitylistids = (known after apply) + state = (known after apply) + subnetdomainname = (known after apply) + time_created = (known after apply) + vcn_id = (known after apply) + virtualrouterip = (known after apply) + virtualroutermac = (known after apply) } # ocicorevcn.tf_101 will be created + resource \"ocicorevcn\" \"tf_101\" { + cidr_block = \"192.168.1.0/24\" + cidr_blocks = (known after apply) + compartment_id = \"ocid1.tenancy.oc1..&lt;sanitized&gt;\" + defaultdhcpoptions_id = (known after apply) + defaultroutetable_id = (known after apply) + defaultsecuritylist_id = (known after apply) + defined_tags = (known after apply) + display_name = \"tf-101\" + dns_label = \"tf101\" + freeform_tags = (known after apply) + id = (known after apply) + ipv6cidr_blocks = (known after apply) + is_ipv6enabled = (known after apply) + state = (known after apply) + time_created = (known after apply) + vcndomainname = (known after apply) } Plan: 2 to add, 0 to change, 0 to destroy. Changes to Outputs: + vcn_state = (known after apply) ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── Note: You didn't use the -out option to save this plan, so Terraform can't guarantee to take exactly these actions if you run \"terraform apply\" now. This output tells us that Terraform is proposing the creation of two new resources: a VCN and a subnet. Both of these are expected, and since everything else appears to be in order, we’ll go ahead and tell Terraform to make the changes. In a console, run: terraform apply We’ll see something like what we saw for terraform plan, but this time there’ll be a prompt asking if we’d like to continue: &lt;snip&gt; Changes to Outputs: + vcn_state = (known after apply) Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: Accept the proposed changes when prompted. Terraform will something similar to: &lt;snip&gt; Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes ocicorevcn.tf_101: Creating... ocicorevcn.tf_101: Creation complete after 2s [id=ocid1.vcn.oc1.phx.&lt;sanitized&gt;] ocicoresubnet.vlan1: Creating... ocicoresubnet.vlan1: Creation complete after 2s [id=ocid1.subnet.oc1.phx.&lt;sanitized&gt;] Apply complete! Resources: 2 added, 0 changed, 0 destroyed. Outputs: vcn_state = \"AVAILABLE\" And that’s it! Just one command to set up multiple resources! &laquo; Back Continue &raquo; 8 Cleaning Up Since we’re at the end of this learning session, we need to clean up after ourselves and remove the test components we just created. Let’s go ahead and remove the VCN and subnet. In the OCI Console, this would typically require multiple clicks. However, since we’re using Terraform, one command is all we need. In a console, run: terraform destroy Terraform echoes something similar to the following: $ terraform destroy ocicorevcn.tf_101: Refreshing state... [id=ocid1.vcn.oc1.phx.&lt;sanitized&gt;] ocicoresubnet.vlan1: Refreshing state... [id=ocid1.subnet.oc1.phx.&lt;sanitized&gt;] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: - destroy Terraform will perform the following actions: # ocicoresubnet.vlan1 will be destroyed - resource \"ocicoresubnet\" \"vlan1\" { - cidr_block = \"192.168.1.0/24\" -&gt; null - compartment_id = \"ocid1.compartment.oc1..&lt;sanitized&gt;\" -&gt; null - defined_tags = { - \"Oracle-Tags.CreatedBy\" = \"&lt;sanitized&gt;\" - \"Oracle-Tags.CreatedOn\" = \"2021-09-30T19:44:47.597Z\" } -&gt; null - dhcpoptionsid = \"ocid1.dhcpoptions.oc1.phx.&lt;sanitized&gt;\" -&gt; null - display_name = \"vlan1\" -&gt; null - dns_label = \"vlan1\" -&gt; null - freeform_tags = {} -&gt; null - id = \"ocid1.subnet.oc1.phx.&lt;sanitized&gt;\" -&gt; null - prohibitinternetingress = true -&gt; null - prohibitpubliciponvnic = true -&gt; null - routetableid = \"ocid1.routetable.oc1.phx.&lt;sanitized&gt;\" -&gt; null - securitylistids = [ - \"ocid1.securitylist.oc1.phx.&lt;sanitized&gt;\", ] -&gt; null - state = \"AVAILABLE\" -&gt; null - subnetdomainname = \"vlan1.tf101.oraclevcn.com\" -&gt; null - time_created = \"2021-09-30 19:44:47.659 +0000 UTC\" -&gt; null - vcn_id = \"ocid1.vcn.oc1.phx.&lt;sanitized&gt;\" -&gt; null - virtualrouterip = \"192.168.1.1\" -&gt; null - virtualroutermac = \"00:00:17:28:74:9C\" -&gt; null } # ocicorevcn.tf_101 will be destroyed - resource \"ocicorevcn\" \"tf_101\" { - cidr_block = \"192.168.1.0/24\" -&gt; null - cidr_blocks = [ - \"192.168.1.0/24\", ] -&gt; null - compartment_id = \"ocid1.compartment.oc1..&lt;sanitized&gt;\" -&gt; null - defaultdhcpoptions_id = \"ocid1.dhcpoptions.oc1.phx.&lt;saanitized&gt;\" -&gt; null - defaultroutetable_id = \"ocid1.routetable.oc1.phx.&lt;sanitized&gt;\" -&gt; null - defaultsecuritylist_id = \"ocid1.securitylist.oc1.phx.&lt;sanitized&gt;\" -&gt; null - defined_tags = { - \"Oracle-Tags.CreatedBy\" = \"&lt;sanitized&gt;\" - \"Oracle-Tags.CreatedOn\" = \"2021-09-30T19:44:46.481Z\" } -&gt; null - display_name = \"tf-101\" -&gt; null - dns_label = \"tf101\" -&gt; null - freeform_tags = {} -&gt; null - id = \"ocid1.vcn.oc1.phx.&lt;sanitized&gt;\" -&gt; null - ipv6cidr_blocks = [] -&gt; null - is_ipv6enabled = false -&gt; null - state = \"AVAILABLE\" -&gt; null - time_created = \"2021-09-30 19:44:46.736 +0000 UTC\" -&gt; null - vcndomainname = \"tf101.oraclevcn.com\" -&gt; null } Plan: 0 to add, 0 to change, 2 to destroy. Changes to Outputs: - vcn_state = \"AVAILABLE\" -&gt; null Do you really want to destroy all resources? Terraform will destroy all your managed infrastructure, as shown above. There is no undo. Only 'yes' will be accepted to confirm. Enter a value: After entering yes at the prompt, Terraform will destroy the resources for us: &lt;snip&gt; Do you really want to destroy all resources? Terraform will destroy all your managed infrastructure, as shown above. There is no undo. Only 'yes' will be accepted to confirm. Enter a value: yes ocicoresubnet.vlan1: Destroying... [id=ocid1.subnet.oc1.phx.&lt;sanitized&gt;] ocicoresubnet.vlan1: Destruction complete after 1s ocicorevcn.tf_101: Destroying... [id=ocid1.vcn.oc1.phx.&lt;sanitized&gt;] ocicorevcn.tf_101: Destruction complete after 1s Destroy complete! Resources: 2 destroyed. Fun facts: It took Terraform under 10 seconds to provision a VCN and subnet (try it yourself by running time terraform apply -auto-approve), and under 7 seconds to destroy (try it yourself by running time terraform destroy -auto-approve) those same resources. Try to beat that doing it by hand in the OCI Console! &laquo; Back Continue &raquo; 9 What's Next If this was your first time using Terraform, that was a LOT to take in! Let’s review everything we covered in this session: Created 5 Terraform code files that defined the inputs, outputs, and resources we wanted Terraform to manage Learned how to review Terraform’s resource configuration plans (terraform plan) Let Terraform create a VCN and subnet for us (very quickly) Organized our Terraform code into logical files (so it’s easy to navigate the code) Got a taste for how handy and easy it is to use the OCI Cloud Shell Hopefully, this short tutorial gave you a glimpse into the basic Terraform workflow and how powerful a tool it can be. This was just a simple example, but it was a solid first start at using Terraform. The next lesson, Understanding Terraform Basics, digs into some of the core concepts and components in a Terraform project. To explore more information about development with Oracle products: Oracle Developers Portal Oracle Cloud Infrastructure &laquo; Back By Tim Clegg","categories": ["iac","opensource"],
        "tags": ["open-source","terraform","iac","devops","get-started"],
        "url": "/tutorials/tf-101/2-experiencing-terraform",
        "teaser": ""
      },{
        "title": "Understanding The Basics Of Terraform",
        "excerpt":" 1 Introduction Terraform 101 In our first lesson, we covered why you should care about Infrastructure as Code (IaC). We also touched on a few of the many tools in this space, and finally, we decided to narrow our focus down to Terraform. The last lesson took you through a really quick and simple scenario using Terraform, setting up a virtual cloud network (VCN) and a subnet. It was short but powerful, and hopefully helped you understand a bit of why Terraform (and IaC) is such a great tool. This lesson will dive a little deeper and take you through some of the basic concepts you’ll need to effectively work with Terraform. Key topics covered in this tutorial: Learning about some of the major Terraform components (executables, providers, code, and states) Discovering some of the basic Terraform commands (init, plan, apply, and console) Setting up a Terraform project (inputs, outputs, and logic) For additional information, see: Signing Up for Oracle Cloud Infrastructure Getting started with Terraform Getting started with OCI Cloud Shell Begin &raquo; 2 Prerequisites To successfully complete this tutorial, you will need to have the following: An Oracle Cloud Infrastructure Free Tier account. Start for Free. A MacOS, Linux, or Windows computer with ssh support installed. Terraform CLI OCI Cloud Shell &laquo; Back Continue &raquo; 3 Major Terraform Components In the IaC world, resources are defined using code. Terraform follows a declarative language model, meaning that you tell it where you want to be after it executes and it figures out what’s needed to get there. Terraform doesn’t need to be told “do this, then that, then finish with this,” as is common with many procedural languages. You simply tell it where you want it to end up and it’ll map out the path to get there. Most of the time, Terraform is able to figure out the right steps on its own. Occasionally, it’ll need some help, but we’ll talk a little more about that in a later tutorial in this series. Terraform has several core components that you should become familiar with: Terraform executable Terraform providers Terraform code Terraform state Terraform executable The Terraform executable can be easily downloaded and installed on a variety of different platforms. Check out Terraform’s download page to locate the Terraform CLI binaries for your system. Linux If you’re using Linux, it’s possible that Terraform might exist in your favorite package manager (look at yum install terraform or apt-get install terraform). Oracle Linux makes it simple and painless to install Terraform. If you’re Using Oracle Linux, just use Yum for the best experience. However, if you’re not using Oracle Linux, you’ll likely need to configure your package manager (see: RHEL/Fedora Yum docs or Debian/Ubuntu APT docs). MacOS In MacOS, simply download the binary and place it somewhere in your path. To verify that your system can locate the executable and confirm that Terraform is up and running, just run terraform -v in a terminal window. If it echoes the current Terraform version number, you should be good to go! If you like, you can also use Homebrew to get things going with Terraform. All you need to do is run brew install terraform in a terminal window. Terraform providers Providers allow Terraform to interact with different platforms. This is the component that bridges the gap between the Terraform code and a specific platform such as OCI. For instance, the OCI provider translates the Terraform code to a form Terraform requires to interact with the OCI API directly. However, Terraform isn’t limited to a single provider. At any given time, one or more providers can be in use. Many clouds have Terraform providers, allowing you to define resources that are specific to a cloud using a standard format, tool, and language. When you have a moment, take a look at the OCI Terraform provider documentation to get a better sense of the different kinds of resources that can be managed with it. At the end of this tutorial, we’ll walk through a really simple example, so don’t sweat the details now. Just hang in there and it’ll all come together! Getting started with providers In order to start using providers with Terraform, you’ll first need to tell Terraform exactly which providers you’ll be using in your code. Providers are typically referenced in terraform and provider blocks. NOTE: A block of Terraform code is anything enclosed within curly brackets. Set up Terraform to use the OCI provider Let’s look at an example of how to tell Terraform we want to use the OCI provider: terraform { required_version = \"&gt;= 0.14.0\" required_providers { oci = { source = \"hashicorp/oci\" version = \"&gt;= 4.39.0\" } } } provider \"oci\" { region = var.region auth = \"InstancePrincipal\" } Zooming in a little closer, the first terraform block instructs Terraform to download and include specific providers (those within the required_providers block). In this case, we’re including the oci provider, specifically version 4.39.0 or greater. NOTE: The required_providers portion of the terraform block is optional, but it’s nice to have in there since it allows you to constrain the version and source of a given provider. Next up, the OCI provider block contains information that’s specific to the provider. In this case, we’re telling the OCI provider which region to use and asking it to authenticate against the OCI API using Instance Principals. NOTE: The auth line in the OCI provider block is optional. It’s one way of authenticating (using Instance Principals), but certainly not the only way. Managing Terraform providers Although it’s possible to manually download and install Terraform providers, by default, Terraform will automatically download and install (manage) providers for you. This is accomplished by running the terraform init command from the directory containing your Terraform code. NOTE: Managed Terraform services such as OCI Resource Manager (ORM) do not require you to go through any Terraform initialization process. This is managed for you by ORM. This step is needed when running Terraform from your own computer (or server). &laquo; Back Continue &raquo; 4 Terraform code Terraform uses a proprietary configuration language. Like any language, it’s reasonable to expect a slight learning curve when you’re first starting out, but it’s ease and versatility will grow on you as you gain some familiarity with it. It’s highly recommended that you at least skim the Terraform configuration language documentation to get some familiarity with its basic concepts, structural components, and functions. NOTE: Terraform’s configuration language has undergone some significant changes over the past several years. If you come across code that was written for Terraform v0.11 (or earlier), you’ll likely need to update it to a more recent version. Within the code, you’ll likely define a combination of variables (user-provided input), locals (local variables), outputs (values shown as output after running Terraform), providers, and resources. Most of the other code elements are engineered to support the management of resources. These remaining elements are really the parts that get changed in OCI (or another environment, depending on the providers you’re using and the resources you’re managing) when Terraform is run. When you get down to it, resources are the beating heart of Terraform. Whether you’re declaring a single resource or many, resources are typically what we’re trying to manage with Terraform. &laquo; Back Continue &raquo; 5 Terraform commands Terraform has a broad set of capabilities, and as you might imagine, there are many different commands that it accepts. Here are some of the most common ones that you’ll find yourself using: terraform init This must be run at least once for a Terraform project. During the init process, Terraform downloads any needed providers, sets up the state (if it doesn’t already exist), and performs any other necessary start-up tasks. terraform plan Prompts Terraform to do a dry run, non-destructively determining what it would do if it was to apply the configuration settings. Terraform simply tells you what it thinks should be done and lets you review the outcomes without any changes being made. It’s always a good idea to run terraform plan and review the output before applying. This way you can make sure that you fully understand what Terraform is intending to do. terraform apply Changes are made with this command. By default, it will show you the same output as terraform plan, but will additionally prompt you to enact the proposed changes. There are ways you can short-circuit this process and automatically accept all of the changes, but it’s generally a good idea to review the recommendations before applying them, especially when you’re running Terraform manually. terraform console Gives you an interactive console where you can enter different Terraform commands. This is particularly useful for building and testing logic in Terraform code. &laquo; Back Continue &raquo; 6 A Terraform project In this section, we’ll take a look at the various parts of a Terraform project. In general, a typical Terraform project can be broken down into familiar constructs common to many other applications: Inputs Outputs Logic Inputs Terraform receives input through variables. Variables may be set via command-line parameters, static files, or environment variables. There are a lot of facets to mastering the usage of variables, so we’ll cover these in greater detail in a separate lesson. For now, know that each variable must be given a unique name and value, like in the following example: variable \"region\" {} The above example is the most basic variable definition. They definitely can get more complex, though. If you want to jump ahead, feel free to look ahead to the lesson on variables and check out the Terraform variable documentation. Variable definitions file At this point, you’re probably wondering where you should place your variables. Most often, variables are defined in a file called variables.tf. Pretty straightforward, right? This isn’t required, but it’s good form and common practice for everything but really small projects. Most of the time, it’s a good idea to make the filename specific to the type of resources defined in it. To dig in deeper, check out the excellent Terraform variables documentation. Outputs There are times when Terraform needs to direct data about the environment back to the display. For example, when a compute instance is deployed, a private IP address may be specified. If it’s not specified, OCI will pick an IP address for us from the subnet the instance is being deployed in. Wouldn’t it be nice to be able to see this private IP address? Outputs provide a way to make that information visible to you. Many different attributes are exported by Terraform resources, allowing you to easily examine them through the use of outputs. In Terraform, outputs are technically referred to as “output variables.” The current values of these variables are included at the end of the output for the terraform apply command. Note that these values are not displayed when you run terraform plan. Let’s take a look at an example of an output: output \"vcn_state\" { description = \"The state of the VCN.\" value = ocicorevcn.tf_101.state } Outputs definition file While outputs can be defined in any Terraform code file, it’s best to get into the practice of separating Terraform code into logical files so the code base is easier to navigate. For this purpose, it’s recommended that you use the outputs.tf file for these definitions. Terraform functions as output definitions Additionally, the value of an output variable doesn’t have to be static, it can be any programmatic calculation supported in Terraform code. We can see this at work with the following example: output \"twoplustwo\" { value = 2+2 } While this is a trivial example, you can use many functions in Terraform. String substitutions, merging, changes, calculating hashes, etc. The world’s your oyster! With this added functionality, accessing outputs can be particularly useful when you’re using Terraform as part of an automated pipelines. It can also be an invaluable resource to users running it manually. Logic Another aspect of Terraform’s versatility is its ability to provide a wealth of functions to embed logic and perform complex computations. Need to iterate through a list or map? You’re covered! How about concatenating strings (or string manipulations in general)? Got it. If-then-else logic? Yep, it’s in there. Need to do some CIDR calculations? There are functions for that too. If you’d like to learn more, take a look at these resources on Terraform functions, Terraform conditional expressions, and the Terraform for expressions. It’s well worth checking out, even if you’re just trying to get some familiarity with what’s available and possible. &laquo; Back Continue &raquo; 7 Terraform State When interacting with an environment, there are three main components Terraform needs (in addition to the Terraform binary): Terraform code Environment being managed Terraform state The first two components (code and environment) are topics you should already be familiar with. What’s left to cover is how Terraform use these resources to develop an accurate picture of available resources. Terraform uses a lot of intelligence to map out relationships between managed resources. Many applications rely on a local database to store information needed by the application. Terraform’s no different, and is very transparent in how it manages its application content, by default storing what’s needed in a local JSON file. The state is where Terraform caches a copy of what it knows about the environment. Details about the managed resources are stored there in verbose form, along with Inputs (variable values). NOTE: State files should be carefully guarded as it’s possible to have secrets or other sensitive data stored in them. Even though a variable might be marked as sensitive, Terraform can still store its contents in the state file. Although it might not show via terraform apply, it might be right there in plain text in the state file. Updates and Deltas When Terraform runs, it will update the state with what actually exists in the managed environment and compare that state against the code. Any deltas (variances between the code and state) will be marked as requiring a remediation (changes that must be made to bring the current resource state in line with the code is asking for it). Keeping the Terraform state in sync It’s important to always use the latest copy of the state, as terraform apply might update the state file. This is particularly important when sharing the management responsibilities for a single environment among multiple people. Each environment has a single state file. If the state file becomes corrupted or out of sync, Terraform can do weird and unexpected things. So, carefully guard your state file! Where the Terraform state lives By default, the state (terraform.tfstate) is stored locally within the project directory. But, keep in mind that there are cases where backends may be defined in such a way that Terraform is required to store the state in a different location. For the remaining parts of the tutorial, though, we’ll stick to keeping the state local. Backends - common use scenarios For production deployments, many Terraform users find the OCI Resource Manager beneficial, as it maintains the Terraform state file for each stack automatically. However, you might find that OCI Object Storage works better for you as a backend. Or, you might prefer using git. Whichever way you prefer, Terraform has you covered. Terraform supports many different kinds of backends are supported. For a full list, check out the backend documentation for more information. &laquo; Back Continue &raquo; 8 What's next Now that you understand some of the basic components used in a Terraform project, let’s dive into variables in the next lesson. To explore more information about development with Oracle products: Oracle Developers Portal Oracle Cloud Infrastructure &laquo; Back By Tim Clegg","categories": ["iac","opensource"],
        "tags": ["open-source","terraform","iac","devops","get-started"],
        "url": "/tutorials/tf-101/3-understanding-terraform-basics",
        "teaser": ""
      },{
        "title": "Terraform Variables",
        "excerpt":" 1 Introduction Terraform 101 This lesson will take a deeper look at Terraform variabales. You’ve already had a little bit of exposure to them in the Experiencing Terraform tutorial, as well as a brief summary in the Understanding Terraform Concepts tutorial. Let’s dive in and take a deeper look at what these are and how they’re used. Begin &raquo; 2 Why use variables? Variables provide a way to easily decouple a value and its reference from your Terraform code. A topology may be defined, but the specifics are given programmatically (or manually). Writing parameterized Terraform code (using variables for many of the customizable values), means that as an environment changes, new values for the variables may be provided without requiring any modification to the underlying Terraform code. Here are some common use-cases for parameterizing code: Multiple deployments of the same topology – this could be for deploying separate development, staging and production environments that share identical topologies (but have unique names, compartments, CIDR blocks, etc.). Passing secret/sensitive data in via variables – rather than hard-coding credentials, keys and other sensitive data in the Terraform code, this can be passed via variables. Writing extensible modules in Terraform – particularly where variables are used as inputs to the module, determining its behavior or setting resource attributes. Warning: NEVER commit secrets, credentials, keys or any other sensitive data to a git repo! &laquo; Back Continue &raquo; 3 Types of variables There are two kinds of variables: Input variables (variables) Local values (locals) Input variables (variables) Most Terraform projects use variables in one way or another. Variables are very common, and for good reason. Variables are accessible within the entire project (or module, if you’re building projects made of one or more modules). Local values (locals) There’s another kind of variable called a local variable (aka “locals”) which is not accessible to your entire project, but is limited to the context of a single Terraform file. This means that if you define a local variable in a particular Terraform file, it will only be available to code within that same file (but not to code in other Terraform files). &laquo; Back Continue &raquo; 4 Getting the contents of a variable To use a variable, prepend var. at the beginning of the variable name. Take, for example, the var.tenancy_ocid which is used in the following VCN resource block: resource ocicorevcn tf_101 { cidr_block = \"192.168.1.0/24\" compartmentid = var.tenancyocid display_name = \"tf-101\" dns_label = \"tf101\" } This may look familiar. It’s borrowed from the Experiencing Terraform tutorial! This is how the tenancy_ocid variable is referenced (read and used). Getting the contents of a local To use a local variable, simply prepend the local variable name with local Here’s an example: resource ocicorevcn tf_101 { cidr_block = local.cidr compartmentid = var.tenancyocid display_name = \"tf-101\" dns_label = \"tf101\" } The above example sets the cidr_block attribute to whatever is in the cidr local. &laquo; Back Continue &raquo; 5 Setting variable values There are multiple ways to set the values of a variable. Terraform uses the following order of precedence when setting the value of a variable (listed from the highest to lowest priority): CLI arguments (-var and -var-file CLI parameters) *.auto.tfvars terraform.tfvars Environment variables Setting local values (locals) Local values are a little different than regular variables (which we’ll be looking at in the rest of this section). Locals (aka “local values”) are set in the Terraform code itself. Because locals are set within the Terraform code, they can be computed programmatically, granting the ability to use Terraform functions, reference to other variables, locals, and resources. Here’s an example of how locals are defined: locals { compid = len(var.compartmentid) &gt; 0 ? var.compartment_id : \"abcd.1234\" } The above example is fictitious (and not even the most efficient method of defining locals), but gives you an idea about how you’d set the compid local (variable). Locals are defined within a plural locals block, but when referenced, are singular (local.compid). Via the command-line interface Terraform supports many command-line parameters, one of which is the -var parameter which allows you to set the value of a variable when you run Terraform. Here’s an example of how you might set the compartment_id variable using the command-line. $ terraform plan -var 'compartment_id=abcd.1234' Or, alternatively, you can use the following formatting. $ terraform plan -var=\"compartment_id=abcd.1234\" You can set one or more variables using this technique. This method is a bit tedious because it can result in a really long command when running Terraform, especially if there are a lot of variables being set. This would need to be used every time you want to run Terraform! Variable files Files ending in .tfvars can be used to set the value of variables. You can tell Terraform which files to read at run-time with the -var-file parameter or you can let it auto-load files (based on their filename). If the terraform.tfvars file exists in the working directory, Terraform will read it and set the variable values that are given there. Here’s a sample terraform.tfvars file: compartmentid = \"&lt;yourcompartmentOCIDhere&gt;\" region = \"us-phoenix-1\" cidr = \"172.16.0.0/20\" These are setting variable values (note that there’s no var. prefix used). Terraform implicitly uses the terraform.tfvars file for setting variable values. The terraform.tfvars file is statically read, meaning that there’s no computation that takes place. Terraform does not reference other variables, resources, or other Terraform functions. It reads only static values. The terraform.tfvars file can often be used for setting environment-specific settings. It is usually not committed to git repos (at least alongside the Terraform code), as its values might determine the characteristics for the environment. *.auto.tfvars You can specify as many files as you’d like that end in .auto.tfvars and Terraform will gladly read these and set variable values accordingly. This allows you to set different variable definitions between different files. For instance, it might make sense to set the variable definitions as-follows (this is just an example, by no means the only way): network.auto.tfvars storage.auto.tfvars compute.auto.tfvars Grouping variable assignments like this allows a person to better navigate between variables, especially if there’s a large number of variables. Via environment variables Terraform is smart enough to look at the environment variables at run-time. If there are any variables that begin with TFVAR (the full environment variable would be: TFVAR&lt;variable_name&gt;), Terraform will assign the value to the given variable. Here’s an example of how to set the compartment_id variable on a MacOS/Linux system: export TFVARcompartmentid=&lt;yourcompartmentOCIDhere&gt; This same environment variable might be set as follows on Windows: setx TFVARcompartmentid &lt;yourcompartmentOCIDhere&gt; Via user-interactive prompts If a variable is not given a value, Terraform will resort to asking the user to provide it at run-time. Terraform cannot proceed without knowing what value to use. This is annoying and can be tedious (especially if there are lots of undefined variables), however there are times when this method might make perfect sense. In situations where the user should be asked for a value (such as for a confirmation prompt), this can be a good solution. &laquo; Back Continue &raquo; 6 Defining variables To define the existence of a variable, simply provide the following anywhere in the Terraform code: variable \"compartment_id\" {} It’s common practice to place variable definitions in a single file: variables.tf. This allows for easier management of variable definitions (having them in one place). Besides the name of the variable, there are several different attributes you can set for a variable, including (but not limited to): type (some of the common types will be discussed shortly) description (it’s nice to let people know how this variable is used) default sensitive Default values The default attribute is important to know about, as its behavior is multi-purpose. Notice how there’s no required attribute? If a variable does not have a default value, Terraform will require the variable value to be set. This means that default not only allows you to provide a default, but it can also make a variable “optional” (sort of). This is really just a side effect of providing a value. Every defined variable must have a value. Giving a default value (even if empty, such as a value of \"\" or null) keeps Terraform from “bugging” the operator, which gives the impression that it’s not required. It’s a matter of how you look at it. Some variables might best be left blank (so it’s very obvious if the user running Terraform doesn’t set it to a specific value). Oftentimes it’s nice to have “sane defaults” set so that a reasonable default value is used on a variable, minimizing the amount of inputs that must be provided. When a variable is given a default value, the default value is used unless it is overridden (with a value explicitly set). To define a default value, add the default attribute to the variable definition. variable \"compartment_id\" { default = \"abcd.1234\" } In the above example, unless a value is explicitly provided, the default value of “abcd.1234” will be used for the the compartment_id variable. Sensitive variables If you set the sensitive attribute of a variable to true, Terraform tries to minimize displaying the value to the user. This is not a guarantee that it’s not accessible to the user or that it won’t be shown on the screen. See the Terraform documentation for more information. Here’s an example: variable \"api_token\" { sensitive = true } In the above example, the visibility of the apitoken variable is minimized through the use of setting the sensitive attribute to true for the apitoken variable. &laquo; Back Continue &raquo; 7 Variable Types Strings The variables used up to this point have been string values. String values are enclosed by double-quotes (“). compartmentid=\"&lt;yourcompartmentOCIDhere&gt;\" Here’s how this is defined in variables.tf: variable tenancy_id { type = string } Strings are common, but are by no means the only kind that you can use. Numbers Numbers are numeric values that are not surrounded by quotes. numberofcomputes=10 Here’s how this might be defined in variables.tf: variable \"numberofcomputes\" { type = number } Boolean Like many other languages, Terraform supports true and false boolean values. Here’s an example of a Boolean variable being set. create_computes = true In the above example, the create_computes variable is set to true. These can be useful for many things, including specifying the desired behavior (like this example, where it might be possible to not create the computes if the value was set to false). Here’s how this might be defined in variables.tf: variable \"create_computes\" { type = bool } Lists There are times when a list is needed. Terraform lists are similar to arrays in many other languages. A Terraform list is an ordered list of values of a given type (could be string values, number values, etc.). Here’s an example of a string list. compute_names = [ \"web1\", \"web2\", \"app1\", \"app2\", \"db1\", \"db2\" ] Here’s how this might be defined in variables.tf. variable \"compute_names\" { type = list(string) } To reference a list element, use the index of the item. Terraform is zero-indexed, so the first item is index 0, the second item is index 1, and so on. Look at how you might reference the db1 value (from the above list example, being element 5, index 4): var.compute_names[4] # this equals \"db1\" Maps When the power of a key-value relationship is needed, Terraform maps are here to help! Maps are similar to hashes in some other languages, allowing you to have multiple keys, with each key having a value. Here’s an example of a map. compute_shapes = { \"web1\" = \"VM.Standard2.1\", \"web2\" = \"VM.Standard2.1\", \"app1\" = \"VM.Standard2.4\", \"app2\" = \"VM.Standard2.4\", \"db1\" = \"VM.Standard2.8\", \"db2\" = \"VM.Standard2.8\" } Here’s how this is defined in variables.tf. variable \"compute_shapes\" { type = map(string) } This could’ve been a map of numbers (instead of strings) or another valid variable type. To reference a map element, use the item key. Here’s how the db1 value might be referenced. var.compute_shapes[\"db1\"] # this equals \"VM.Standard2.8\" &laquo; Back Continue &raquo; 8 Sample Variable Definitions Here’s an example of a “toggle” variable (a boolean, which is either true or false): variable \"extra_power\" { type = bool default = true } And here’s an example of a more complex variable (taken from https://github.com/oracle-devrel/terraform-oci-ocloud-landing-zone/blob/main/component/networkdomain/input.tf): variable \"subnet\" { type = object({ cidr_block = string, prohibitpubliciponvnic = bool, dhcpoptionsid = string, routetableid = string }) description = \"Parameters for each subnet to be managed\" } In the above example, the subnet variable contains several attributes (cidrblock, prohibitpublicipon_vnic, etc.), which are of different types. This is just one example of how complex variables can be crafted. Don’t worry about this though, as complex variables are optional (you can stick with single-value variables for most use-cases). These are just a few examples. You can get really crazy with variables! Have fun with them, be creative, and remember that variables largely define the input interface for the Terraform environment. Look at the Terraform language documentation on input variables to discover some of the other variable types and Terraform variable functionality. &laquo; Back By Tim Clegg","categories": ["iac","opensource"],
        "tags": ["open-source","terraform","iac","devops","get-started"],
        "url": "/tutorials/tf-101/4-variables",
        "teaser": ""
      },{
        "title": "Creating Resources with Terraform",
        "excerpt":" 1 Introduction Terraform 101 The next few lessons in this tutorial will guide you through an adventure where resources are created, modified, and then finally destroyed using Terraform. While you got a little taste for this in the Experiencing Terraform lesson, we’ll cover the topics in greater detail here, pointing out more details along the way. Let’s start using Terraform on Oracle Cloud Infrastructure (OCI) by deploying a Virtual Cloud Network (VCN), a foundational OCI resource in which you can deploy other OCI resources to. Begin &raquo; 2 Prerequisites You should have an OCI account setup. Click here to create a new cloud account. We’ll be using the OCI Cloud Shell in this tutorial, as it provides a great platform for quickly working with Terraform (as well as many other OCI interfaces and tools), having many of these pre-installed and ready-to-go. &laquo; Back Continue &raquo; 3 Terraform code Terraform describes the resources it manages via Terraform code (also sometimes called the Terraform configuration). These are plain-text files with a .tf extension. All of the .tf files for a given project reside in a single directory. When Terraform is run, it’ll read the .tf files in the current directory. Each Terraform project should have its own directory. This gives you a great deal of flexibility as to how you structure your Terraform code. You could place all of your code in a single file, or multiple files. When a single file is used, usually it’s called main.tf. This is fine for small projects, but can be a bit difficult and tedious to navigate in large environments with many resources. When spreading the code across many different files, resources are usually grouped in a logical manner. This could be by an arbitrary category of resources, though one favorite technique is to use a file name that shares the resource name and place of all of the given type of resources in it. For example, all compartments could be defined in compartments.tf, all Security Lists be defined in security_lists.tf, etc. A key takeaway here is to think about your environment and try to identify a logical way to represent the code itself. It should be intuitive and fairly easy to navigate (find resources within the project). &laquo; Back Continue &raquo; 4 Start a new project Since this will be a new project, we’ll start by creating a new directory for this project: cd ~ mkdir tf-101 cd tf-101 For simplicity, we’ll just use a single file called main.tf. This isn’t recommended beyond a super simple environment, but that’s what we’ll be doing now, so this will work just fine. While Terraform doesn’t require any specific filename to be used for its code (beyond having a .tf or .tfvars extension), it’s common to see main.tf used when there are only a few resources. For any non-trivial environment (often with many resources), different files will be used for different types or groups of resources, which makes it easier to navigate and manage the environment. We took the approach of separating resources into their own files in the Experiencing Terraform tutorial. From within your Cloud Shell session, edit main.tf (which will also create it, as it doesn’t yet exist in this new directory): nano main.tf NOTE: GNU nano is being used as the text editor in this tutorial as it’s a simple editor to use, but feel free to use vi, vim, or your favorite editor instead! Place the following code within main.tf: terraform { required_providers { oci = { source = \"hashicorp/oci\" version = \"&gt;= 4.0.0\" } } } provider \"oci\" { region = \"us-phoenix-1\" } resource \"ocicorevcn\" \"tf_101\" { dns_label = \"tf101\" cidr_block = \"172.16.0.0/20\" compartmentid = \"&lt;yourcompartmentOCIDhere&gt;\" displayname = \"tf101\" } Make sure to update the compartment_id attributes to values you would like to use. For the most basic implementation, use your tenancy OCID (to find it, follow the directions found in the OCI documentation). In the above sample, the provider region is configured to use us-phoenix-1. Update this to the OCI region you want to deploy the VCN to. You can find a list of available OCI regions in the OCI documentation. Let’s pick apart what we’ve just done. Terraform block Terraform is built to be extensible and modular, providing a single tool for managing many different platforms. Providers increase the flexibility and power of Terraform, allowing Terraform to adapt to many different platforms that might be managed by Terraform. A provider defines a Terraform interface that is used to manage a given platform, eliminating the need for you to interact with the platform API directly. Providers translate between the Terraform interface and the underlying platform API. The terraform {} block tells Terraform which provider(s) to download from the Terraform Registry. In this case, Terraform is being instructed to download the OCI provider (hashicorp/oci, which is short for registry.terraform.io/hashicorp/oci). Terraform by default uses the public Terraform Registry, while also supporting the use of local (private) registries (something we won’t be going into in this tutorial series). The version is not required, but is a best practice to include, as functionality might differ from one provider version to another. See the Terraform provider documentation for more information. Provider Terraform uses this configuration to locate the OCI provider from the Terraform Registry.The provider uses an API Key, the default method, to authenticate to OCI. Refer to the OCI provider documentation for additional authentication methods. As mentioned previously, ensure that you specify the correct region you wish to deploy resources into. This example is using the us-phoenix-1 region, however you might need to modify this for your needs. See the OCI regions documentation for more information. Resource definition A Virtual Cloud Network (VCN) is an OCI resource that is required by many other OCI services. It’s a core, foundational resource in many OCI cloud environments, providing a logical network definition. Because it’s needed before you can deploy subnets, compute instances, load balancers, etc., you’ll start with creating a VCN. As a bonus, VCNs don’t cost a penny in OCI, so this minimizes your chances of incurring charges as you experiment with Terraform on OCI. To manage the VCN, the ocicorevcn Terraform resource is used. The example configuration creates a ocicorevcn resource named tf101, in the compartment you provide (you’ll need to make sure to use your compartment OCID), with a CIDR block of 172.16.0.0/20, a DNS label of tf101, and a displayname of tf101. Note that the Terraform resource name does not need to match the OCI display name, however it’s a good idea to keep them the same, so that no matter what interface (API, OCI Console, Terraform, etc.) you’re dealing with the same familiar name(s). Make sure to set the the compartment_id value to the OCID of the compartment you wish to use! &laquo; Back Continue &raquo; 5 Setting up OCI Authentication If we were using Terraform outside of OCI Cloud Shell, we’d need to worry about how Terraform will authenticate with the OCI API. By running it inside of Cloud Shell, we don’t need to worry about these details – it’ll be authenticated automatically for us! &laquo; Back Continue &raquo; 6 Initialize configuration Terraform needs different files to be able to be able to properly function reliably. This is largely hidden out of view so that you don’t need to be mired down in the nitty-gritty details. It’s still a good idea to have a high-level idea of what’s going on, so let’s touch on a few things that Terraform needs. Terraform doesn’t ship with any providers, so one of the first things that Terraform does is to examine any referenced providers and then download them as needed. In this case, we’ve told Terraform to use the OCI Terraform provider, but it doesn’t have this provider -— yet. This is taken care of during the Terraform initialization process. Terraform uses a state file to cache the state of resources. It compares the state file against the Terraform code base (what you want implemented) and what’s actually configured in OCI. Looking at the differences between the three sources, Terraform maps out a plan of attack around how to get the as-built (what exists in OCI) to what you want (what’s in your code). The Terraform initialization process checks for a state file. If it does not find one, it goes ahead and creates it. Let’s go ahead and initialize Terraform now for this project: $ terraform init Initializing the backend... Initializing provider plugins... - Finding latest version of hashicorp/oci... - Installing hashicorp/oci v4.45.0... - Installed hashicorp/oci v4.45.0 (unauthenticated) Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run \"terraform init\" in the future. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. $ Terraform downloaded the OCI provider and created a new state file for the environment. &laquo; Back Continue &raquo; 7 Format and validate the configuration When you first created the main.tf Terraform file, it was just a plain-text file. Terraform code is just plain text! It’s easy to create and manage Terraform code from virtually anywhere. Most languages have text formatting requirements. Though Terraform is pretty lenient around many formatting preferences, it’s best to keep your code consistently formatted, particularly the line indentations. A good text editor can help minimize this burden, but it’s still something that can be a challenge to manage (especially when working with a team of developers). Here’s what your provider section looks like: provider \"oci\" { region = \"us-phoenix-1\" } Notice how the region attribute isn’t nicely indented? This could be modified by hand, but instead let’s use Terraform’s built-in formatting command to clean-up the code: $ terraform fmt main.tf Look at the main.tf file now. See how it’s nicely indented now? $ cat main.tf # ... provider \"oci\" { region = \"us-phoenix-1\" } # ... Although there are plenty of ways to programmatically generate Terraform code, Terraform code is often created and managed by people. People make mistakes, which can be disastrous. An easy way to look for egregious Terraform code syntax violations is to use the validation built into Terraform. To see this in action, run terraform validate. $ terraform validate Success! The configuration is valid. This will tell you about syntax errors in your Terraform code. Keep in mind if you do something silly like using the wrong CIDR, incorrect name, etc., this validation won’t help you. Terraform validation will have no idea that you’ve made a mistake like that, but it is a great way to lint the Terraform code, looking for bad syntax. It’s certainly worth having in your release process (ideally using automated pipelines). &laquo; Back Continue &raquo; 8 View Terraform plan So far, Terraform has what it needs (OCI provider, state file, etc.), your code looks nicely formatted, and you have a high level of confidence that your code is “good.” There’s one more step you can take to safeguard your work. Ask Terraform what it thinks should be done to deploy the environment. This is accomplished by running terraform plan: $ terraform plan An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # ocicorevcn.example will be created + resource \"ocicorevcn\" \"tf101\" { + cidr_block = \"172.16.0.0/20\" + cidr_blocks = (known after apply) + compartment_id = \"ocid1.compartment.oc1..&lt;sanitized&gt;\" + defaultdhcpoptions_id = (known after apply) + defaultroutetable_id = (known after apply) + defaultsecuritylist_id = (known after apply) + defined_tags = (known after apply) + display_name = \"tf101\" + dns_label = \"tf101\" + freeform_tags = (known after apply) + id = (known after apply) + ipv6cidr_block = (known after apply) + ipv6publiccidrblock = (known after apply) + is_ipv6enabled = (known after apply) + state = (known after apply) + time_created = (known after apply) + vcndomainname = (known after apply) } Plan: 1 to add, 0 to change, 0 to destroy. Note: You didn't specify an \"-out\" parameter to save this plan, so Terraform can't guarantee that exactly these actions will be performed if \"terraform apply\" is subsequently run. $ Looking at the output, Terraform provides exactly what it proposes be done based on the current state of the OCI tenancy and the Terraform code you’ve provided. The + means it’ll be adding a VCN resource, which is what we expect to see! While it seems like a bit much for this simple example (creating one VCN), it’s a good habit to get into: checking what Terraform proposes be done before it makes any changes. &laquo; Back Continue &raquo; 9 Create infrastructure Run terraform apply to create the VCN you defined in main.tf: $ terraform apply An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # ocicorevcn.example will be created + resource \"ocicorevcn\" \"tf101\" { + cidr_block = \"172.16.0.0/20\" + cidr_blocks = (known after apply) + compartment_id = \"ocid1.compartment.oc1..&lt;sanitized&gt;\" + defaultdhcpoptions_id = (known after apply) + defaultroutetable_id = (known after apply) + defaultsecuritylist_id = (known after apply) + defined_tags = (known after apply) + display_name = \"tf101\" + dns_label = \"tf101\" + freeform_tags = (known after apply) + id = (known after apply) + ipv6cidr_block = (known after apply) + ipv6publiccidrblock = (known after apply) + is_ipv6enabled = (known after apply) + state = (known after apply) + time_created = (known after apply) + vcndomainname = (known after apply) } Plan: 1 to add, 0 to change, 0 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: Terraform will prompt you before creating the infrastructure. Type yes to confirm. # ... Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes ocicorevcn.example: Creating... ocicorevcn.example: Creation complete after 5s [id=ocid1.vcn.oc1.phx.&lt;sanitized&gt;] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Terraform shows essentially what we just saw when running terraform plan. Plus, it prompts you as to whether you want to proceed or not (which can give you a chance to have a final review before making the changes). Voila! That’s it. With no errors given, everything deployed successfully. Looking at the State Terraform tracks the state of each resource in the state file, including attributes that weren’t defined in the Terraform configuration. Run terraform show to view your VCN’s state. $ terraform show # ocicorevcn.example: resource \"ocicorevcn\" \"tf101\" { cidr_block = \"172.16.0.0/20\" cidr_blocks = [ \"172.16.0.0/20\", ] compartment_id = \"ocid1.compartment.oc1..&lt;sanitized&gt;\" defaultdhcpoptions_id = \"ocid1.dhcpoptions.oc1.phx.&lt;sanitized&gt;\" defaultroutetable_id = \"ocid1.routetable.oc1.phx.&lt;sanitized&gt;\" defaultsecuritylist_id = \"ocid1.securitylist.oc1.phx.&lt;sanitized&gt;\" defined_tags = {} display_name = \"tf101\" dns_label = \"tf101\" freeform_tags = {} id = \"ocid1.vcn.oc1.phx.&lt;sanitized&gt;\" state = \"AVAILABLE\" time_created = \"2021-02-09 00:08:46.373 +0000 UTC\" vcndomainname = \"tf101.oraclevcn.com\" } $ There’s more info shown here than we’d specified in the Terraform code. For example, OCI assigns an OCI Identifier (OCID) to a VCN when it’s created. The VCN OCID appears in your state file as the id attribute, even though you haven’t defined it in your configuration. Terraform provides some mechanisms to add or remove elements in the state file, but it shouldn’t be modified directly. &laquo; Back Continue &raquo; 10 Troubleshooting If terraform validate was successful and your apply still failed, you may be encountering a common error. Refer to the following troubleshooting steps to resolve your issue. Invalid Region If you see the following error when you run terraform apply, you may have used an invalid (non-existent) region name. ocicorevcn.tf_101: Creating... Error: Post https://iaas.us-does-not-exist-1.oraclecloud.com/20160918/vcns: x509: certificate signed by unknown authority on main.tf line 18, in resource \"ocicorevcn\" \"tf_101\": 18: resource \"ocicorevcn\" \"tf_101\" { Please verify you are using a valid region. You can find a list of available OCI regions in the OCI documentation. Unsubscribed Region The following error might occur if you’re trying to use a valid region, but one that your tenancy is not subscribed to. $ terraform apply # ... ocicorevcn.tf_101: Creating... Error: Service error:NotAuthenticated. The required information to complete authentication was not provided or was incorrect.. http status code: 401. Opc request id: &lt;sanitized&gt;, The service for this resource encountered an error. Please contact support for help with that service on main.tf line 18, in resource \"ocicorevcn\" \"tf_101\": 18: resource \"ocicorevcn\" \"tf_101\" { You aren’t told that it’s an invalid region. However, OCI is unable to authenticate you properly because you’re trying to talk to a region that you’re not subscribed to. Bad Resource Attribute(s) This scenario might occur when trying to set an attribute to an invalid value. $ terraform apply # ... ocicorevcn.tf_101: Creating... Error: Service error:InvalidParameter. The requested CIDR 172.16.300.0/20 is invalid: unable to parse.. http status code: 400. Opc request id: &lt;sanitized&gt; on main.tf line 18, in resource \"ocicorevcn\" \"tf_101\": 18: resource \"ocicorevcn\" \"tf_101\" { It caught the problem and returned a meaningful message! Notice how the plan (the portion before saying yes) didn’t catch this? It’s possible to set an invalid attribute value, yet not have it caught until the attribute is applied (planning might not be of help here). &laquo; Back Continue &raquo; 11 Next Steps You’ve had a chance to look at how to create new OCI resources with Terraform. Next up, you’ll learn how to modify OCI infrastructure using Terraform. Helpful Resources Terraform OCI Provider Documentation Official OCI Terraform Documentation Terraform Registry – OCI Modules Oracle (Terraform) Quick Start Oracle Terraform Modules (GitHub) &laquo; Back By Tim Clegg","categories": ["iac","opensource"],
        "tags": ["open-source","terraform","iac","devops","get-started"],
        "url": "/tutorials/tf-101/5-creating",
        "teaser": ""
      },{
        "title": "Making changes using Terraform",
        "excerpt":" 1 Introduction Terraform 101 Most DevOps environments tend to change on a regular basis. These changes can be easily managed in a reliable, scalable way using Terraform. Let’s try this out now. Begin &raquo; 2 Prerequisites I’m assuming that you’re building off of the environment found in the Creating Resources with Terraform tutorial. &laquo; Back Continue &raquo; 3 Managing Change When managing an environment with Terraform, there are largely three data sources that Terraform relies on: Terraform Code Terraform State Environment As-Built Terraform Code The Terraform code is where you tell Terraform what you want it to manage. As you’ve already seen, Terraform code is just a plain-text file with a .tf extension. Many teams find it beneficial to commit their Terraform source code to a version control system (VCS) such as a git repository. The VCS allows for limiting access to the source code (which can effectively contain your Oracle Cloud Infrastructure (OCI) topology, assuming it’s defined in Terraform code). Another benefit to using a VCS is having a natural audit trail of what has transpired in the environment (who did what and when). Terraform State The Terraform state is a critical piece of information that Terraform will use upon each execution, cross-referencing what is deployed against the code and against what is cached in the state. Additional resource data points are stored in the state, such as the OCID for a resource (after it’s been created) as well as other attributes which might not be explicitly set in the Terraform code. When working as a team, it’s important that each team member have a current, up-to-date copy of the Terraform state. To keep things consistent, only one member of the team should make changes to the environment at any single point in time. By default, the Terraform state is stored in the terraform.tfstate file located in the working directory. There are several options for managing the Terraform State, but for now we’ll be using the local backend, which happens to be the default. Environment As-Built During each run, Terraform will check the current as-built state of the environment, comparing it against the Terraform State and the provided code, to determine what might need to occur to bring the actual environment configuration into alignment with the desired end-state configuration given in the Terraform code. &laquo; Back Continue &raquo; 4 Adding a Resource To start, let’s add three new Subnets to the Virtual Cloud Network (VCN) which was created in the previous tutorial. Add the following to your main.tf file: resource \"ocicoresubnet\" \"dev_1\" { vcnid = ocicorevcn.tf101.id cidr_block = \"172.16.0.0/24\" compartmentid = \"&lt;yourcompartmentOCIDhere&gt;\" display_name = \"Dev subnet 1\" prohibitpubliciponvnic = true dns_label = \"dev\" } resource \"ocicoresubnet\" \"test\" { vcnid = ocicorevcn.tf101.id cidr_block = \"172.16.1.0/24\" compartmentid = \"&lt;yourcompartmentOCIDhere&gt;\" display_name = \"Testing subnet\" prohibitpubliciponvnic = true dns_label = \"test\" } resource \"ocicoresubnet\" \"stage\" { vcnid = ocicorevcn.tf101.id cidr_block = \"172.16.2.0/24\" compartmentid = \"&lt;yourcompartmentOCIDhere&gt;\" display_name = \"Staging subnet\" prohibitpubliciponvnic = true dns_label = \"stage\" } NOTE: Make sure to replace my compartment_id placeholder with your own. The above code adds three new subnets: one development subnet, one testing subnet and, one staging subnet. Each of these is placed in the specified compartment within the VCN that was created in the previous tutorial. Each subnet has a unique IPv4 CIDR block which falls within the broader VCN IPv4 CIDR block. None of the subnets permit public IPs to be used (they’re prohibited). Confirm that this will work as intended by examining the Terraform plan: $ terraform plan ocicorevcn.tf_101: Refreshing state... [id=ocid1.vcn.oc1.phx.&lt;sanitized&gt;] An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # ocicoresubnet.dev_1 will be created + resource \"ocicoresubnet\" \"dev_1\" { + availability_domain = (known after apply) + cidr_block = \"172.16.0.0/24\" + compartment_id = \"ocid1.compartment.oc1.&lt;sanitized&gt;\" + defined_tags = (known after apply) + dhcpoptionsid = (known after apply) + display_name = \"Dev subnet 1\" + dns_label = \"dev\" + freeform_tags = (known after apply) + id = (known after apply) + ipv6cidr_block = (known after apply) + ipv6publiccidrblock = (known after apply) + ipv6virtualrouterip = (known after apply) + prohibitpubliciponvnic = true + routetableid = (known after apply) + securitylistids = (known after apply) + state = (known after apply) + subnetdomainname = (known after apply) + time_created = (known after apply) + vcn_id = \"ocid1.vcn.oc1.phx.&lt;sanitized&gt;\" + virtualrouterip = (known after apply) + virtualroutermac = (known after apply) } # ocicoresubnet.stage will be created + resource \"ocicoresubnet\" \"stage\" { + availability_domain = (known after apply) + cidr_block = \"172.16.2.0/24\" + compartment_id = \"ocid1.compartment.oc1.&lt;sanitized&gt;\" + defined_tags = (known after apply) + dhcpoptionsid = (known after apply) + display_name = \"Staging subnet\" + dns_label = \"stage\" + freeform_tags = (known after apply) + id = (known after apply) + ipv6cidr_block = (known after apply) + ipv6publiccidrblock = (known after apply) + ipv6virtualrouterip = (known after apply) + prohibitpubliciponvnic = true + routetableid = (known after apply) + securitylistids = (known after apply) + state = (known after apply) + subnetdomainname = (known after apply) + time_created = (known after apply) + vcn_id = \"ocid1.vcn.oc1.phx.&lt;sanitized&gt;\" + virtualrouterip = (known after apply) + virtualroutermac = (known after apply) } # ocicoresubnet.test will be created + resource \"ocicoresubnet\" \"test\" { + availability_domain = (known after apply) + cidr_block = \"172.16.1.0/24\" + compartment_id = \"ocid1.compartment.oc1.&lt;sanitized&gt;\" + defined_tags = (known after apply) + dhcpoptionsid = (known after apply) + display_name = \"Testing subnet\" + dns_label = \"test\" + freeform_tags = (known after apply) + id = (known after apply) + ipv6cidr_block = (known after apply) + ipv6publiccidrblock = (known after apply) + ipv6virtualrouterip = (known after apply) + prohibitpubliciponvnic = true + routetableid = (known after apply) + securitylistids = (known after apply) + state = (known after apply) + subnetdomainname = (known after apply) + time_created = (known after apply) + vcn_id = \"ocid1.vcn.oc1.phx.&lt;sanitized&gt;\" + virtualrouterip = (known after apply) + virtualroutermac = (known after apply) } Plan: 3 to add, 0 to change, 0 to destroy. Note: You didn't specify an \"-out\" parameter to save this plan, so Terraform can't guarantee that exactly these actions will be performed if \"terraform apply\" is subsequently run. The plus sign (+) next to the three subnets listed in the above output means that Terraform is going to add three subnet resources to your environment. The existing VCN resource will remain unchanged. Being satisfied with this plan, implement it using the apply command: $ terraform apply ocicorevcn.tf_101: Refreshing state... [id=ocid1.vcn.oc1.phx.&lt;sanitized&gt;] An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # ocicoresubnet.dev_1 will be created + resource \"ocicoresubnet\" \"dev_1\" { + availability_domain = (known after apply) + cidr_block = \"172.16.0.0/24\" + compartment_id = \"ocid1.compartment.oc1..&lt;sanitized&gt;\" + defined_tags = (known after apply) + dhcpoptionsid = (known after apply) + display_name = \"Dev subnet 1\" + dns_label = \"dev\" + freeform_tags = (known after apply) + id = (known after apply) + ipv6cidr_block = (known after apply) + ipv6publiccidrblock = (known after apply) + ipv6virtualrouterip = (known after apply) + prohibitpubliciponvnic = true + routetableid = (known after apply) + securitylistids = (known after apply) + state = (known after apply) + subnetdomainname = (known after apply) + time_created = (known after apply) + vcn_id = \"ocid1.vcn.oc1.phx.&lt;sanitized&gt;\" + virtualrouterip = (known after apply) + virtualroutermac = (known after apply) } # ocicoresubnet.stage will be created + resource \"ocicoresubnet\" \"stage\" { + availability_domain = (known after apply) + cidr_block = \"172.16.2.0/24\" + compartment_id = \"ocid1.compartment.oc1..&lt;sanitized&gt;\" + defined_tags = (known after apply) + dhcpoptionsid = (known after apply) + display_name = \"Staging subnet\" + dns_label = \"stage\" + freeform_tags = (known after apply) + id = (known after apply) + ipv6cidr_block = (known after apply) + ipv6publiccidrblock = (known after apply) + ipv6virtualrouterip = (known after apply) + prohibitpubliciponvnic = true + routetableid = (known after apply) + securitylistids = (known after apply) + state = (known after apply) + subnetdomainname = (known after apply) + time_created = (known after apply) + vcn_id = \"ocid1.vcn.oc1.phx.&lt;sanitized&gt;\" + virtualrouterip = (known after apply) + virtualroutermac = (known after apply) } # ocicoresubnet.test will be created + resource \"ocicoresubnet\" \"test\" { + availability_domain = (known after apply) + cidr_block = \"172.16.1.0/24\" + compartment_id = \"ocid1.compartment.oc1.&lt;sanitized&gt;\" + defined_tags = (known after apply) + dhcpoptionsid = (known after apply) + display_name = \"Testing subnet\" + dns_label = \"test\" + freeform_tags = (known after apply) + id = (known after apply) + ipv6cidr_block = (known after apply) + ipv6publiccidrblock = (known after apply) + ipv6virtualrouterip = (known after apply) + prohibitpubliciponvnic = true + routetableid = (known after apply) + securitylistids = (known after apply) + state = (known after apply) + subnetdomainname = (known after apply) + time_created = (known after apply) + vcn_id = \"ocid1.vcn.oc1.phx.&lt;sanitized&gt;\" + virtualrouterip = (known after apply) + virtualroutermac = (known after apply) } Plan: 3 to add, 0 to change, 0 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes ocicoresubnet.stage: Creating... ocicoresubnet.test: Creating... ocicoresubnet.dev_1: Creating... ocicoresubnet.stage: Creation complete after 7s [id=ocid1.subnet.oc1.phx.&lt;sanitized&gt;] ocicoresubnet.test: Creation complete after 8s [id=ocid1.subnet.oc1.phx.&lt;sanitized&gt;] ocicoresubnet.dev_1: Still creating... [10s elapsed] ocicoresubnet.dev_1: Creation complete after 12s [id=ocid1.subnet.oc1.phx.&lt;sanitized&gt;] Apply complete! Resources: 3 added, 0 changed, 0 destroyed. Voila! Your VCN now has three subnets in it. This was a really easy way to add OCI resources to the VCN: just add to the Terraform code in main.tf and ask Terraform to apply it! &laquo; Back Continue &raquo; 5 Modifying a Resource As your fictitious environment is coming together, you realize that the name of the development subnet isn’t ideal. Rather than calling it dev_1, let’s say you want to rename it to just dev. Because Terraform uses Infrastructure-as-Code (IaC), this can be done with a few minor changes to the main.tf file, after which you just tell Terraform to apply it. It’s that easy! Modify the existing the ocicoresubnet.dev_1 resource definition in your main.tf to match the following: resource \"ocicorevcn\" \"tf_101\" { dns_label = \"tf101\" cidr_block = \"172.16.0.0/20\" compartmentid = \"&lt;yourcompartmentOCIDhere&gt;\" displayname = \"tf101\" } resource \"ocicoresubnet\" \"dev\" { vcnid = ocicorevcn.tf101.id cidr_block = \"172.16.0.0/24\" compartmentid = \"&lt;yourcompartmentOCIDhere&gt;\" display_name = \"Dev subnet\" prohibitpubliciponvnic = true dns_label = \"dev\" } resource \"ocicoresubnet\" \"test\" { vcnid = ocicorevcn.tf101.id cidr_block = \"172.16.1.0/24\" compartmentid = \"&lt;yourcompartmentOCIDhere&gt;\" display_name = \"testing subnet\" prohibitpubliciponvnic = true dns_label = \"test\" } Now look at the Terraform plan: $ terraform plan ocicorevcn.tf_101: Refreshing state... [id=ocid1.vcn.oc1.phx.&lt;sanitized&gt;] ocicoresubnet.dev_1: Refreshing state... [id=ocid1.subnet.oc1.phx.&lt;sanitized&gt;] ocicoresubnet.test: Refreshing state... [id=ocid1.subnet.oc1.phx.&lt;sanitized&gt;] ocicoresubnet.stage: Refreshing state... [id=ocid1.subnet.oc1.phx.&lt;sanitized&gt;] An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create - destroy Terraform will perform the following actions: # ocicoresubnet.dev will be created + resource \"ocicoresubnet\" \"dev\" { + availability_domain = (known after apply) + cidr_block = \"172.16.0.0/24\" + compartment_id = \"ocid1.compartment.oc1..&lt;sanitized&gt;\" + defined_tags = (known after apply) + dhcpoptionsid = (known after apply) + display_name = \"Dev subnet\" + dns_label = \"dev\" + freeform_tags = (known after apply) + id = (known after apply) + ipv6cidr_block = (known after apply) + ipv6publiccidrblock = (known after apply) + ipv6virtualrouterip = (known after apply) + prohibitpubliciponvnic = true + routetableid = (known after apply) + securitylistids = (known after apply) + state = (known after apply) + subnetdomainname = (known after apply) + time_created = (known after apply) + vcn_id = \"ocid1.vcn.oc1.phx.&lt;sanitized&gt;“ + virtualrouterip = (known after apply) + virtualroutermac = (known after apply) } # ocicoresubnet.dev_1 will be destroyed - resource \"ocicoresubnet\" \"dev_1\" { - cidr_block = \"172.16.0.0/24\" -&gt; null - compartment_id = \"ocid1.compartment.oc1..&lt;sanitized&gt;\" -&gt; null - defined_tags = {} -&gt; null - dhcpoptionsid = \"ocid1.dhcpoptions.oc1.phx.&lt;sanitized&gt;\" -&gt; null - display_name = \"Dev subnet 1\" -&gt; null - dns_label = \"dev\" -&gt; null - freeform_tags = {} -&gt; null - id = \"ocid1.subnet.oc1.phx.&lt;sanitized&gt;\" -&gt; null - prohibitpubliciponvnic = true -&gt; null - routetableid = \"ocid1.routetable.oc1.phx.&lt;sanitized&gt;\" -&gt; null - securitylistids = [ - \"ocid1.securitylist.oc1.phx.&lt;sanitized&gt;\", ] -&gt; null - state = \"AVAILABLE\" -&gt; null - subnetdomainname = \"dev.example.oraclevcn.com\" -&gt; null - time_created = \"2021-02-09 20:22:38.422 +0000 UTC\" -&gt; null - vcn_id = \"ocid1.vcn.oc1.phx.&lt;sanitized&gt;\" -&gt; null - virtualrouterip = \"172.16.0.1\" -&gt; null - virtualroutermac = \"00:11:...:22:33\" -&gt; null } Plan: 1 to add, 0 to change, 1 to destroy. Note: You didn't specify an \"-out\" parameter to save this plan, so Terraform can't guarantee that exactly these actions will be performed if \"terraform apply\" is subsequently run. Oh no - it looks like something has gone terribly awry! Terraform wants to destroy the dev_1 subnet and create a new dev subnet. Let’s find out why. Why is a Resource Being Deleted-and-Re-Created? There are two primary reasons why a resource might be destroyed and re-created: platform (cloud) constraints renaming a resource name In OCI, many changes are non-disruptive. However, there are some resource changes that might be disruptive. Take for instance the dnslabel attribute. According to the OCI provider documentation for the Subnet resource, the dnslabel cannot be changed. This would mean that if you wanted to change it on an existing OCI Subnet, you could see this delete-and-re-create behavior. Terraform is smart enough to know that if an attribute cannot be updated in-place, then it must delete and re-create the resource. The second situation occurs when you totally change the name of a resource in the Terraform code and don’t ask Terraform to update the Terraform state. From Terraform’s perspective, it looks like you want to delete the old resource (which is present in the Terraform state, but does not exist in the code) and create a net-new resource (because your code refers to a resource that’s non-existent in the Terraform state). This can be a common mistake and be troublesome, but it can be easily resolved. Rename the resource name in the Terraform state from the old name to the new name. Terraform will then move forward as expected. Do this now with the terraform state mv command: $ terraform state mv ocicoresubnet.dev1 ocicore_subnet.dev Move \"ocicoresubnet.dev1\" to \"ocicore_subnet.dev\" Successfully moved 1 object(s). The above command asks Terraform to rename (move) the old resource name to a new resource name in the Terraform State. Take a look at the Terraform documentation for more information. NOTE: When referencing resources in Terraform, both the type of resource and its name must be used. This is why in the above command, ocicoresubnet.dev1 is used instead of dev1. The same resource name may be used multiple times as long as it’s used for different types of resources. For example, an OCI Route Table called dev1 could also exist without problem (referenced as ocicoreroutetable.dev_1). When choosing names for your resources, it’s unnecessary (and a bad idea) to include the type of resource in its name, because the resource type will always prefix the name when referencing it. For example, instead of using ocicoresubnet.subnetdev1, consider using ocicoresubnet.dev_1. It’s clear that this is referencing a subnet. However, the second version is a lot shorter and easier to read! Look at the Terraform plan again: $ terraform plan ocicorevcn.tf_101: Refreshing state... [id=ocid1.vcn.oc1.phx.&lt;sanitized&gt;] ocicoresubnet.test: Refreshing state... [id=ocid1.subnet.oc1.phx.&lt;sanitized&gt;] ocicoresubnet.stage: Refreshing state... [id=ocid1.subnet.oc1.phx.&lt;sanitized&gt;] ocicoresubnet.dev: Refreshing state... [id=ocid1.subnet.oc1.phx.&lt;sanitized&gt;] An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: ~ update in-place Terraform will perform the following actions: # ocicoresubnet.dev will be updated in-place ~ resource \"ocicoresubnet\" \"dev\" { ~ display_name = \"Dev subnet 1\" -&gt; \"Dev subnet\" id = \"ocid1.subnet.oc1.phx.&lt;sanitized&gt;\" # (15 unchanged attributes hidden) } Plan: 0 to add, 1 to change, 0 to destroy. Note: You didn't specify an \"-out\" parameter to save this plan, so Terraform can't guarantee that exactly these actions will be performed if \"terraform apply\" is subsequently run. The tilde (~) indicates an in-place change, which is what is expected for the display_name. Proceed with applying the change: $ terraform apply # ... Terraform will perform the following actions: # ocicoresubnet.dev will be updated in-place ~ resource \"ocicoresubnet\" \"dev\" { ~ display_name = \"Dev subnet 1\" -&gt; \"Dev subnet\" id = \"ocid1.subnet.oc1.phx.&lt;sanitized&gt;\" # (15 unchanged attributes hidden) } Plan: 0 to add, 1 to change, 0 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes ocicoresubnet.dev: Modifying... [id=ocid1.subnet.oc1.phx.&lt;sanitized&gt;] ocicoresubnet.dev: Modifications complete after 3s [id=ocid1.subnet.oc1.phx.&lt;sanitized&gt;] Apply complete! Resources: 0 added, 1 changed, 0 destroyed. This tutorial took a slight detour, highlighting a scenario where a resource had to be renamed in the Terraform State to achieve the desired outcome, but now your OCI environment has a VCN with three subnets in it. We’ve had fun creating new OCI resources and then making modifications to the environment. It’s now time to clean things up. We’ll explore destroying resources in the next lesson. &laquo; Back By Tim Clegg","categories": ["iac","opensource"],
        "tags": ["open-source","terraform","iac","devops","get-started"],
        "url": "/tutorials/tf-101/6-changing",
        "teaser": ""
      },{
        "title": "Destroying resources with Terraform",
        "excerpt":" 1 Introduction Terraform 101 So far, we’ve had some fun creating and changing OCI resources. Our tutorial is coming to a close though, so it’s time to remove the resources we’ve added and clean-up after ourselves. Terraform makes this amazingly easy. Let’s explore this now. Begin &raquo; 2 Removing a Resource To remove the stage subnet, we’ll first need to remove its resource definition (the following code snippet) from the main.tf file: resource \"ocicoresubnet\" \"stage\" { vcnid = ocicorevcn.tf101.id cidr_block = \"172.16.2.0/24\" compartmentid = \"&lt;yourcompartmentOCIDhere&gt;\" display_name = \"Staging subnet\" prohibitpubliciponvnic = true dns_label = \"stage\" } Look at the Terraform plan to ensure it’s correct: $ terraform plan ocicoresubnet.stage: Refreshing state... [id=ocid1.subnet.oc1.phx.&lt;sanitized&gt;] ocicorevcn.tf101: Refreshing state... [id=ocid1.vcn.oc1.phx.&lt;sanitized&gt;] ocicoresubnet.test: Refreshing state... [id=ocid1.subnet.oc1.phx.&lt;sanitized&gt;] ocicoresubnet.dev: Refreshing state... [id=ocid1.subnet.oc1.phx.&lt;sanitized&gt;] An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: - destroy Terraform will perform the following actions: # ocicoresubnet.stage will be destroyed - resource \"ocicoresubnet\" \"stage\" { - cidr_block = \"172.16.2.0/24\" -&gt; null - compartment_id = \"ocid1.compartment.oc1..&lt;sanitized&gt;\" -&gt; null - defined_tags = {} -&gt; null - dhcpoptionsid = \"ocid1.dhcpoptions.oc1.phx.&lt;sanitized&gt;\" -&gt; null - display_name = \"Staging subnet\" -&gt; null - dns_label = \"stage\" -&gt; null - freeform_tags = {} -&gt; null - id = \"ocid1.subnet.oc1.phx.&lt;sanitized&gt;\" -&gt; null - prohibitpubliciponvnic = true -&gt; null - routetableid = \"ocid1.routetable.oc1.phx.&lt;sanitized&gt;\" -&gt; null - securitylistids = [ - \"ocid1.securitylist.oc1.phx.&lt;sanitized&gt;\", ] -&gt; null - state = \"AVAILABLE\" -&gt; null - subnetdomainname = \"stage.example.oraclevcn.com\" -&gt; null - time_created = \"2021-02-09 20:22:37.634 +0000 UTC\" -&gt; null - vcn_id = \"ocid1.vcn.oc1.phx.&lt;sanitized&gt;\" -&gt; null - virtualrouterip = \"172.16.2.1\" -&gt; null - virtualroutermac = \"00:11:...:22:33\" -&gt; null } Plan: 0 to add, 0 to change, 1 to destroy. Note: You didn't specify an \"-out\" parameter to save this plan, so Terraform can't guarantee that exactly these actions will be performed if \"terraform apply\" is subsequently run. NOTE: It might feel redundant to keep looking at the output from terraform plan when the same output is given when you run terraform apply (before telling it to continue). It might feel redundant, but it’s a good habit to always review proposed changes before making them. By running the plan and then applying it, you’re forcing yourself to closely look at what’s going to happen to the environment (before it happens) giving you valuable time to stop or change what’s going to take place. The minus sign (-) in front of the ocicoresubnet.stage is how Terraform indicates it will be removing the resource from the environment (“terminated,” in OCI speak). Proceed with applying it: $ terraform apply # ... Plan: 0 to add, 0 to change, 1 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes ocicoresubnet.stage: Destroying... [id=ocid1.subnet.oc1.phx.&lt;sanitized&gt;] ocicoresubnet.stage: Destruction complete after 4s Apply complete! Resources: 0 added, 0 changed, 1 destroyed. With the stage subnet removed, the environment is a bit cleaner. Deleting and Re-creating a Resource We chose to permanently delete the stage subnet. In situations where a single resource should be destroyed and then re-created, there are a couple of options (rather than modifying the Terraform code): terraform destroy command taint a resource The terraform destroy -target=type.name command is handy. Instead of deleting the stage subnet in your code and running terraform apply, you could have run terraform destroy -target=ocicoresubnet.stage. Of course, if you don’t remove (or comment out) the code for the stage subnet, the next time you run terraform apply, it would want to re-create the stage Subnet. When a resource is “tainted,” it will be deleted and re-created. The command terraform taint type.name is how a resource is tainted. Here’s an example of how the staging subnet could’ve been tainted: terraform taint ocicoresubnet.stage (followed by terraform plan and terraform apply). The next time Terraform applies, it will delete and re-create the resource. Look at the Terraform taint command documentation for more information. &laquo; Back Continue &raquo; 3 Removing All Resources When it’s time to fully terminate (destroy) an environment, Terraform has a single command that accomplishes this. While this could be accomplished by removing resource definitions from the main.tf (Terraform code) file, that isn’t ideal. What if the environment needs to be provisioned in the future? Keep the Terraform code and use the terraform destroy command to clean-up (terminate/destroy) the environment: $ terraform destroy An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: - destroy Terraform will perform the following actions: # ocicoresubnet.dev will be destroyed - resource \"ocicoresubnet\" \"dev\" { - cidr_block = \"172.16.0.0/24\" -&gt; null - compartment_id = \"ocid1.compartment.oc1..&lt;sanitized&gt;\" -&gt; null - defined_tags = {} -&gt; null - dhcpoptionsid = \"ocid1.dhcpoptions.oc1.phx.&lt;sanitized&gt;\" -&gt; null - display_name = \"Dev subnet\" -&gt; null - dns_label = \"dev\" -&gt; null - freeform_tags = {} -&gt; null - id = \"ocid1.subnet.oc1.phx.&lt;sanitized&gt;\" -&gt; null - prohibitpubliciponvnic = true -&gt; null - routetableid = \"ocid1.routetable.oc1.phx.&lt;sanitized&gt;\" -&gt; null - securitylistids = [ - \"ocid1.securitylist.oc1.phx.&lt;sanitized&gt;\", ] -&gt; null - state = \"AVAILABLE\" -&gt; null - subnetdomainname = \"dev.example.oraclevcn.com\" -&gt; null - time_created = \"2021-02-09 20:22:38.422 +0000 UTC\" -&gt; null - vcn_id = \"ocid1.vcn.oc1.phx.&lt;sanitized&gt;\" -&gt; null - virtualrouterip = \"172.16.0.1\" -&gt; null - virtualroutermac = \"00:11:...:22:33\" -&gt; null } # ocicoresubnet.test will be destroyed - resource \"ocicoresubnet\" \"test\" { - cidr_block = \"172.16.1.0/24\" -&gt; null - compartment_id = \"ocid1.compartment.oc1..&lt;sanitized&gt;\" -&gt; null - defined_tags = {} -&gt; null - dhcpoptionsid = \"ocid1.dhcpoptions.oc1.phx.&lt;sanitized&gt;\" -&gt; null - display_name = \"Testing subnet\" -&gt; null - dns_label = \"test\" -&gt; null - freeform_tags = {} -&gt; null - id = \"ocid1.subnet.oc1.phx.&lt;sanitized&gt;\" -&gt; null - prohibitpubliciponvnic = true -&gt; null - routetableid = \"ocid1.routetable.oc1.phx.&lt;sanitized&gt;\" -&gt; null - securitylistids = [ - \"ocid1.securitylist.oc1.phx.&lt;sanitized&gt;\", ] -&gt; null - state = \"AVAILABLE\" -&gt; null - subnetdomainname = \"test.example.oraclevcn.com\" -&gt; null - time_created = \"2021-02-09 20:22:38.097 +0000 UTC\" -&gt; null - vcn_id = \"ocid1.vcn.oc1.phx.&lt;sanitized&gt;\" -&gt; null - virtualrouterip = \"172.16.1.1\" -&gt; null - virtualroutermac = \"00:11:...:22:33\" -&gt; null } # ocicorevcn.tf_101 will be destroyed - resource \"ocicorevcn\" \"example\" { - cidr_block = \"172.16.0.0/20\" -&gt; null - cidr_blocks = [ - \"172.16.0.0/20\", ] -&gt; null - compartment_id = \"ocid1.compartment.oc1..&lt;sanitized&gt;\" -&gt; null - defaultdhcpoptions_id = \"ocid1.dhcpoptions.oc1.phx.&lt;sanitized&gt;\" -&gt; null - defaultroutetable_id = \"ocid1.routetable.oc1.phx.&lt;sanitized&gt;\" -&gt; null - defaultsecuritylist_id = \"ocid1.securitylist.oc1.phx.&lt;sanitized&gt;\" -&gt; null - defined_tags = {} -&gt; null - displayname = \"tf101\" -&gt; null - dns_label = \"tf101\" -&gt; null - freeform_tags = {} -&gt; null - id = \"ocid1.vcn.oc1.phx.&lt;sanitized&gt;\" -&gt; null - state = \"AVAILABLE\" -&gt; null - time_created = \"2021-02-09 20:20:05.841 +0000 UTC\" -&gt; null - vcndomainname = \"tf101.oraclevcn.com\" -&gt; null } Plan: 0 to add, 0 to change, 3 to destroy. Do you really want to destroy all resources? Terraform will destroy all your managed infrastructure, as shown above. There is no undo. Only 'yes' will be accepted to confirm. Enter a value: yes ocicoresubnet.test: Destroying... [id=ocid1.subnet.oc1.phx.&lt;sanitized&gt;] ocicoresubnet.dev: Destroying... [id=ocid1.subnet.oc1.phx.&lt;sanitized&gt;] ocicoresubnet.test: Destruction complete after 3s ocicoresubnet.dev: Destruction complete after 4s ocicorevcn.tf_101: Destroying... [id=ocid1.vcn.oc1.phx.&lt;sanitized&gt;] ocicorevcn.tf_101: Destruction complete after 1s Destroy complete! Resources: 3 destroyed. Much like the apply command, the destroy command alerts you as to what it intends to do, prompting you to authorize it before continuing. Things are now really cleaned up. The subnets and VCN are gone. Speaking of being gone, did you notice how Terraform removed the OCI resources in the order of their dependency? It didn’t try to remove the VCN first (which would’ve failed because of the presence of the subnets), but instead destroyed the two subnets first, then destroyed the VCN. That’s part of the graph logic that Terraform applies to make managing your environment easy. Pretty slick, right? We’ve had a great time together, but this tutorial is coming to a close. Before we part, make sure to check out some of the resources offered in the next section. &laquo; Back By Tim Clegg","categories": ["iac","opensource"],
        "tags": ["open-source","terraform","iac","devops","get-started"],
        "url": "/tutorials/tf-101/7-destroying",
        "teaser": ""
      },{
        "title": "More Terraform Resources",
        "excerpt":" Terraform 101 I hope that you’ve found this tutorial series helpful and worth your time. Terraform is powerful and can make managing your cloud environments easy and efficient. While our time together has been great, we’ve only had time to scratch the surface! Before parting ways, there are several resources that I’ve curated with the hope that it’ll help expand your skills and experience around using Terraform with OCI. Without further ado: Terraform Modules We haven’t had a chance to talk about modules, but they’re a great way to extend and scale Terraform. Here are a few places to look for OCI Terraform modules: HashiCorp Terraform Module Registry github.com/oracle-devrel github.com/oracle-terraform-modules/ github.com/oracle-quickstart Terraform Projects There are Terraform projects for many common applications just waiting for you to deploy: Oracle Architecture Center github.com/oracle-devrel github.com/oracle-quickstart Terraform Documentation There’s a plethora of great documentation on the terraform.io site. Be sure to check it out and go through the documentation and learning guides! For OCI-specific documentation, check out: OCI Terraform provider Official OCI Terraform provider documentation Until we meet again, happy coding! By Tim Clegg","categories": ["iac","opensource"],
        "tags": ["open-source","terraform","iac","devops","get-started"],
        "url": "/tutorials/tf-101/8-resources",
        "teaser": ""
      },{
        "title": "About",
        "excerpt":"About DevO/DevRel. By ","categories": null,
        "tags": null,
        "url": "/about/",
        "teaser": ""
      },{
        "title": "AI/ML and Data Science",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/use-cases/ai-ml",
        "teaser": ""
      },{
        "title": "Get Started with Apache and PHP on Ubuntu and OCI",
        "excerpt":" 1 Introduction In this tutorial, you’ll use an Oracle Cloud Infrastructure (OCI) Free Tier account to set up a compute instance on the latest version of Ubuntu. Then, you’ll install PHP and an Apache web server to access your new server from the internet. This tutorial covers all the steps necessary to set up a virtual network for your host and connect that host to the internet. Key tasks include how to: Set up a compartment for your development work. Install your Ubuntu instance and connect it to your Virtual Cloud Network (VCN). Set up an Oracle Cloud Infrastructure virtual cloud network and related network services required for your host to connect to the internet. Set up ssh encryption keys to access your Ubuntu server. Configure ingress rules for your VCN. Configure Apache and PHP 7 on your instance. Connect to your instance from the internet. Here’s a simplified diagram for setting up your Linux VM. ![] ![]( &lt;figcaption&gt;&lt;/figcaption&gt; ) For additional information, see: Start for Free Launch your first Linux VM Create a VCN Begin &raquo; 2 Requirements To successfully complete this tutorial, you’ll need: An Oracle Cloud Infrastructure Free Tier account. Start for Free. A MacOS, Linux, or Windows computer with ssh support installed. &laquo; Back Continue &raquo; 3 1. Set up a Compartment for Development First, let’s configure a compartment for your development. Create a Compartment Create a compartment for the resources that you create in this tutorial. Log in to the Oracle Cloud Infrastructure Console. Open the navigation menu and click Identity &amp; Security. Under Identity, click Compartments. Click Create Compartment. Complete the following fields: Name: &lt;your-compartment-name&gt; Description: Compartment for &lt;your-description&gt;. Parent Compartment: &lt;your-tenancy&gt;(root) Click Create Compartment. Reference: Create a compartment &laquo; Back Continue &raquo; 4 2. Install your Ubuntu Linux Instance Use the Create a VM Instance wizard to create a new compute instance. The wizard does several things when installing the instance: Creates and installs a compute instance running Ubuntu Linux. Creates a VCN with the required subnet and components needed to connect your Ubuntu Linux instance to the internet. Creates an ssh key pair you use to connect to your instance. Review Installation Steps To get started installing your instance with the Create a VM Instance wizard, follow these steps: From the main landing page, select Create a VM Instance wizard. ![]( &lt;figcaption&gt;&lt;/figcaption&gt; ) The Create Compute Instance page is displayed. It has a section for Placement, Image and shape, Networking, Add SSH keys, and Boot volume. Choose the Name and Compartment. Initial Options Name: &lt;name-for-the-instance&gt; Create in compartment: &lt;your-compartment&gt; Enter a value for the name or leave the system-supplied default. Review the Placement settings, and click the Show advanced options link. Take the default values. Your data might look similar to the following: Availability domain Availability domain: AD-1 Capacity type: On-demand capacity Fault domain: Let Oracle choose the best fault domain Note: For Free Tier, use Always Free Eligible option for availability domain. Review the Image and shape settings. Select the latest Ubuntu image. Click Change Image. Select the latest Ubuntu image. Click Select Image. Your image is displayed, for example your data looks similar to the following: Image Image: Canonical Ubuntu 20.04 Image build: 2020.12.11-0 Take the default values for Shape. For example, your data should look similar to the following: Shape Shape: VM.Standard.E2.1.Micro OCPU count: 1 Memory (GB): 1 Network bandwidth (Gbps): 0.48 Note: For Free Tier, use Always Free Eligible shape options. Review the Networking settings. Take the default values provided by the wizard. Note: The following is sample data. The actual values change over time or differ in a different data center. Virtual cloud network: vcn-- Subnet: vcn-- Assign a public IPv4 address: Yes Review the Add SSH keys settings. Take the default values provided by the wizard. Select the Generate a key pair for me option. Click Save Private Key and Save Public Key to save the private and public SSH keys for this compute instance. If you want to use your own SSH keys, select one of the options to provide your public key. Note: Put your private and public key files in a safe location. You cannot retrieve keys again after the compute instance has been created. Review the Boot volume settings. Take the default values provided by the wizard. Leave all check boxes unchecked. Click Create to create the instance. Provisioning the system might take several minutes. You have successfully created an Ubuntu Linux instance to run your Apache Web Server. &laquo; Back Continue &raquo; 5 3. Enable Internet Access The Create a VM Instance wizard automatically creates a VCN for your VM. You add an ingress rule to your subnet to allow internet connections on port 80. Create an Ingress Rule for your VCN Follow these steps to select your VCN’s public subnet and add the ingress rule. Open the navigation menu and click Networking, and then click Virtual Cloud Networks. Select the VCN you created with your compute instance. With your new VCN displayed, click &lt;your-subnet-name&gt; subnet link. The public subnet information is displayed with the Security Lists at the bottom of the page. A link to the Default Security List for your VCN is displayed. Click the Default Security List link. The default Ingress Rules for your VCN are displayed. Click Add Ingress Rules. An Add Ingress Rules dialog is displayed. Fill in the ingress rule with the following information. Stateless: Checked Source Type: CIDR Source CIDR: 0.0.0.0/0 IP Protocol: TCP Source port range: (leave-blank) Destination Port Range: 80 Description: Allow HTTP connections Click Add Ingress Rule. Now HTTP connections are allowed. Your VCN is configured for Apache server. Click Add Ingress Rule. Now HTTP connections are allowed. Your VCN is configured for Apache server. You have successfully created an ingress rule that makes your instance available from the internet. &laquo; Back Continue &raquo; 6 4. Set up Apache and PHP Next, install and configure Apache web server and PHP to run on your Ubuntu Linux instance. Install and Configure Apache and PHP To install and set up Apache and PHP, perform the following steps: Open the navigation menu and click Compute. Under Compute, click Instances. Click the link to the instance you created in the previous step. From the Instance Details page look under the Instance Access section, the Public IP Address field. Write down the public IP address the system created for you. You use this IP address to connect to your instance. Open a Terminal or Command Prompt window. Change into the directory where you stored the ssh encryption keys you created before. Connect to your instance with this SSH command. ssh -i &lt;your-private-key-file&gt; ubuntu@&lt;x.x.x.x&gt; Since you identified your public key when you created the instance, this command logs you into your instance. You can now issue sudo commands to install and start your server. Install Apache Server. sudo apt update sudo apt -y install apache2 Start Apache. sudo systemctl restart apache2 Update firewall settings. The Ubuntu firewall is disabled by default. However, you still need to update your iptablesconfiguration to allow HTTP traffic. Update iptables with the following commands: sudo iptables -I INPUT 6 -m state --state NEW -p tcp --dport 80 -j ACCEPT sudo netfilter-persistent save These commands add a rule to allow HTTP traffic and saves the changes to the iptablesconfiguration files. You can now test your server. You can test your server from the command line with curl localhost. Or, you can connect your browser to your public IP address assigned to your instance: http://&lt;x.x.x.x&gt;. The page looks similar to: ![]( &lt;figcaption&gt;&lt;/figcaption&gt; ) Install PHP 7 with the following commands. sudo apt -y install php libapache2-mod-php Verify installation and restart Apache. $ php -v $ sudo systemctl restart apache2 Add a PHP test file to your instance. Create the file: sudo vi /var/www/html/info.php In the file, input the following text and save the file: &lt;?php phpinfo(); ?&gt; Connect to http://&lt;your-public-ip-address&gt;/info.php. The browser produces a listing of PHP configuration on your instance similar to the following. ![]( &lt;figcaption&gt;&lt;/figcaption&gt; ) Note: After you are done testing, remove info.php from your system. Congratulations! You have successfully installed Apache and PHP 7 on your Oracle Cloud Infrastructure instance. &laquo; Back Continue &raquo; 7 What's Next You have successfully installed and deployed an Apache web server and PHP on Oracle Cloud Infrastructure using a Linux instance. To explore more information about development with Oracle products, check out these sites: Oracle Developers Portal Oracle Cloud Infrastructure &laquo; Back By ","categories": null,
        "tags": ["always-free","get-started","open-source","apache","php","ubuntu","nodejs","front-end"],
        "url": "/tutorials/apache-php-ubuntu-oci-installation",
        "teaser": ""
      },{
        "title": "AR/VR and Makers",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/use-cases/arvr-makers",
        "teaser": ""
      },{
        "title": "Installing and using Calico on Oracle Container Engine (OKE)",
        "excerpt":"Introduction There are many cluster networking options for Kubernetes. Two of the most popular are: Flannel and Calico. Flannel is a simple and easy way to configure a layer 3 network fabric designed for Kubernetes. It is also used by default by Oracle Container Services for use with Kubernetes (aka Kubernetes on Oracle Linux) and by Oracle Container Engine (OKE). Calico provides both a layer 3 networking and a network policy engine. Its policy engine can also be used together with Flannel. What we’ll cover This tutorial will focus on Calico. In this tutorial, you’ll install Calico for network pod policy on your OKE Cluster. You will then test your new installation. For additional information, see: Creating a Kubernetes Cluster using Terraform Terraform execution environment Additional resources for getting started with Kubernetes Before You Begin To successfully complete this tutorial, you must have the following: Requirements An Oracle Cloud Infrastructure account (required for use with Terraform). See Signing Up for Oracle Cloud Infrastructure. A MacOS, Linux, or Windows computer with ssh support installed. Installing Calico There are two routes available to you depending on how you’ve created your OKE cluster. If you’ve used Terraform in the past, you can follow the section on installing with Terraform below. Or, if you’ve previously used the cli or the Oracle Cloud Infrastructure (OCI) console, you can continue with the section on manual installation. Installing Calico when provisioning with Terraform If you’re provisioning your cluster with the terraform-oci-oke module, there is an option to automate its installation. The Calico installation script in terraform-oci-oke also handles the cases when you have more than 50 nodes in your cluster and and the number of replicas needed are calculated and scaled to accordingly. To install Calico using Terraform: Set the following variables in your terraform.tfvars file: create_bastion = \"true\" install_calico = \"true\" Run terraform apply: terraform apply -auto-approve Calico is now installed. Next, test your Calico installation. Manually installing Calico If you’ve manually created the OKE Cluster using the cli or the Oracle Cloud Infrastructure (OCI) console, you can use the following procedure: Obtain the kubeconfig file. Set up your KUBECONFIG environment variable: export KUBECONFIG=/path/to/kubeconfig Download the Calico policy-only manifest for the Kubernetes API datastore: curl \\ https://docs.projectcalico.org/v3.6/getting-started/kubernetes/installation/hosted/kubernetes-datastore/policy-only/1.7/calico.yaml \\ -O Set a POD_CID environment varible. By default, the pod CIDR block on OKE is 10.244.0.0/16. To set this as an environment variable, use: export POD_CID=\"10.244.0.0/16\" Replace the default pod CIDR block value (192.168.0.0/16) in the calico.yaml file. You can skip this step if your pod CIDR block is already set to 192.168.0.0/16. sed -i -e \"s?192.168.0.0/16?$POD_CIDR?g\" calico.yaml [ OKE cluster with more than 50 worker nodes only ] If your cluster consists of more than 50 worker nodes, then you need to do one additional step: sed -i -e 's/typhaservicename:\\s\"none\"/typhaservicename: calico-typha/g' calico.yaml Apply the manifest: kubectl apply -f calico.yaml [ Recommended ] Calico also recommends a minimum of 3 replicas in production environment and 1 replica per every 200 nodes: kubectl -n kube-system scale --current-replicas=1 --replicas=3 deployment/calico-typha The installation steps and other recommendations can be viewed on the Calico website. Testing Calico If you want to dive right in and test Calico as a network pod policy engine, there are some excellent recipes ready and available for you. You should be able to take any of these for a spin. Alternatively, if you’d prefer a more directed approach, you can always try the security tutorials on the Calico website. What’s next Congratulations! You’ve successfully installed Calico on your OKE Cluster. To explore more information about development with Oracle products: Oracle Developers Portal Oracle Cloud Infrastructure By Ali Mukadam","categories": ["cloudapps","opensource"],
        "tags": ["oke","devops"],
        "url": "/tutorials/calico-with-oke",
        "teaser": ""
      },{
        "title": "Call a Function using API Gateway",
        "excerpt":"In this tutorial, you’ll use Oracle Functions to process data passed from an Oracle API Gateway. Then, you’ll create a Python function that uses the runtime context to extract HTTP information passed in a request. Key tasks include how to: Gather required information. Create an application for your function. Create a “Hello World!” function. Convert your function to process runtime context data. Deploy and test your function. Create an API Gateway for your function Call your function from the internet using your API Gateway. Cloud diagram For additional information, see: Start for Free Command Line Interface (CLI) Oracle Functions Oracle API Gateway Before You Begin To successfully perform this tutorial, you will need to meet: OCI Account Requirements A paid Oracle Cloud Infrastructure account. See Signing Up for Oracle Cloud Infrastructure. Your OCI account configured to support Oracle Functions development. See: Oracle Functions on Cloud Shell Quick Start Completed of one of the two Oracle Functions introduction tutorials. Functions: Get Started using Cloud Shell Functions: Get Started using the CLI After completing either one of these two tutorials: Oracle Functions is set up and configured to create applications and deploy functions. Oracle Registry is set up to store function images. Docker is logged into the Oracle Registry. The required VCN and resources needed for Oracle Functions. An API key pair and an auth token. Software Requirements Oracle CLI Python 3.6+ and pip3. Docker Engine: A Linux computer or Linux VM. See Docker engine requirements for versions, and distros supported. Docker Desktop: Available for MacOS or Windows 10. Windows 10: Windows 10 update 2004 with WSL 2 and Ubuntu or other distro installed (See: Windows Subsystem for Linux Installation Guide for Windows 10). Install Docker Desktop for Windows 10. Note: Docker includes special Linux support for WSL 2 on Windows 10 update 2004. MacOS: See Install Docker Desktop for MacOS. Oracle Cloud Shell If you use Cloud Shell, the preceding list of software is already installed. Gather required information In order to be able to complete the tutorial, we’ll start by collecting some crucial bits of information. We recommend copying the following information into your notepad. Get compartment information First, we’ll need to create a compartment. Once your compartment is created, save the compartment’s OCID and name. To get the compartment OCID from an existing compartment: Open the navigation menu and select Identity &amp; Security. Under Identity, select Compartments. Select your compartment. Select the Copy link for the OCID field. Save the compartment OCID and name. Additional information Ensure you have the following information collected for the tutorial: Compartment Name: &lt;your-compartment-name&gt; Example: my-compartment Compartment ID: &lt;compartment-OCID&gt; Example: ocid1.compartment.oc1.aaaaaaa... VCN Name: &lt;your-vcn-name&gt; Example: my-vcn Open the navigation menu and select Networking. Select Virtual Cloud Networks. From the list of networks, select your VCN. VCN Public Subnet Name: &lt;Public-Subnet-your-vcn-name&gt; Example: Public-Subnet-my-vcn Open the navigation menu and select Networking Select Virtual Cloud Networks. From the list of networks, select your VCN. Perform required configuration Here, you’ll configure everything you need for the tutorial. Create a Functions Application To create the application: Open the navigation menu and select Developer Services. Under Functions, select Applications. Select your compartment from the Compartment drop-down. Select Create Application. Fill in the form data: Name: &lt;your-app-name&gt; VCN: &lt;your-vcn-name&gt; Subnets: &lt;Public-Subnet-your-vcn-name&gt; Select Create. Your app is created. Setup Ingress Rule for HTTPS Open the navigation menu and select Networking. Select Virtual Cloud Networks. Select the name of the VCN you used to for your Oracle Functions application. With your new VCN displayed, select your Public subnet link. Note: The public subnet information is displayed with the Security Lists at the bottom of the page. Select the Default Security List link or appropriate security list link. The default Ingress Rules for your VCN are displayed. Select Add Ingress Rules. An Add Ingress Rules dialog is displayed. Fill in the ingress rule with the following information: Stateless: Checked Source Type: CIDR Source CIDR: 0.0.0.0/0 IP Protocol: TCP Source port range: (leave-blank) Destination Port Range: 443 Description: VCN for applications After all the data is entered, select Add Ingress Rules. Note: Once you select Add Ingress Rule, HTTPS connections are allowed to your public subnet. Set up a policy for API Gateway access to functions In this section, we’ll set up a policy which allows the API Gateway to invoke functions. Create a Dynamic Group for the API Gateway Open the navigation menu and select Identity &amp; Security. Under Identity, select Dynamic Groups. Select Create Dynamic Group. Fill in the following information to define your dynamic group: Name: &lt;name-for-your-dynamic-group&gt; Under Matching Rules, use Rule 1: &lt;the-rule-text&gt; Here is the sample name and the rule you need to fill out. Name: api-gtw-func-dynamic-group Under Matching Rules use Rule 1: ALL {resource.type = 'ApiGateway', resource.compartment.id = 'ocid1.compartment.&lt;your-compartment-OCID&gt;'} Select Create. Create the policy for API Gateway Open the navigation menu and select Identity &amp; Security. Under Identity, select Policies. Select Create Policy. To define your policy, fill in the following information: Name: &lt;name-for-your-policy&gt; Description: &lt;description-for policy&gt; Compartment: &lt;name-of-functions-compartment&gt; Note: This last parameter is the compartment name, not the compartment OCID. For the Policy Builder Section: Select the Customize (Advanced) link. Enter your policy in the text box. For example: Allow dynamic-group api-gtw-func-dynamic-group to use functions-family in compartment &lt;your-compartment-name&gt; Select Create. You’ve now created a policy to allow API Gateway to use Functions. Create “Hello World” Python function In this section, we’ll explore a simple, practical application. Open a terminal. Create a directory to store your functions and navigate into that directory. mkdir my-dir-name cd my-dir-name Create a Python “Hello World” function with Fn: fn init --runtime python my-func-name This command creates a directory called my-func-name containing both the function and its associated configuration files within it. Navigate into the my-func-name directory. Deploy the function, by running: fn -v deploy --app your-app-name Various messages are displayed as the Docker images are built, pushed to OCIR, and eventually deployed to Oracle Functions. Invoke the function, by running: fn invoke your-app-name my-func-name This returns: {\"message\": \"Hello World\"} Invoke the function with a parameter, by running: echo -n '{\"name\":\"Bob\"}' | fn invoke your-app-name my-func-name This echoes: {\"message\": \"Hello Bob\"} Create an API Gateway To call your function, create an API Gateway. Create the API Gateway Open the navigation menu and select Developer Services. Under API Management, select Gateways. Select your compartment from the Compartment drop-down. Select Create Gateway. Fill in the following information to define your API Gateway: Name: &lt;your-gateway-name&gt; Type: &lt;Public&gt; Compartment: &lt;your-compartment-name&gt; Virtual Cloud Network in &lt;your-vcn-name&gt;: &lt;select-your-vcn&gt; Subnet in &lt;your-compartment-name&gt;: &lt;your-public-subnet-name&gt; Select Create. Wait a few minutes for your API Gateway to be created. Create an API Deployment for your Gateway Select Deployments in the Resources section on the left-hand side of the screen. Select Create Deployment. Ensure that From Scratch is selected for the deployment type. To define your deployment, fill in the Basic Information section: Name: &lt;your-deployment-name&gt; Path Prefix (example): /v1 Compartment: &lt;your-compartment-name&gt; API Request Policies: Take default values API Logging Policies: Take default value of Information Select Next. The Routes dialog appears with Route 1 selected. To define your route, fill in the Route 1 section: Path: &lt;your-route-path&gt; Example: /http-info Methods: GET POST Type: Oracle Functions Application in &lt;your-compartment-name&gt;: Select the Functions application you created. Function Name: Select the function you created in the configuration section. Select Next. The Review dialog is displayed summarizing the choices you have made. Select Create. Your deployment is created. Select the Deployments link for your gateway. Copy the base endpoint for the deployment you created. For example: https://aaaaa.apigateway.us-ashburn-X.oci.customer-oic.com/v1 Test your API Gateway With your API Gateway and deployment created, we’re ready to test your installation. We can do this by creating a simple script with the curl command. To create the URL for curl, add your deployment path to your endpoint. Create the script file: touch gtw01.sh &amp;&amp; chmod 755 gtw01.sh Add the curl command to the script file: #!/bin/bash curl &lt;your-api-gateway-endpoint&gt;/http-info The command returns: {\"message\":\"Hello World\"} You’ve now connected your API Gateway to a boilerplate Python function! Next, you’ll update your Python function to display information passed in an HTTP request. Update Function to Access HTTP and Function Data Here, we’ll modify the boilerplate Python function to access the runtime context and display HTTP information. Review Starting Python Code If we look at the boilerplate function, the Python function looks something like this: import io import json import logging from fdk import response def handler(ctx, data: io.BytesIO = None): name = \"World\" try: body = json.loads(data.getvalue()) name = body.get(\"name\") except (Exception, ValueError) as ex: logging.getLogger().info('error parsing json payload: ' + str(ex)) logging.getLogger().info(\"Inside Python Hello World function\") return response.Response( ctx, response_data=json.dumps( {\"message\": \"Hello {0}\".format(name)}), headers={\"Content-Type\": \"application/json\"} ) Using this code as a starting point, the sections that follow convert this function into a Python function that returns HTTP and configuration data. Update Required Packages First, update the function for each of the required packages. OCI - Update the requirements.txt file for the oci package: fdk oci HTTP - Update the import statements in func.py for packages required to support the HTTP features: import io import json import oci import logging from urllib.parse import urlparse, parse_qs The oci package is required for some of the context requests. The urlparse, parse_qs packages are used for parsing. Add HTTP Request Information Remove the main body of the function. We’ll add the response method and its related code as we go. import io import json import oci import logging from urllib.parse import urlparse, parse_qs from fdk import response def handler(ctx, data: io.BytesIO = None): Add code to display HTTP information in the response. Here is the code with explanatory comments following: import io import json import oci import logging from urllib.parse import urlparse, parse_qs from fdk import response def handler(ctx, data: io.BytesIO = None): logging.getLogger().info(\"HTTP function start\") resp = {} # retrieving the request headers headers = ctx.Headers() logging.getLogger().info(\"Headers: \" + json.dumps(headers)) resp[\"Headers\"] = headers # retrieving the request body, e.g. {\"key1\":\"value\"} try: requestbody_str = data.getvalue().decode('UTF-8') if requestbody_str: resp[\"Request body\"] = json.loads(requestbody_str) else: resp[\"Request body\"] = {} except Exception as ex: print('ERROR: The request body is not JSON', ex, flush=True) raise # retrieving the request URL, e.g. \"/v1/http-info\" requesturl = ctx.RequestURL() logging.getLogger().info(\"Request URL: \" + json.dumps(requesturl)) resp[\"Request URL\"] = requesturl # retrieving query string from the request URL, e.g. {\"param1\":[\"value\"]} parsed_url = urlparse(requesturl) resp[\"Query String\"] = parseqs(parsedurl.query) logging.getLogger().info(\"Query string: \" + json.dumps(resp[\"Query String\"])) # retrieving the request method, e.g. \"POST\", \"GET\"... method = ctx.Method() if method: logging.getLogger().info(\"Request Method: \" + method) resp[\"Request Method\"] = method else: logging.getLogger().info(\"No Request Method\") resp[\"Request Method\"] = None Comments: The handler function receives system information about the current request through the ctx and data parameters. All the data is added to the resp dictionary which is eventually returned in the response. Notice that the function runtime context (ctx) contains much of the HTTP data passed from a request including: headers, request URL, and method. The data parameter returns the body of the request. Add the Function-related Data to the Response Next, we’ll retrieve Oracle Functions’ related data from the context and then return a response. Comments describing the code follow afterwards. # retrieving the function configuration resp[\"Configuration\"] = dict(ctx.Config()) logging.getLogger().info(\"Configuration: \" + json.dumps(resp[\"Configuration\"])) # retrieving the Application ID, e.g. \"ocid1.fnapp.oc1.phx.aaaaxxxx\" appid = ctx.AppID() logging.getLogger().info(\"AppID: \" + appid) resp[\"AppID\"] = appid # retrieving the Function ID, e.g. \"ocid1.fnfunc.oc1.phx.aaaaxxxxx\" fnid = ctx.FnID() logging.getLogger().info(\"FnID: \" + fnid) resp[\"FnID\"] = fnid # retrieving the Function call ID, e.g. \"01E9FE6JBW1BT0C68ZJ003KR1Q\" callid = ctx.CallID() logging.getLogger().info(\"CallID: \" + callid) resp[\"CallID\"] = callid # retrieving the Function format, e.g. \"http-stream\" fnformat = ctx.Format() logging.getLogger().info(\"Format: \" + fnformat) resp[\"Format\"] = fnformat # retrieving the Function deadline, e.g. \"2020-05-29T05:24:46Z\" deadline = ctx.Deadline() logging.getLogger().info(\"Deadline: \" + deadline) resp[\"Deadline\"] = deadline logging.getLogger().info(\"function handler end\") return response.Response( ctx, response_data=json.dumps(resp), headers={\"Content-Type\": \"application/json\"} ) Comments: Notice that all the Functions-related data is retrieved from the ctx object including: AppID, FnID, andFormat. Review Final Function Here is the final function code: import io import json import oci import logging from urllib.parse import urlparse, parse_qs from fdk import response def handler(ctx, data: io.BytesIO = None): logging.getLogger().info(\"HTTP function start\") resp = {} # retrieving the request headers headers = ctx.Headers() logging.getLogger().info(\"Headers: \" + json.dumps(headers)) resp[\"Headers\"] = headers # retrieving the request body, e.g. {\"key1\":\"value\"} try: requestbody_str = data.getvalue().decode('UTF-8') if requestbody_str: resp[\"Request body\"] = json.loads(requestbody_str) else: resp[\"Request body\"] = {} except Exception as ex: print('ERROR: The request body is not JSON', ex, flush=True) raise # retrieving the request URL, e.g. \"/v1/http-info\" requesturl = ctx.RequestURL() logging.getLogger().info(\"Request URL: \" + json.dumps(requesturl)) resp[\"Request URL\"] = requesturl # retrieving query string from the request URL, e.g. {\"param1\":[\"value\"]} parsed_url = urlparse(requesturl) resp[\"Query String\"] = parseqs(parsedurl.query) logging.getLogger().info(\"Query string: \" + json.dumps(resp[\"Query String\"])) # retrieving the request method, e.g. \"POST\", \"GET\"... method = ctx.Method() if method: logging.getLogger().info(\"Request Method: \" + method) resp[\"Request Method\"] = method else: logging.getLogger().info(\"No Request Method\") resp[\"Request Method\"] = None # retrieving the function configuration resp[\"Configuration\"] = dict(ctx.Config()) logging.getLogger().info(\"Configuration: \" + json.dumps(resp[\"Configuration\"])) # retrieving the Application ID, e.g. \"ocid1.fnapp.oc1.phx.aaaaxxxx\" appid = ctx.AppID() logging.getLogger().info(\"AppID: \" + appid) resp[\"AppID\"] = appid # retrieving the Function ID, e.g. \"ocid1.fnfunc.oc1.phx.aaaaxxxxx\" fnid = ctx.FnID() logging.getLogger().info(\"FnID: \" + fnid) resp[\"FnID\"] = fnid # retrieving the Function call ID, e.g. \"01E9FE6JBW1BT0C68ZJ003KR1Q\" callid = ctx.CallID() logging.getLogger().info(\"CallID: \" + callid) resp[\"CallID\"] = callid # retrieving the Function format, e.g. \"http-stream\" fnformat = ctx.Format() logging.getLogger().info(\"Format: \" + fnformat) resp[\"Format\"] = fnformat # retrieving the Function deadline, e.g. \"2020-05-29T05:24:46Z\" deadline = ctx.Deadline() logging.getLogger().info(\"Deadline: \" + deadline) resp[\"Deadline\"] = deadline logging.getLogger().info(\"function handler end\") return response.Response( ctx, response_data=json.dumps(resp), headers={\"Content-Type\": \"application/json\"} ) You’re all set! You’re now ready to retest your function and view the results. Create Functions Configuration Variables Oracle Functions allows you to store configuration data in your context that is available in your request. Configuration data can be stored in either an application or a function. The following commands store database information in the application context: fn config app &lt;your-app-name&gt; DB-NAME your-db-name fn config app &lt;your-app-name&gt; DB-USER your-user-name For more information, see Fn Project’s tutorial on runtime context. Test your Function Redeploy the updated function. Invoke the function to ensure that the function is working. Run your script again. To get formatted JSON output, use the jq utility which is included with the cloud shell. CLI: If you are using the CLI, install jq on your local machine: gtw01.sh | jq The data returned should look similar to: { \"Headers\": { \"host\": [ \"localhost\", \"ctr6kqbjpza5tjnzafaqpqif5i.apigateway.us-phoenix-1.oci.customer-oci.com\" ], \"user-agent\": [ \"lua-resty-http/0.14 (Lua) ngx_lua/10015\", \"curl/7.64.1\" ], \"transfer-encoding\": \"chunked\", \"content-type\": [ \"application/octet-stream\", \"application/octet-stream\" ], \"date\": \"Thu, 10 Dec 2020 01:35:43 GMT\", \"fn-call-id\": \"01ES54MAKK1BT0H50ZJ00NGX00\", \"fn-deadline\": \"2020-12-10T01:36:13Z\", \"accept\": \"*/*\", \"cdn-loop\": \"iQPgvPk4HZ74L-PRJqYw7A\", \"forwarded\": \"for=73.34.74.159\", \"x-forwarded-for\": \"73.34.74.159\", \"x-real-ip\": \"73.34.74.159\", \"fn-http-method\": \"GET\", \"fn-http-request-url\": \"/v1/http-info\", \"fn-intent\": \"httprequest\", \"fn-invoke-type\": \"sync\", \"oci-subject-id\": \"ocid1.apigateway.oc1.phx.0000000000000000000000000000000000000000000000000000\", \"oci-subject-tenancy-id\": \"ocid1.tenancy.oc1..aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\", \"oci-subject-type\": \"resource\", \"opc-request-id\": \"/A79EAB4A240E93EB226366B190A494BC/01ES54MAK21BT0H50ZJ00NGWZZ\", \"x-content-sha256\": \"47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=\", \"accept-encoding\": \"gzip\" }, \"Configuration\": { \"PATH\": \"/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\", \"HOSTNAME\": \"7747cc436a14\", \"FN_LISTENER\": \"unix:/tmp/iofs/lsnr.sock\", \"FN_CPUS\": \"125m\", \"FNLOGFRAMENAME\": \"01ES54E5RN00000000000001JF\", \"FNLOGFRAMEHDR\": \"Opc-Request-Id\", \"FN_FORMAT\": \"http-stream\", \"DB-NAME\": \"your-db-name\", \"DB-USER\": \"your-user-name\", \"FNAPPID\": \"ocid1.fnapp.oc1.phx.aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\", \"FNFNID\": \"ocid1.fnfunc.oc1.phx.aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\", \"FN_MEMORY\": \"256\", \"FN_TYPE\": \"sync\", \"OCIRESOURCEPRINCIPAL_RPST\": \"/.oci-credentials/rpst\", \"OCIRESOURCEPRINCIPALPRIVATEPEM\": \"/.oci-credentials/private.pem\", \"OCIRESOURCEPRINCIPAL_VERSION\": \"2.2\", \"OCIRESOURCEPRINCIPAL_REGION\": \"us-phoenix-1\", \"OCIREGIONMETADATA\": \"{\\\"realmDomainComponent\\\":\\\"oraclecloud.com\\\",\\\"realmKey\\\":\\\"oc1\\\",\\\"regionIdentifier\\\":\\\"us-phoenix-1\\\",\\\"regionKey\\\":\\\"PHX\\\"}\", \"LANG\": \"C.UTF-8\", \"GPG_KEY\": \"E3FF2839C048B25C084DEBE9B26995E310250568\", \"PYTHON_VERSION\": \"3.8.5\", \"PYTHONPIPVERSION\": \"20.2.2\", \"PYTHONGETPIP_URL\": \"https://github.com/pypa/get-pip/raw/5578af97f8b2b466f4cdbebe18a3ba2d48ad1434/get-pip.py\", \"PYTHONGETPIP_SHA256\": \"d4d62a0850fe0c2e6325b2cc20d818c580563de5a2038f917e3cb0e25280b4d1\", \"PYTHONPATH\": \"/function:/python\", \"HOME\": \"/home/fn\" }, \"Request body\": {}, \"Request URL\": \"/v1/http-info\", \"Query String\": {}, \"Request Method\": \"GET\", \"AppID\": \"ocid1.fnapp.oc1.phx.aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\", \"FnID\": \"ocid1.fnfunc.oc1.phx.aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\", \"CallID\": \"01ES54MAKK1BT0H50ZJ00NGX00\", \"Format\": \"http-stream\", \"Deadline\": \"2020-12-10T01:36:13Z\" } Comments: The second half of the response contains a lot of the Functions data, including: AppID, FnID, and Format. In addition, notice that in the Configuration section you’ll find the Functions-generated environment variables like FN_FORMAT and the configuration variables, DB-NAME and DB-USER. Update your script to pass headers and POST data to the script: /bin/bash curl -X POST --header \"X-MyHeader1: headerValue\" -d '{\"key1\":\"value\"}' https://aaaaa.apigateway.us-ashburn-X.oci.customer-oic.com/v1/http-info The output from the script should look similar to: { \"Headers\": { \"host\": [ \"localhost\", \"ctr6kqbjpza5tjnzafaqpqif5i.apigateway.us-phoenix-1.oci.customer-oci.com\" ], \"user-agent\": [ \"lua-resty-http/0.14 (Lua) ngx_lua/10015\", \"curl/7.64.1\" ], \"transfer-encoding\": \"chunked\", \"content-type\": [ \"application/x-www-form-urlencoded\", \"application/x-www-form-urlencoded\" ], \"date\": \"Thu, 10 Dec 2020 17:05:14 GMT\", \"fn-call-id\": \"000000000000000000000000000\", \"fn-deadline\": \"2020-12-10T17:05:44Z\", \"accept\": \"*/*\", \"cdn-loop\": \"iQPgvPk4HZ74L-PRJqYw7A\", \"content-length\": \"16\", \"forwarded\": \"for=73.34.74.159\", \"x-forwarded-for\": \"73.34.74.159\", \"x-myheader1\": \"headerValue\", \"x-real-ip\": \"73.34.74.159\", \"fn-http-method\": \"POST\", \"fn-http-request-url\": \"/v1/http-info\", \"fn-intent\": \"httprequest\", \"fn-invoke-type\": \"sync\", \"oci-subject-id\": \"ocid1.apigateway.oc1.phx.0000000000000000000000000000000000000000000000000000\", \"oci-subject-tenancy-id\": \"ocid1.tenancy.oc1..aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\", \"oci-subject-type\": \"resource\", \"opc-request-id\": \"/32DE93ED4A72B932E62460362A24DA40/01ES6STAH91BT0G48ZJ00J07ZT\", \"x-content-sha256\": \"xMAO2Qww/EVSr1CsSxtHsZu9VicSjb2EMvMmDMjZcVA=\", \"accept-encoding\": \"gzip\" }, \"Configuration\": { \"PATH\": \"/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\", \"HOSTNAME\": \"1afb03686740\", \"FN_LISTENER\": \"unix:/tmp/iofs/lsnr.sock\", \"FN_CPUS\": \"125m\", \"FNAPPID\": \"ocid1.fnapp.oc1.phx.aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\", \"FNFNID\": \"ocid1.fnfunc.oc1.phx.aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\", \"FN_MEMORY\": \"256\", \"FN_TYPE\": \"sync\", \"FN_FORMAT\": \"http-stream\", \"DB-NAME\": \"your-db-name\", \"DB-USER\": \"your-user-name\", \"FNLOGFRAMENAME\": \"01ES6SSJY600000000000000BF\", \"FNLOGFRAMEHDR\": \"Opc-Request-Id\", \"OCIRESOURCEPRINCIPAL_RPST\": \"/.oci-credentials/rpst\", \"OCIRESOURCEPRINCIPALPRIVATEPEM\": \"/.oci-credentials/private.pem\", \"OCIRESOURCEPRINCIPAL_VERSION\": \"2.2\", \"OCIRESOURCEPRINCIPAL_REGION\": \"us-phoenix-1\", \"OCIREGIONMETADATA\": \"{\\\"realmDomainComponent\\\":\\\"oraclecloud.com\\\",\\\"realmKey\\\":\\\"oc1\\\",\\\"regionIdentifier\\\":\\\"us-phoenix-1\\\",\\\"regionKey\\\":\\\"PHX\\\"}\", \"LANG\": \"C.UTF-8\", \"GPG_KEY\": \"E3FF2839C048B25C084DEBE9B26995E310250568\", \"PYTHON_VERSION\": \"3.8.5\", \"PYTHONPIPVERSION\": \"20.2.2\", \"PYTHONGETPIP_URL\": \"https://github.com/pypa/get-pip/raw/5578af97f8b2b466f4cdbebe18a3ba2d48ad1434/get-pip.py\", \"PYTHONGETPIP_SHA256\": \"d4d62a0850fe0c2e6325b2cc20d818c580563de5a2038f917e3cb0e25280b4d1\", \"PYTHONPATH\": \"/function:/python\", \"HOME\": \"/home/fn\" }, \"Request body\": { \"key1\": \"value\" }, \"Request URL\": \"/v1/http-info\", \"Query String\": {}, \"Request Method\": \"POST\", \"AppID\": \"ocid1.fnapp.oc1.phx.aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\", \"FnID\": \"ocid1.fnfunc.oc1.phx.aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\", \"CallID\": \"000000000000000000000000000\", \"Format\": \"http-stream\", \"Deadline\": \"2020-12-10T17:05:44Z\" } Comments: Note the header data and the request body data. The key/value JSON data is listed under the Request Body section. You can download the complete source code for the function from the [Oracle Function Samples site]. Congratulations! You’ve converted the boilerplate Python function into a new function that returns HTTP and Oracle Function data. The function demonstrates how data can be passed to API Gateway and processed in a function. What’s Next Let’s review everything you’ve accomplished. You successfully created an API Gateway and called a function from it. Then, in the latter part of this tutorial, you extended this function to display both HTTP and Oracle Function data. To explore more information about development with Oracle products, check out these sites: Oracle Developers Portal Oracle Cloud Infrastructure Oracle Functions Oracle API Gateway By ","categories": ["clouddev","cloudapps","enterprise"],
        "tags": ["python","oci"],
        "url": "/tutorials/call-a-function-api-oci-clouddev",
        "teaser": ""
      },{
        "title": "Deploying Cassandra in Oracle Linux",
        "excerpt":"Apache Cassandra is a scalable, open-source NoSQL distributed database known for its high availability and compatibility with Oracle Cloud Infrastructure (OCI). This quick start guides you through configuring your environment to run Cassandra in OCI. It’s fun, we promise! For more information, see: Signing Up for Oracle Cloud Infrastructure Prerequisites In order to successfully complete this tutorial, you will need to: Have deployed a VM 2.1 with Oracle Linux 7.9 (OEL7) in OCI. Have installed Oracle Linux 7.9 with pip3.6 by default. Installed Python 3.6 or higher. Have access to root, either directly or by using sudo. By default in OCI, you are connected like an “opc” user with sudo privilege. Have an Oracle Cloud Infrastructure Free Tier account. Start for Free. Getting started Let’s start with setting up the Python environment. Python setup By default, OEL7 runs Python 3. In order to prepare us for the JupyterLab installation later in the guide, we’ll use pip to install virtualenv in this next section. Install virtualenv Virtualenv enables us to create isolated sandboxes to develop Python applications without running into module or library conflicts. It’s also super easy to install: sudo pip3.6 install virtualenv With virtualenv installed, we can create a virtual environment and enable it. Create an environment myvirtualenv virtualenv -p /usr/bin/python3 myvirtualenv Activate the env source myvirtualenv/bin/activate List Python libraries in your environment Running pip3 list will show which Python models we currently have installed: (myvirtualenv) [opc@lab1 ~]$ pip3 list Which should return output similar to: Package Version ---------- ------- pip 21.1.3 setuptools 57.1.0 wheel 0.36.2 WARNING: You are using pip version 21.1.3; however, version 21.2.1 is available. You should consider upgrading via the '/home/opc/myvirtualenv/bin/python -m pip install --upgrade pip' command. Upgrade your pip environment for this virtual environment /home/opc/myvirtualenv/bin/python -m pip install --upgrade pip Jupyterlab setup pip3 install jupyterlab Install Python libraries for Machine Learning or ETL Process pip install pandas pip install pandarallel pip install dask pip install seaborn pip install matplotlib pip install plotly pip install -lxml==4.6.3 pip install selenium pip install beautifulsoup4 pip install scikit-learn Install additional Python libraries for Kafka and web server access pip install kafka-python (v2.0.0) pip install Flask pip install gunicorn Install extensions for JupyterLab environment pip install jupytercontribnbextensions jupyter contrib nbextension install --user jupyter nbextension enable execute_time/ExecuteTime Configure JupyterLab like an OEL7 Linux service Create a script to automatically instantiate and reboot JupyterLab with user opc: vi /home/opc/launchjupyterlab.sh Script for launchjupyterlab.sh Using the virtualenv, you can launch JupyterLab on a specific port (e.g., 8001) and listen via public IP: #!/bin/bash # Activate myvirtualenv Environment source myvirtualenv/bin/activate cd /home/opc if [ \"$1\" = \"start\" ] then nohup jupyter-lab --ip=0.0.0.0 --port=8001 &gt; ./nohup.log 2&gt;&amp;1 &amp; echo $! &gt; /home/opc/jupyter.pid else kill $(cat /home/opc/jupyter.pid) fi Set the script to executable mode in order to be executed from the jupyterlab service: chmod 777 /home/opc/launchjupyterlab.sh Connect to root user sudo -i Create a script to start/stop jupyterlab service vi /etc/systemd/system/jupyterlab.service Add code to launch the script launchjupyterlab.sh as an oci user [Unit] Description=Service to start jupyterlab for opc Documentation= [Service] User=opc Group=opc Type=forking WorkingDirectory=/home/opc ExecStart=/home/opc/launchjupyterlab.sh start ExecStop=/home/opc/launchjupyterlab.sh stop [Install] WantedBy=multi-user.target Test jupyterlab service systemctl start jupyterlab systemctl status jupyterlab systemctl enable jupyterlab Reboot your machine for a final check Home stretch! Reboot your machine to check if the JupyterLab script is enabled by default on port 8001. Open port 8001 to your virtual machine VM 2.1 in order to access it using your Public IP: firewall-cmd --permanent --zone=public --list-ports firewall-cmd --get-active-zones firewall-cmd --permanent --zone=public --add-port=8001/tcp firewall-cmd --reload Note: If you’re running directly on a virtual machine and have a browser installed, it should take you directly into the Jupyter environment. Connection on port 8001: http://xxx.xxx.xxx.xxx:8001/. You should now see the next Python Web environment JupyterLab. That’s it! Enjoy using Cassandra with OCI. What’s next To explore more information about development with Oracle products: Oracle Developers Portal Oracle Cloud Infrastructure By ","categories": ["modernize"],
        "tags": ["open-source","oci","back-end"],
        "url": "/tutorials/cassandra-cloud",
        "teaser": ""
      },{
        "title": "Build and Run Cloud Native Apps",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/use-cases/cloudapps",
        "teaser": ""
      },{
        "title": "Cloud-Native Software Development on OCI",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/use-cases/clouddev",
        "teaser": ""
      },{
        "title": "Oracle DevRel Slack Code of Conduct",
        "excerpt":"Welcome! This is the official Oracle DevRel Community (OCI DevRel) Public Slack. We strive for an open and welcoming community which respects all people who contribute to our open-source projects and via this Slack channel. We are committed to making the OCI DevRel Community a pleasant, knowledgeable, and enjoyable experience for everyone. As such, we are providing this Code of Conduct (CoC) to ensure all are welcome. The OCI DevRel Public Slack is a dedicated community for anyone to gather for support and collaboration around the Oracle Cloud Platform ecosystem. Started in 2021, this Slack community has grown to hundreds of active individuals from around the world who have gathered to share their knowledge and expertise. By using this Slack Channel, you agree to abide by the following Code of Conduct, the Slack Acceptable Use Policy, and the Oracle privacy policy (pinned to the #general channel). We reserve the right to update this code of conduct at any time. When updates do occur, a message will be posted in the proper channels to notify users. Expectations The OCI DevRel Public Slack is not an official support channel of Oracle. While this Slack may contain Oracle employees who are happy to assist where they can, please refrain from mentioning or messaging Oracle employees with direct questions. If you have a question in which you require direct support from Oracle, please use our official support channel or the community forums. Oracle reserves the right to enforce, interpret, and extend these guidelines for the betterment of the community. The current admins are: Caroline Caldwell, Head of Developer Marketing Global (DevAMP) Niki Chen, Operations Manager We want this to be a fun, pleasant, and harassment-free experience for everyone, regardless of gender, gender identity and expression, sexual orientation, disability, physical appearance, body size, race, or religion. We do not tolerate harassment of participants in any form. Participants who are asked to stop any harassing behavior are expected to comply immediately. Unwanted Solicitation of Services This Slack is for community discussion and support; it is not for selling products or services. It is unacceptable to message users directly offering your services unless the user has specifically requested this information. This also includes direct messages regarding employment opportunities or freelance projects. We have provided a #jobs channel inside of our Slack workspace where these kinds of messages can be posted to. Channel and User Spamming This Slack has many channels tailored toward various functionalities and many users. When posting your message, please use the appropriate channel for posting your message. It is unacceptable to copy/paste your message across multiple channels for added visibility, post the same message or similar message frequently within the same channel, and/or directly spam users. Harassment Harassment will not be tolerated. Harassment includes: Offensive comments related to gender, gender identity and expression, sexual orientation, disability, mental illness, neuro(a)typicality, physical appearance, body size, race, or religion. Unwelcome comments regarding a person’s lifestyle choices and practices, including those related to food, health, parenting, drugs, and employment. Deliberate misgendering or use of ‘dead’ or rejected names Gratuitous or off-topic sexual images or behavior in spaces where they’re not appropriate Simulated physical contact (e.g. textual descriptions like “hug” or “backrub”) without consent or after a request to stop. Threats of violence Incitement of violence towards any individual, including encouraging a person to commit suicide or to engage in self-harm Deliberate intimidation Sustained disruption of discussion Unwelcome sexual attention Continued one-on-one communication after requests to cease Publication of non-harassing private communication Reporting If you are being harassed, notice that someone else is being harassed, or have any other concerns, please contact Devrelcommunityww@oracle.com. The moderators of this email address will respond promptly. We will respect the confidentiality of reporting for the purpose of protecting victims of harassment or abuse. Consequences Participants asked to stop any harassing behavior are expected to comply immediately. If a participant engages in harassing behavior, the admins may take any action they deem appropriate, up to and including expulsion from this Slack. In Summary Treat everyone with respect and kindness. Be thoughtful in how you communicate. Be welcoming, contribute to the conversations, and enjoy yourself. If you encounter an issue, please get in contact with the Slack Admins. By ","categories": null,
        "tags": null,
        "url": "/devrel-slack/code-of-conduct",
        "teaser": ""
      },{
        "title": "Creating an E-commerce Site with Oracle Coherence CE and Micronaut",
        "excerpt":"Welcome! In this tutorial, we’ll walk through creating a stateful, microservices-based application that uses Oracle Coherence CE as a scalable embedded data store and Micronaut Framework as an application framework. Ultimately, the application we’re building is an online store that sells socks, and is based on the SockShop Microservices Demo originally written and published under Apache 2.0 license by Weaveworks. If you’re curious, check out a working demo of the original application. Demo summary: This demo still uses the original front-end implementation provided by Weaveworks, but all back-end services have been re-implemented from scratch using Micronaut Framework and Oracle Coherence in order to showcase the many features of the Coherence Micronaut integration. We also provide the implementations of the same application that uses Spring Boot or Helidon as the application framework, in case one of those is your framework of choice. Coherence Spring Sock Shop Coherence Helidon Sock Shop Topics covered in this tutorial: Local install Installing the Coherence Operator Installing a back end (Optional) Installing the back end into the sockshop namespace Scaling the back end Complete application deployment Development (extending the application) For more information, see: Signing Up for Oracle Cloud Infrastructure Getting started with OCI Cloud Shell Prerequisites To successfully complete this tutorial, you’ll need the following: An Oracle Cloud Infrastructure (OCI) Free Tier account. Start for Free. A MacOS, Linux, or Windows computer with ssh support installed. OCI Cloud Shell - It provides a host of other OCI interfaces and tools. Kustomize - Make sure that you have a newer version of kubectl that supports it (at least 1.16 or above) Architecture Before we get started, let’s take a quick look at how this is all put together. The application consists of six back-end services rewritten from the ground up on top of Micronaut, implementing the API that the legacy front-end service expects. Reference - You can find additional details for each service by following these links: Link REST API Product Catalog allows you to search product catalog and retrieve individual product details Shopping Cart allows you to manage customers’ shopping carts Orders allows customers to place orders Payment allows you to process payments Shipping allows you to ship orders and track shipments Users allows you to manage customer information and provides registration and authentication functionality for the customers Project Structure The main Sock Shop repository also contains Kubernetes deployment files for the whole application as well as a top-level POM file which allows you to easily build the whole project and import it into your favorite IDE. Getting started Kubernetes scripts depend on Kustomize, so make sure that you have a newer version of kubectl that supports it (at least 1.16 or above). The easiest way to try the demo is to use the Kubernetes deployment scripts from this repo. If you do, you can simply run the following commands from the coherence-micronaut-sockshop-sample directory. Install the Coherence Operator Install the Coherence Operator using the instructions in the Coherence Operator Quick Start documentation. Installing a back end Create a namespace in Kubernetes called sockshop: kubectl create namespace sockshop Install the back end into the sockshop namespace: kubectl --namespace sockshop apply -k k8s/coherence The -k parameter above will use kubectl with kustomize to merge all the files under the specified directory and create all Kubernetes resources defined by them, such as deployments and services for each microservice. (Optional) Install the original WeaveSocks front end Warning: There are a few important things to note about the original implementation of the the WeaveSocks front end, so keep these in mind as you try out the demo. It has a few bugs, including some security issues, and it hasn’t been actively maintained for a few years. However, if you want to deploy it to see how it interacts with our back-end services, you can follow the steps in the sections below. Install the front-end service by running the following command: kubectl apply -f k8s/optional/original-front-end.yaml --namespace sockshop Port-forward to the front-end UI using the following processes: Mac/Linux: kubectl port-forward --namespace sockshop service/front-end &lt;localPort&gt;:80 Windows: kubectl port-forward --namespace sockshop service/front-end &lt;localPort&gt;:80 Note: If you have installed into a namespace then add the --namespace option to all kubectl commands in these instructions. At this point, you should be able to access the home page for the application by pointing your browser to: http://localhost:&lt;localPort&gt;/. You should then be able to browse the product catalog, add products to shopping cart, register as a new user, place an order, and browse order history, among other actions. Once you are finished, you can clean up the environment by executing the following: kubectl delete -f k8s/optional/original-front-end.yaml --namespace sockshop kubectl delete -k k8s/coherence --namespace sockshop Scale the back end If you wish to scale the back end, use one of the following commands: Scale only the orders microservice kubectl --namespace sockshop scale coherence orders --replicas=3 Scale all the microservices $ for name in carts catalog orders payment shipping users do kubectl --namespace sockshop scale coherence $name --replicas=3 done Complete application deployment The steps in the Getting Started section showed you how to run the application locally. However, that may not be enough if you want to experiment with scaling individual services such as tracing data in Jaeger, monitoring services via Prometheus and Grafana, or making API calls directly using Swagger UI. To do all of the above, you’ll need to deploy the services into a managed Kubernetes cluster in the cloud. You can accomplish this by following the same set of steps described above (except for port forwarding, which isn’t necessary) and performing a few additional steps described more fully in the Complete Application Deployment document. Development If you want to modify the demo, follow these steps: check out the code for the project build it locally (optionally) push new container images to the repository of your choice Reference - Development section Next steps To explore more information about development with Oracle products: Oracle Developers Portal Oracle Cloud Infrastructure License The Universal Permissive License (UPL), Version 1.0 By Aleks Seovic","categories": null,
        "tags": ["analytics","oci"],
        "url": "/tutorials/coherence-micronaut-sock-shop",
        "teaser": ""
      },{
        "title": "Connecting To and Managing HeatWave on AWS",
        "excerpt":"It’s a multi-cloud world today, and that’s why MySQL HeatWave for Amazon Web Service gives you a massively parallel, high performance, in-memory query accelerator for the MySQL Database Service as a fully managed service, developed and supported by the MySQL team in Oracle. This accelerates MySQL performance by orders of magnitude for combined analytics and transactional workloads (OLAP and OLTP). Oracle designed this so developers can focus on the important things, like managing data, creating schemas, and providing highly-available applications. Oracle automates tasks such as backup, recovery, and database and operating system patching. If you’ve never heard of HeatWave, think of it as a database query accelerator with boost buttons. One of the incredible things about Oracle MySQL HeatWave is the ability to run analytics directly against your existing transactional data, so there’s no need to shuffle that data off to a separate system when you need to perform massively parallel analysis. Let’s get started! MySQL HeatWave on AWS resides in an Oracle-managed tenancy on AWS. You can access it from the browser-based HeatWave Console or from a MySQL client or application. For this article, we’ll just dip our toe in by signing in, provisioning the service, and seeing an overview of what the HeatWave Console offers. Prerequisites The MySQL HeatWave Console supports browser platforms supported by Oracle Jet, such as the following browsers and versions: Google Chrome 69 or later Safari 12.1 or later Firefox 62 or later The Console does not support Firefox Private Browsing. You’ll also need an AWS account, optionally an OCI account as well. Plus, you’ll need a DB System somewhere to connect to the HeatWave instance (this could be a HeatWave cluster in an OCI tenancy, for example). Signing Up To sign up for MySQL HeatWave on AWS (if you don’t have an Oracle account): Navigate your browser to http://cloud.mysql.com/. You are taken to the MySQL HeatWave on AWS welcome page. Click Sign Up. A field is displayed for entering your email address. Enter your email address and click Continue. You are directed to an Account Information page for creating an Oracle Cloud account, which is required to use MySQL HeatWave on AWS. NOTE: Do NOT change your Oracle Cloud Account name after provisioning the MySQL HeatWave on AWS service, as it can cause a loss of access to the MySQL HeatWave on AWS service, requiring Support assistance. Enter the required information and click Verify my email. A verification email is sent to the specified address. This may take a minute! In the verification email, click on the verification link. You are directed to a page for verifying the email address and providing initial account information. Follow the prompts. After providing the initial account information, you’ll go to a Get Started page in the Oracle Cloud Infrastructure (OCI) Console, which includes an Enable MySQL HeatWave on AWS link in the Quickstarts section of the page. Click Enable MySQL HeatWave on AWS. You are directed to a MySQL HeatWave on AWS Administration page where you are presented with a dialog that guides you through the setup process. A paid account is required to use MySQL HeatWave on AWS. Follow the prompts to complete the account upgrade process. You will be prompted to select an account type and payment method. When the upgrade process is complete, you are directed to the OCI Console. From the OCI Console navigation menu, select Databases. MySQL HeatWave on AWS appears on the Home tab under the Featured label. Under MySQL HeatWave on AWS, click Administration. You are returned to the setup dialog. Click Provision to provision MySQL HeatWave on AWS. After the provisioning operation is completed, a message appears stating that MySQL HeatWave on AWS is ready and you are presented with options to open the MySQL HeatWave console, set up users, and view billing information. For those with an existing Oracle account MySQL HeatWave on AWS requires a subscription to the US East (Ashburn) region on OCI. MySQL HeatWave on AWS is integrated with OCI in the US East (Ashburn) region for identity and access management and billing. You are billed for the MySQL HeatWave on AWS Service in the US East (Ashburn) region. Navigate your browser to http://cloud.mysql.com/. You are taken to the MySQL HeatWave on AWS welcome page. Enter your Oracle Cloud Account name and click Continue. You are directed to a Get Started page in the Oracle Cloud Infrastructure (OCI) Console, which includes an Enable MySQL HeatWave on AWS link in the Quickstarts section of the page. Click Enable MySQL HeatWave on AWS. You are directed to a MySQL HeatWave on AWS Administration page where you are presented with a dialog that guides you through the setup process. IMPORTANT: A paid account is required to use MySQL HeatWave on AWS. If you do not have a paid account, follow the prompts to compete the account upgrade process. You will be prompted to select an account type and payment method. When the upgrade process is complete, you are directed to the OCI Console. From the OCI Console navigation menu, select Databases. MySQL HeatWave on AWS appears on the Home tab under the Featured label. Under MySQL HeatWave on AWS, click Administration. You’ll go back to the setup dialog. Click Provision to provision MySQL HeatWave on AWS. After the provisioning operation is completed, a message appears stating that MySQL HeatWave on AWS is ready and you are presented with options to open the MySQL HeatWave console, set up users, and view billing information. Signing In To sign in to MySQL HeatWave on AWS, you must have: Your Oracle Cloud Account name. This is the Cloud Account name you chose during account signup or that was provided to you by an Account Administrator. In either case, you can find your Cloud Account name in your Oracle Cloud Account welcome email. Your Cloud Account user name and password. Let’s get started! Point your browser to http://cloud.mysql.com/ You are taken to the MySQL HeatWave on AWS welcome page. You’re familiar by now with this screen. Enter your Cloud Account name. Click Continue. You are directed to the Oracle Cloud Account Sign In dialog. Enter your user name and password and click Sign In. Once your user name and password are authenticated, you are directed to the HeatWave Console. Eventually you’ll need to create users and groups (for various levels of access control), and luckily you can access the Oracle Identity Cloud Service from the MySQL HEatWave on AWS Console: Sign into the HeatWave Console as an Account Administrator. From the profile menu, select Administration. You are directed to the MySQL HeatWave on AWS Administration page in the OCI Console. Select Identity Service. This will take you to the Identity section in the OCI Account Center. What’s Next? From here, you can log into the MySQL HeatWave on AWS Console and create a MySQL DB System, manage them, see HeatWave Clusters, Workspaces (where you can create and run queries), and Performance (analytics) to see how efficient HeatWave is. Connecting to a DB System in Workspaces By now you’ve got MySQL HeatWave on AWS provisioned, and you want to connect it to a DB System somewhere. We do this in the Workspaces tab in the console. A Connection Information dialog will appear, and you choose a DB System from the drop-down, enter the proper username/password combo, and click Connect. Managing HeatWave Cluster Data Also in the Workspaces tab, we can use the Console to load or unload data from a HeatWave cluster. Here’s how: In the DB System workspace, expand the Manage Data in HeatWave pane. Select the databases and tables that you want to load or unload. Databases are selected in the Databases pane. When a database is selected, the tables from the selected database appear in the Tables from selected databases pane. There’s lots to see there but we’re interested in selecting the tables we want to load or unload, so click Load into HeatWave or Unload from HeatWave. If you’re loading tables, the MySQL Auto Parallel Load tables into Heatwave dialog appears, providing a summary of the load operation about to happen. Click Load Tables to start the parallel load operation. To stop a load or unload operation, click Stop Load/Unload. NOTE: The Refresh button refreshes the page, displaying the current state of databases and tables loaded in HeatWave. Creating Backups To create a DB System backup: In the HeatWave Console, select the MySQL DB Systems tab. Under MySQL, select DB Systems. In the list of DB Systems, find the DB System you want to create a backup for, and do one of the following: Click on the row of the DB System to highlight it, and choose Create Backup from the Actions menu. Click the name of the DB System to open the DB System Details page. Click Create Backup. The Create Backup dialog is displayed. Edit the fields as required: Display Name: The name of the backup. If you do not define a name, one is generated in the format DB-System-Name - Backup - Date&amp;Time. Description: The description of the backup. If you do not define a description, one is generated in the format DB-System-Name - Manual Backup - Date&amp;Time. Click Create to create the backup. Maintenance Good news! Essential patching and maintenance of MySQL DB Systems is an automatic process. Patches of the underlying operating system, update versions (-uN) of the MySQL server, and any underlying hardware are performed during the Maintenance Window defined on the DB System. A Maintenance Window Start Time is defined for you, automatically, and can be viewed on the DB System Details page in the MySQL HeatWave on AWS Console. When maintenance is performed, your DB System’s status changes to UPDATING and the DB System may be unavailable for a short time while the maintenance completes. Such maintenance is performed infrequently, and only when absolutely necessary. This is usually for security or reliability issues. Upgrading MySQL Server Use the Console to manually upgrade the MySQL Server of your DB system. NOTE: It is recommended to perform a full backup of your DB system before upgrading. In the HeatWave Console, select the MySQL DB Systems tab. Under MySQL, select DB Systems. Find the DB system you want to upgrade, and do one of the following: Click on the row of the DB System to highlight it, and choose Edit MySQL Version from the Actions menu. If this option is not enabled, your DB system is already using the latest version of the MySQL Server. Click the name of the DB System to open the DB System Details page. Click Edit MySQL Version. If this button is not enabled, your DB system is already using the latest version of the MySQL Server. The Edit MySQL Version dialog is displayed. In the Edit MySQL Version, select the required MySQL version. Click Save Changes. The DB system enters the UPDATING state while the MySQL Server is upgraded. Managing a HeatWave Cluster When a HeatWave cluster is stopped through a stop or restart action, the data loaded in HeatWave cluster memory is lost. Starting, stopping, or restarting a HeatWave Cluster These actions have no effect on the DB System to which the HeatWave cluster is attached. However, start, stop, or restart actions on the DB System also affect the attached HeatWave cluster. When a HeatWave cluster is stopped as a result of a stop or restart action on the DB System, any data that was loaded on the HeatWave cluster must be reloaded when the HeatWave cluster is restarted. To start, stop, or restart a HeatWave cluster: In the HeatWave Console, select the HeatWave Clusters tab. In the list of HeatWave clusters, find the HeatWave cluster you want to start, stop, or restart, and do one of the following: Click on the row of the HeatWave cluster to highlight it, and choose the required action from the Actions menu. Click the name of the HeatWave cluster to open the HeatWave Cluster Details page. On this page you can stop, start, or restart the HeatWave cluster. Select one of the following actions: Start: Starts a stopped HeatWave cluster. After the HeatWave cluster is started, the Stop action is enabled and the Start option is disabled. Stop: Stops a running HeatWave cluster. After the HeatWave cluster is stopped, the Start action is enabled. Restart: Shuts down a HeatWave cluster and restarts it. Deleting a HeatWave Cluster Deleting a HeatWave cluster removes the HeatWave cluster nodes permanently. The DB System to which the HeatWave cluster is attached is unaffected. Perhaps you just want to crunch some data for a bit, while keeping costs low. Bear in mind, the cluster will need to be set up from scratch again after deletion, so there’s that. To delete a HeatWave cluster: In the HeatWave Console, select the HeatWave Clusters tab. In the list of HeatWave clusters, find the HeatWave cluster you want to delete, and do one of the following: Click on the row of the HeatWave cluster to highlight it, and choose the Delete action from the Actions menu. Click the name of the HeatWave cluster to open the HeatWave Cluster Details page. Click the Delete button. The Delete HeatWave Cluster dialog is displayed. Click Delete HeatWave cluster. That’s a top-level overview of getting your HeatWave on AWS set up, and a little bit of management. Want to know more? Join the discussion in our public Slack channel! By ","categories": ["cloudapps"],
        "tags": ["mysql","database","heatwave","aws"],
        "url": "/tutorials/connecting-managing-heatwave-aws-devrel0622",
        "teaser": ""
      },{
        "title": "Deploy a modern data lake on OCI",
        "excerpt":" What is a data lake? Simply, a data lake is a place to store both your structured and unstructured data. It’s also a great method for organizing large volumes of diverse data from diverse sources. In this article, we’ll guide you through deploying a data lake in OCI and quickly get you up and running so you can explore its many benefits! For more information, see: Signing Up for Oracle Cloud Infrastructure Getting started with Terraform Getting started with OCI Cloud Shell What is a data lake? Prerequisites In order to successfully complete this tutorial, you’ll need: An Oracle Cloud Infrastructure (OCI) Free Tier account. Start for Free. A MacOS, Linux, or Windows computer with ssh support installed. Access to the OCI Cloud Shell - It provides a great platform for quickly working with Terraform as well as a host of other OCI interfaces and tools. The OCI Resource Manager (ORM) - This Quick Start uses the ORM to make deployment easy. The ORM stack - Select the button below to download the master.zip file: Getting started After logging into the console you’ll be taken through the same steps described in the Deploy section below. NOTE: If you use this template to create another repo you’ll need to change the link for the button to point at your repo. Local Development Make sure your credentials are defined in $HOME/.oci/config file since Terraform takes takes the default value from the .oci/config file. For example: user=ocid1.user.oc1..aaaaaxxxwf3a \\ fingerprint=de:50:15:13:...:d6 \\ keyfile=/Users/shadab/.oci/ociapi_key.pem \\ tenancy=ocid1.tenancy.oc1..aaaaaaaa2txfa \\ compartment=ocid1.compartment.oc1..aaaa5pti7sq \\ region=us-ashburn-1 git clone https://github.com/oracle-quickstart/oci-datalake &amp;&amp; cd oci-datalake Initialize Initialize the Terraform provider for OCI and Random: terraform init Build Plan terraform plan -var-file=config.tfvars -out oci_datalake.out Apply terraform apply \"oci_datalake.out\" Destroy terraform destroy -var-file=config.tfvars Deploy with ORM Import the stack - Log in to OCI to import the stack: Home &gt; Solutions &amp; Platform &gt; Resource Manager &gt; Stacks &gt; Create Stack Upload stack - Upload the master.zip and provide a name and description for the stack: Configure the Stack - The UI will present the variables to the user dynamically, based on their selections. By ","categories": null,
        "tags": ["kubernetes","devops","terraform","oci"],
        "url": "/tutorials/create-modern-data-lake-oci-readme",
        "teaser": ""
      },{
        "title": "Creating a Simple Chatbot using NodeJS on OCI",
        "excerpt":"Introduction: Why this? When I started this project I wanted to make a Racter-like chatbot. For those who don’t remember, Racter was a weird little text “conversation simulator” released in the 1980s by publisher Mindscape, known for educational software. I wound up playing quite a bit on my dad’s Apple //c instead of my computer because his had a printer, and you could print out conversations – presumably to share them with friends. A literal “share sheet,” if you will. However, what I wound up doing is establishing the beginnings of a home assistant. Racter is pretty outdated tech, as it just takes nouns you say and adds them into sentences. Very much like Mad Libs, but weirder. You can actually try out Racter on the Abandonware site. Let’s face it, home assistants are much more conversational and practical these days. I have a mix of IOT devices in my home, but have lately become a fan of open source projects that integrate the disparate platforms (Amazon, Apple, and Google, primarily) and increase privacy. To start, I looked around for some existing “chatbots” and found this clever implementation by Naman Baranwall, which uses a little bit of training to choose the best response. Obviously we’ll be adding to this later, but for now, I wanted to show how to get this up and running on Oracle Cloud Infrastructure (OCI). It was honestly a lot easier than I thought it would be! But note that we’re going for the shortest distance between two points, and that’s getting the application running using NodeJS. If this were a production environment, we’d likely use bastions and some stricter access controls. All in good time! Pre-Requisites OCI Free Tier account Wait, that’s it??? You may want a GitHub account if you want to branch the project as I did for extension later, but honestly this is all so simple you’ll be amazed. Steps Create a compartment Menu: Identity and Security &gt; Instances The point is that we’re not messing about in our root compartment, just as we like to avoid mucking about in root on our personal machine, right? I created a very simple compartment and just made the parent my root, but I also used this for my Virtual Cloud Network (VCN) so I can connect later. Identity and Security is also where you would create users, groups, and all manner of access controls. But, it’s just little ol’ us, so we’ll just go in as admins. Create a VCN if you don’t have one Menu: Networking &gt; Virtual Cloud Networks This is where my advance prep of a compartment came in handy, as I’d already set up a VCN using default route tables and it has a public IP so I can ssh in later. VCN’s are really powerful, but for our purposes all we need is a subnet and a public IP to steer to, all of which is easy to set up using the tool provided. Of course, almost all of these things can be automated using something like Terraform, but we’re just testing a chatbot for ourselves today. Create a compute instance Menu: Compute &gt; Instances Here’s where things get streamlined. Oracle does a good job of choosing a general purpose shape, but we’re going to adjust it so it’s free-tier and connecting to the Internet. Instead of AMD, we’ll go with Ampere, an exceptionally good value (free-tier, remember?) just click on change shape… Also, don’t forget to change the memory to 8GB As you scroll down, you’ll see that our previous VCN is available, how handy! That means we can assign a public IP to this fellow and ssh in shortly. And to ssh in, we’ll need the private key, which I just generate right here while I’m spinning up my compute, and of course save it and the public key somewhere safe (more on this in a bit). I personally provision 50GB of storage, but you don’t have to as OCI will provision some block memory to start with. NOTE: You can save these as a stack for later use as well, which is also handy. To review, we’ve just taken 3 steps to spin up a publicly-available free-tier compute instance so we can start creating our dev environment and chances are you haven’t even finished that beverage sitting too close to your keyboard! Connecting to our compute instance It’ll take a minute for the compute instance to spin up, but when it does, the panel will show you the public IP, which you’ll need to ssh in. Menu: Compute &gt; Instances &gt; Instance Details There’s even a handy copy link! I’m using Terminal on my Mac, so I cd over to where I’m storing my private key, chmod 400 it, then: ssh -i &lt;path to private key&gt; opc@&lt;public IP address&gt; And of course, when prompted by security, you want to continue connecting, which will add you to the list of known hosts and you’re off to the races! Installing what we need As we’re using nodejs, we’ll want to install it and then create a folder for our project, then initialize a repo in that folder to install our modules. We’ll also set up some text files with code and parameters. Oracle has a nifty yum repo for NodeJS and all we need to do is install the latest NodeJS using this command: sudo yum install nodejs Navigate to your home folder, and then opc (the admin user for this VM), and create a folder for your project. Like the tutorial, I named mine chatbot_nodejs. I branched the repo beforehand so I could extend it a bit later, but either way we initialize it in that folder with npm init and use npm i node-nlp to install the appropriate modules. Note that the package.json file will look for an index.js file, and we add two scripts: train.js and index.js, plus the repo for our dependencies. Writing and testing The way all of this works is by storing a set of “intents” as questions and responses, then using the node-nlp module to weigh responses over time. The train.js file is where we have the code to actually teach our model. We also create an index.js file, which will get everything loaded and set up – it’s pretty simple right now, but has the capacity to extend itself to a more conversant home assistant in the future. As you can see in the article, we import the NlpManager from node-nlp so we can save and process what goes on, then create a new instance of the NlpManager class, read input from the terminal, send it to the manager for a response, and then display that response. Nothing too fancy just yet, but we’re laying some important groundwork. The term “intent” here is very specific, referring to the natural language processing (NLP) we’re using to train our system. This can be used in many ways, from knowing preferences to avoiding specific words to understanding what a person is saying better over time. Just like it reads, “intent” is what the person is trying to say. You can read more about intent recognition in this excellent piece by Christopher Marshall. The model needs something to start with, so we create a couple of documents in our intents folder (inside our project folder): one for “hello” and one for “goodbye.” The author creates a set of questions and answers, which can, of course, be as long as you like. I took the liberty of changing a few, including one that calls me “FNAME” as an homage to the days when I would get press releases gone horribly awry on the database side. If you know, you know. You could create many of these to accomodate frequent queries like weather, sports, news, etc., and then create data agents who fetch what you need and return it using phrases that (over time) will be weighed for preference and ultimately “converse” in a more natural way. Integrating live data is something we’d love to do here, but we’ll wait to do this another day. Once I fixed my own minor syntax errors (check those commas, people!), the whole thing worked just as expected. First you run train to get the module initialized and ready (you’ll see a bunch of timing to let you know it’s doing the work), then run start to get the chatbot chatting. Next Steps: IOT OK, maybe not the very next step, but eventually this could become my own little Siri or Alexa, running in OCI, tied to systems in my house, capable of giving me whatever information I need on request. Sort of like a certain comic book character who came to life on screen not too long ago… You can try all this for yourself by setting up a free tier account, and reviewing the article for the chatbot code. Join us on our public Slack if you do some cool stuff with it! By Victor Agreda","categories": ["frameworks","cloudapps"],
        "tags": ["open-source","oci","nodejs"],
        "url": "/tutorials/create-simple-chatbot-nodejs-oci",
        "teaser": ""
      },{
        "title": "Kubernetes - Deploy a Node Express Application",
        "excerpt":"In this tutorial, you’ll use an Oracle Cloud Infrastructure (OCI) account to set up a Kubernetes cluster. Then, you’ll deploy a Node Express application to your cluster. Key tasks include how to: Set up a Kubernetes cluster on OCI. Set up OCI CLI to access your cluster. Build a Node Express application and Docker Image. Push your image to the Oracle Cloud Infrastructure Registry (OCIR). Deploy your Node.js Docker application to your cluster. Connect to your application from the internet. For additional information, see: Kubernetes Documentation OCI Container Engine for Kubernetes OCI Container Registry Getting started with OCI Cloud Shell Prerequisites To successfully perform this tutorial, you’ll need to have the following: For Container Registry, Kubernetes, and Load Balancers A paid Oracle Cloud Infrastructure account. See: Signing Up for Oracle Cloud Infrastructure For building applications and Docker images One of the following local environments: A MacOS or Linux machine. A Windows machine with Linux support. For example: Windows Subsystem for Linux Oracle Virtual Box You have access to root either directly or using sudo. By default in OCI, you are connected as an opc user with sudo privilege. A MacOS, Linux, or Windows computer with ssh support installed. The following applications on your local environment: JDK 11 and set JAVA_HOME in .bashrc. Python 3.6.8+ and pip installer for Python 3 Kubernetes Client 1.11.9+ Apache Maven 3.0+ Docker 19.0.3+ Git 1.8+ Node.js 10+ Note: If you don’t want to set up the required applications in your local environment, you can use OCI Cloud Shell instead. The advantage of using Cloud Shell is that all the required tools to manage your application are already installed and ready to use. If you’d like to go this route, follow the steps in the Kubernetes Using Cloud Shell: Deploy a Spring Boot Application guide. Get the Applications for Linux on OCI Free Tier If you prefer your deployment without commitments or are just seeing if OCI is right for you, we’ve got you covered! You can use an OCI Free Tier Linux compute instance to manage your deployment. The following sections will tell you how to install all of the software required for this tutorial. Install a Linux Instance We’ll start off by installing a Linux VM with an Always Free compute shape on Oracle Cloud Infrastructure. Below, we’ll outline the steps for both an Oracle Linux and an Ubuntu VM. Note: For this, you’ll need a machine with ssh support to connect to your Linux instance. To install an Oracle Linux VM Follow sections 2 and 3. Section 2: if you have a paid account, choose your compute options based on your offerings. Section 4: to connect to your instance, follow steps 1-5. Skip the Apache instructions. To install an Ubuntu VM Follow sections 2 and 3. Section 2: if you have a paid account, choose compute options based on your offerings. Section 4: to connect to your instance, follow steps 1-5. Skip the Apache instructions. To update the firewall settings, in section 4, perform step 8. Install Node.js on your system Let’s start digging into the heart of this tutorial and get Node.js installed in your environment. To install Node.js and Node Package Manager (NPM), run the following commands on the appropriate system: Oracle Linux Get and install any new versions of previously-installed packages: sudo yum update Set up the Yum repo for Node.js. Install the nodejs package: sudo yum install -y oracle-nodejs-release-el7 sudo yum install -y nodejs Ubuntu Get and install any new versions of previously-installed packages: sudo apt update Install the nodejs and the npm packages: sudo apt install -y nodejs sudo apt install -y npm Verify the installation: node -v npm -v Optional setups Configure the firewall Note: If you want to perform browser-based testing of your Node application, make port 3000 available on your Linux instance. On Oracle Linux sudo firewall-cmd --add-service=http --permanent sudo firewall-cmd --add-service=https --permanent sudo firewall-cmd --reload On Ubuntu Linux sudo iptables -I INPUT 6 -m state --state NEW -p tcp --dport 3000 -j ACCEPT sudo netfilter-persistent save Create an Ingress Rule for your VCN Follow these steps to select your VCN’s public subnet and add the ingress rule: Open the navigation menu and select Networking. Select Virtual Cloud Networks. Select the VCN you created with your compute instance. With your new VCN displayed, select &lt;your-subnet-name&gt; subnet link. You will see: The public subnet information displayed with the Security Lists at the bottom of the page. A link to the Default Security List for your VCN. Select the Default Security List link. The default Ingress Rules for your VCN are displayed. Select Add Ingress Rules. An Add Ingress Rules dialog is displayed. Fill in the ingress rule with the following information: Stateless: Checked Source Type: CIDR Source CIDR: 0.0.0.0/0 IP Protocol: TCP Source port range: (leave-blank) Destination Port Range: 3000 Description: Allow HTTP connections Select Add Ingress Rule. At this point, HTTP connections are allowed and your VCN is configured for Node Express. Congratulations! You’ve successfully created an ingress rule that makes your instance available from the internet. Install Python 3 and Pip 3 Verify your current installation: python3 --version For Python 3, run the following commands: Oracle Linux: sudo yum update sudo yum install -y python3 Ubuntu: sudo apt update sudo apt install -y python3 Verify the pip installation for Python3: pip3 -V Sample output if pip for Python3 is installed: pip &lt;version&gt; from xxx/lib/python3.x/site-packages/pip (python 3.x) To install Pip for Python 3, run the following commands: Oracle Linux: sudo yum update sudo yum install -y python3-pip Ubuntu: sudo apt update sudo apt install -y python3-pip Verify the pip for Python 3 installation: pip3 -V Install Kubernetes Client Verify your current installation: kubectl version --client If you have Kubernetes, then the version is &lt;major-version&gt;.&lt;minor-version&gt;. For example, if you have Kubernetes version 1.20, you’ll get the following: version.Info{Major:\"1\", Minor:\"20\"... To install he kubectl client, refer to the following links: Install Kubernetes client on Linux Install Kubernetes client on MacOS Verify the installation: kubectl version --client Install Docker Verify your current installation: docker -v Oracle Linux sudo yum install docker-engine sudo systemctl start docker sudo systemctl enable docker Note: The last command enables Docker to start on reboots. Ubuntu Linux To install Docker on Ubuntu Linux, refer to the Get Docker guide. Verify the installation: docker -v Prepare We’re finally here! It’s time to prepare your environment to create and deploy your application. Check your Service Limits Log in to the OCI Console. Open the navigation menu and select Governance and Administration. Under Governance, select Limits, Quotas and Usage. Find your service limit for Regions Filter for the following options: Service: Regions Scope: Tenancy Resource: Subscribed region count Compartment: &lt;tenancy-name&gt; (root) Find service limit: Limit Name: subscribed-region-count Service Limit: minimum 2 Find your available Compute core count for the VM.Standard.E3.Flex shape Filter for the following options: Service: Compute Scope: &lt;first-availability-domain&gt;. Example: EMlr:US-ASHBURN-AD-1 Resource: Cores for Standard.E3.Flex and BM.Standard.E3.128 Instances Compartment: &lt;tenancy-name&gt; (root) Find available core count: Limit Name: standard-e3-core-ad-count Available: minimum 1 Repeat for Scope: &lt;second-availability-domain&gt; and &lt;third-availability-domain&gt;. Each region must have at least one core available for this shape. Note: This tutorial creates three compute instances with a VM.Standard.E3.Flex shape for the cluster nodes. To use another shape, filter for its core count. For example, for VM.Standard2.4, filter for Cores for Standard2 based VM and BM Instances and get the count. Find out if you have 50 GB of Block Volume available: Filter for the following options: Service: Block Volume Scope: &lt;first-availability-domain&gt;. Example: EMlr:US-ASHBURN-AD-1 Resource Volume Size (GB) Compartment: &lt;tenancy-name&gt; (root) Find available block volume storage: Limit Name: total-storage-gb Available: minimum 50 Repeat for Scope: &lt;second-availability-domain&gt; and &lt;third-availability-domain&gt;. Each region must have at least 50 GB of block volume available. Find out how many Flexible Load Balancers you have available Filter for the following options: Service: LBaaS Scope: &lt;your-region&gt;. Example: us-ashburn-1 Resource: &lt;blank&gt; Compartment: &lt;tenancy-name&gt; (root) Find the number of available flexible load balancers: Limit Name: lb-flexible-count Available: minimum 1 Note: This tutorial creates a load balancer with a flexible shape. To use another bandwidth, filter for its count, for example 100-Mbps bandwidth or 400-Mbps bandwidth. Reference: For a list of all shapes, see VM Standard Shapes Create an Authorization Token In the Console’s top navigation bar, select the Profile menu (your avatar). Select your username. Select Auth Tokens. Select Generate Token. Give the token a description. Select Generate Token. Copy the token and save it. Select Close. Note: It’s crucial that you’ve made sure to save your token right after you create it since you’ll have no access to it later. Gather Required Information Collect the following credential information from the OCI Console: Tenancy name: &lt;tenancy-name&gt; Select your Profile menu (your avatar) and find your Tenancy:&lt;tenancy-name&gt;. Tenancy namespace: &lt;tenancy-namespace&gt; Select your Profile menu (your avatar). Select Tenancy:&lt;tenancy-name&gt;. Copy the value for Object Storage Namespace. Note: For some accounts, tenancy name and namespace differ. Ensure that you use namespace in this tutorial. Tenancy OCID: &lt;tenancy-ocid&gt; Select your Profile menu (your avatar), then select Tenancy:&lt;tenancy-name&gt;, and copy OCID. Username: &lt;user-name&gt; Select your Profile menu (your avatar). User OCID: &lt;user-ocid&gt; Select your Profile menu (your avatar), then select User Settings, and copy OCID. Find your region information. Region: &lt;region-identifier&gt; In the Console’s top navigation bar, find your region. Example: US East (Ashburn). Find your Region Identifier from the table in Regions and Availability Domains. Example: us-ashburn-1. Region Key: &lt;region-key&gt; Find your Region Key from the table in Regions and Availability Domains. Example: iad Copy your authentication token from the Create an Authentication Token section. Auth Token: &lt;auth-token&gt; Set up OCI Command Line Interface Install a Python Virtual Environment and Wrapper The Python virtualenv creates a folder that contains all the executables and libraries for your project. The virtualenvwrapper is an extension of virtualenv. It provides a set of commands which makes working with virtual environments much easier. Not only that, it also conveniently places all of your virtual environments in one place. One particularly useful (and sanity-saving) feature is that virtualenvwrapper provides tab-completion of environment names. Install virtualenv: pip3 install --user virtualenv Install virtualenvwrapper: pip3 install --user virtualenvwrapper Find the location of the virtualenvwrapper.sh script: grep -R virtualenvwrapper.sh Example paths: Linux example: /home/ubuntu/.local/bin/virtualenvwrapper.sh MacOS example: /usr/local/bin/virtualenvwrapper.sh Configure the virtual environment wrapper in .bashrc: sudo vi .bashrc Amend the following text with the updated information noted below: # set up Python env export WORKON_HOME=~/envs export VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3 export VIRTUALENVWRAPPERVIRTUALENVARGS=' -p /usr/bin/python3 ' source &lt;path-to-virtualenvwrapper.sh&gt; Updated information: Replace &lt;path-to-virtualenvwrapper.sh&gt; with its appropriate value. Based on the location of Python3 binaries in your environment, update /usr/bin/python3 to its correct location. Save the file. Activate the commands in the current window: source ~/.bashrc Example output: virtualenvwrapper.user_scripts creating /home/ubuntu/envs/premkproject virtualenvwrapper.user_scripts creating /home/ubuntu/envs/postmkproject virtualenvwrapper.user_scripts creating /home/ubuntu/envs/initialize virtualenvwrapper.user_scripts creating /home/ubuntu/envs/premkvirtualenv virtualenvwrapper.user_scripts creating /home/ubuntu/envs/postmkvirtualenv virtualenvwrapper.user_scripts creating /home/ubuntu/envs/prermvirtualenv virtualenvwrapper.user_scripts creating /home/ubuntu/envs/postrmvirtualenv virtualenvwrapper.user_scripts creating /home/ubuntu/envs/predeactivate virtualenvwrapper.user_scripts creating /home/ubuntu/envs/postdeactivate virtualenvwrapper.user_scripts creating /home/ubuntu/envs/preactivate virtualenvwrapper.user_scripts creating /home/ubuntu/envs/postactivate Install OCI CLI Start a virtual environment: workon cli-app Confirm the name of your virtual environment. If you are, cli-app will appear immediately the left of your command prompt. Example: (cli-app) ubuntu@&lt;ubuntu-instance-name&gt;:~$ Install OCI CLI: pip3 install oci-cli Test the installation: oci --version If everything is set up correctly, the fllowing command will return the correct version: oci --help Configure the OCI CLI Enter the following command in your virtual environment: oci setup config Enter your answers from the Gather Required Information section: Location for your config [$HOME/.oci/config]: &lt;take-default&gt; User OCID: &lt;user-ocid&gt; Tenancy OCID: &lt;tenancy-ocid&gt; Region (e.g., us-ashburn-1): &lt;region-identifier&gt; OpenSSL API encryption keys - Enter the following information to set up your keys: Generate a new API Signing RSA key pair? [Y/n]: Y Directory for your keys [$HOME/.oci]: &lt;take-default&gt; Name for your key [ociapikey] &lt;take-default&gt; Note: Your private key is: ociapikey.pem Your public key is: ociapikey_public.pem. Deactivate the virtual environment: deactivate After you deactivate the virtual environment, you should notice that the (cli-app) prefix in your environment is no longer displayed. Add the Public Key to your User Account Activate the cli-app environment: workon cli-app Display the public key: cat $HOME/.oci/ociapikey_public.pem Copy the public key. Add the public key to your user account: Go to the Console. Select your Profile menu (your avatar), and then select User Settings. Select API Keys. Select Add API Key. Select Paste Public Key. Paste value from previous step, including the lines with BEGIN PUBLIC KEY and END PUBLIC KEY. Select Add. Some useful tips Quickly activate the OCI CLI - Whenever you want to use the OCI CLI, all you have to do is run: workon cli-app Quickly deactivate your current working environment - Whenever you change project names, workon deactivates your current working environment. This way, you can quickly switch between environments. Set Up a Cluster In this section, we’ll install and configure management options for your Kubernetes cluster. Later, we’ll deploy your application to this cluster. Add Compartment Policy Note: If your username is in the Administrators group, you can skip this section. If your username is not in the Administrator group, you’ll need to have your administrator add the following policy to your tenancy: allow group &lt;the-group-your-username-belongs&gt; to manage compartments in tenancy Why this added step? With this additional privilege, you’ll be able to create a compartment for all the resources in your tutorial. Steps to add the Compartment Policy In the top navigation bar, open the Profile menu. Select your username. In the left pane, select Groups. In a notepad, copy the Group Name to which your username belongs. Open the navigation menu and slect Identity &amp; Security. Under Identity, select Policies. Select your compartment from the Compartment drop-down. Select Create Policy. Fill in the following information: Name: manage-compartments Description: “Allow the group &lt;the-group-your-username-belongs&gt; to list, create, update, delete and recover compartments in the tenancy.” Compartment: &lt;your-tenancy&gt;(root) For Policy Builder, select Customize (Advanced). Paste in the following policy: allow group &lt;the-group-your-username-belongs&gt; to manage compartments in tenancy Select Create. Reference: The compartments resource-type can be found in the Verbs + Resource-Type Combinations for IAM guide. Create a Compartment Here, you’ll use your tenancy privileges to create a compartment for the resources that you create in this tutorial: Log in to the OCI Console. Open the navigation menu and select Identity &amp; Security. Under Identity, select Compartments. Select Create Compartment. Fill in the following information: Name: &lt;your-compartment-name&gt; Description: Compartment for &lt;your-description&gt;. Parent Compartment: &lt;your-tenancy&gt;(root) Select Create Compartment. Reference: Create a compartment Add Resource Policy If your username is in the Administrators group, you can skip this section. Otherwise, have your administrator add the following policy to your tenancy: allow group &lt;the-group-your-username-belongs&gt; to manage all-resources in compartment &lt;your-compartment-name&gt; With this privilege, you can manage all resources in your compartment, essentially giving you administrative rights in that compartment. Steps to add the Resource Policy Open the navigation menu and select Identity &amp; Security. Under Identity, select Policies. Select your compartment from the Compartment drop-down. Select Create Policy. Fill in the following information: Name: manage-&lt;your-compartment-name&gt;-resources Description: Allow users to list, create, update, and delete resources in &lt;your-compartment-name&gt;. Compartment: &lt;your-tenancy&gt;(root) For Policy Builder, select the following choices: Policy use cases: Compartment Management Common policy templates: Let compartment admins manage the compartment Groups: &lt;the-group-your-username-belongs&gt; Location: &lt;your-tenancy&gt;(root) Select Create. Reference: Common Policies Create a Cluster with ‘Quick Create’ Next, we’ll create a cluster with default settings and a new set of network resources through the Quick Create workflow. Sign in to the OCI Console. Open the navigation menu and select Developer Services. Under Containers &amp; Artifacts, select Kubernetes Clusters (OKE). Select Create Cluster. Select Quick Create. Select Launch Workflow. The Create Cluster dialog is displayed. Fill in the following information: Name: &lt;your-cluster-name&gt; Compartment: &lt;your-compartment-name&gt; Kubernetes Version: &lt;take-default&gt; Kubernetes API Endpoint: Public Endpoint With this selected, the Kubernetes cluster will be hosted in a public subnet with an auto-assigned public IP address. Kubernetes Worker Nodes: Private Workers With this selected, the Kubernetes worker nodes are hosted in a private subnet. Shape: VM.Standard.E3.Flex Number of Nodes: 3 Specify a custom boot volume size: Clear the check box. Select Next. Once you do, all your choices are displayed. Take a moment to review them to ensure that everything is configured correctly. Select Create Cluster. The services setup for your cluster is displayed. Select Close. And… get yourself a cup of coffee! Creating the cluster will definitely take a few minutes. When this process completes, you’ll have successfully created a Kubernetes cluster! Set Up Local Access to Your Cluster After you create a Kubernetes cluster, you’ll need to be able to reach it. In this section, we’ll set up your local system to access the cluster. Sign in to the OCI Console. Open the navigation menu and select Developer Services. Under Containers &amp; Artifacts, select Kubernetes Clusters (OKE). Select the link to &lt;your-cluster&gt;. The information about your cluster is displayed. Select Access Cluster. Select Local Access. Follow the steps provided in the dialog. We’ve included a copy of the steps here for your reference: Note: If you are not in your virtual environment, enter: workon cli-app before you run kubectl commands. Check your oci CLI version: oci -v Make a .kube directory if it doesn’t already exist: mkdir -p $HOME/.kube Create a kubeconfig file for your setup. Use the information from Access Your Cluster dialog. oci ce cluster create-kubeconfig &lt;use data from dialog&gt; Export the KUBECONFIG environment variable: export KUBECONFIG=$HOME/.kube/config Note: If you want to have the environment variable start in a new shell, then add export KUBECONFIG=$HOME/.kube/config to your ~/.bashrc file. Test your cluster configuration with the following commands: List clusters: kubectl get service Get deployment details: kubectl describe deployment Output: “No resources found in default namespace.” This is expected since no application is deployed. Get pods: kubectl get pods Output: “No resources found in default namespace.” This is expected since no application is deployed. Note: To look at a different cluster, specify a different config file on the command line. Example: kubectl --kubeconfig=&lt;/path/to/config/file&gt; With your cluster access set up, you’re finally ready to prepare your application for deployment. Build a Local Application Before we deploy your application, we’ll need to set up a few things. First, we’ll build a local application and then create a Docker image for the application. Create a Local Application Create your Node.js application. Start an OCI CLI session. Create a directory for your application. mkdir node-hello-app cd node-hello-app Create a package.json file: vi package.json In the package.json file, input the following text, updating the author and repository fields: { \"name\": \"node-hello-app\", \"version\": \"1.0.0\", \"description\": \"Node Express Hello application\", \"author\": \"Example User &lt;username@example.com&gt;\", \"main\": \"app.js\", \"scripts\": { \"start\": \"node app.js\" }, \"repository\": { \"type\": \"git\", \"url\": \"git://github.com/username/repository.git\" }, \"dependencies\": { \"express\": \"^4.0.0\" }, \"license\": \"UPL-1.0\" } Save the file. Install the NPM packages: npm install Create a “Hello, World!” application. Create the file: vi app.js In the file, input the following text: const express = require('express') const app = express() port = 3000 app.get('/', function (req, res) { res.send('&lt;h1&gt;Hello World from Node.js!&lt;/h1&gt;') }) app.listen(port, function() { console.log('Hello World app listening on port ' + port); }) You have successfully set up your Node.js app! Run the local application Let’s make sure that your application is working properly. Run your Node.js application: node app.js The Node Express server starts and displays: Hello World app listening on port 3000 Test the application: To test with curl, run: curl -X GET http://localhost:3000 To test with your browser, connect a browser window to: http://&lt;your-ip-address&gt;:3000: The page should display: &lt;h1&gt;Hello World from Node.js!&lt;/h1&gt; Stop the running application. Press Ctrl+C to stop your application in the terminal window you started with. That’s it! You’ve successfully created a Hello World application using Node.js and Express. Reference: For detailed information on this example, see Getting Started with Express. Build a Docker Image Next, create a Docker image for your Node.js Express application. Ensure that you’re in the node-hello-app directory. Build a Docker image: docker build -t node-hello-app . You should see the following message: [INFO] BUILD SUCCESS Successfully tagged node-hello-app:latest Run the Docker image: docker run --rm -p 3000:3000 node-hello-app:latest Test the application. curl -X GET http://localhost:3000 The app should return: &lt;h1&gt;Hello World from Node.js!&lt;/h1&gt; Stop the running application. Congratulations! You’ve successfully created a Node.js Express image. Deploy Your Docker Image In this section, we’ll push your Node.js Express image to OCI Container Registry and then use the image to deploy your application. Note: Before you can push a Docker image into a registry repository, the repository must exist in your compartment. If the repository does not exist, the Docker push command will not work correctly. Create a Docker Repository Open the navigation menu and select Developer Services. Under Containers &amp; Artifacts, select Container Registry. In the left navigation, select &lt;your-compartment-name&gt;. Select Create Repository. Create a private repository with your choice of repo name: &lt;repo-name&gt; = &lt;image-path-name&gt;/&lt;image-name&gt; Example: node-apps/node-hello-app Note: The slash in a repository name does not represent a hierarchical directory structure. The optional &lt;image-path-name&gt; helps to organize your repositories. You are now ready to push your local image to Container Registry. Push Your Local Image With your local Docker image created, push the image to the Container Registry. Follow these steps: Open your OCI CLI session. Log in to OCI Container Registry: docker login &lt;region-key&gt;.ocir.io You are prompted for your login name and password: Username: &lt;tenancy-namespace&gt;/&lt;user-name&gt; Password: &lt;auth-token&gt; List your local Docker images: docker images The Docker images on your system are displayed. Identify the image you created in the last section: node-hello-app Tag your local image with the URL for the registry plus the repo name, so you can push it to that repo: docker tag &lt;your-local-image&gt; &lt;repo-url&gt;/&lt;repo-name&gt; Replace &lt;repo-url&gt; with &lt;region-key&gt;.ocir.io/&lt;tenancy-namespace&gt;/ Replace &lt;repo-name&gt; with &lt;image-folder-name&gt;/&lt;image-name&gt; from the Create a Docker Repository section. Here is an example after combining both: docker tag node-hello-app iad.ocir.io/my-namespace/node-apps/node-hello-app In this example, the components are: Repo URL: iad.ocir.io/my-namespace/ Repo name: node-apps/node-hello-app Note: OCI Container Registry now supports creating a registry repo in any compartment rather than only in the root compartment (tenancy). To push the image to the repo you created, combine the registry URL with the exact repo name. OCI Container Registry matches based on the unique repo name and pushes your image. Check your Docker images to see if the image is copied: docker images The tagged image has the same image ID as your local image. The tagged image name is: &lt;region-key&gt;.ocir.io/&lt;tenancy-namespace&gt;/&lt;image-path-name&gt;/&lt;image-name&gt; Push the image to Container Registry: docker push &lt;copied-image-name&gt;:latest Example: docker push iad.ocir.io/my-namespace/node-apps/node-hello-app:latest Open the navigation menu and select Developer Services. Under Containers &amp; Artifacts, select Container Registry. Find your image in the Container Registry after the push command is complete. Deploy the Image We’re finally here! The moment of truth. With your image in Container Registry, you can now deploy your image and app: Create a registry secret for your application - This secret authenticates your image when you deploy it to your cluster. To create your secret, fill in the information in this template: kubectl create secret docker-registry ocirsecret --docker-server=&lt;region-key&gt;.ocir.io --docker-username='&lt;tenancy-namespace&gt;/&lt;user-name&gt;' --docker-password='&lt;auth-token&gt;' --docker-email='&lt;email-address&gt;' After the command runs, you get a message similar to: secret/ocirsecret created. Verify that the secret is created - Issue the following command: kubectl get secret ocirsecret --output=yaml The output includes information about your secret in yaml format. Host URL - Determine the host URL to your registry image using the following template: &lt;region-code&gt;.ocir.io/&lt;tenancy-namespace&gt;/&lt;repo-name&gt;/&lt;image-name&gt;:&lt;tag&gt; Example: iad.ocir.io/my-namespace/node-apps/node-hello-app:latest On your system, create a file called node-app.yaml. Include the text below and replace the following placeholders: &lt;your-image-url&gt; &lt;your-secret-name&gt; apiVersion: apps/v1 kind: Deployment metadata: name: node-app spec: selector: matchLabels: app: app replicas: 3 template: metadata: labels: app: app spec: containers: - name: app image: &lt;your-image-url&gt; imagePullPolicy: Always ports: - name: app containerPort: 3000 protocol: TCP imagePullSecrets: - name: &lt;your-secret-name&gt; --- apiVersion: v1 kind: Service metadata: name: node-app-lb labels: app: app annotations: service.beta.kubernetes.io/oci-load-balancer-shape: \"flexible\" service.beta.kubernetes.io/oci-load-balancer-shape-flex-min: \"10\" service.beta.kubernetes.io/oci-load-balancer-shape-flex-max: \"100\" spec: type: LoadBalancer ports: - port: 3000 selector: app: app Note: In the node-app.yaml file, the code after the dashes adds a flexible load balancer. Deploy your application with the following command: kubectl create -f node-app.yaml Sample output: deployment.apps/node-app created service/node-app-lb created Test Your App After you deploy your app, it might take the load balancer a few seconds to load. Check if the load balancer is live: kubectl get service Repeat the command until load balancer is assigned an IP address. Note: While waiting for the load balancer to deploy, you can check the status of your cluster with these commands: Get each pods status: kubectl get pods Get app status: kubectl get deployment Use the load balancer IP address to connect to your app in a browser: http://&lt;load-balancer-IP-address&gt;:3000 The browser should display &lt;h1&gt;Hello World from Node.js!&lt;/h1&gt; [OPTIONAL] Undeploy your application from the cluster To remove your application, run this command: kubectl delete -f node-app.yaml Sample output: deployment.apps/node-app deleted service \"node-app-lb\" deleted Your application is now removed from your cluster. What’s Next We’ve accomplish a lot in this tutorial! You’ve successfully created a Hello World application, deployed it to a Kubernetes cluster, and made it accessible on the internet using the Node Express framework. Check out these sites to explore more information about development with Oracle products: Oracle Developers Portal By ","categories": null,
        "tags": ["OCI","open-source","nodejs","front-end","kubernetes"],
        "url": "/tutorials/deploy-a-node-express-application",
        "teaser": ""
      },{
        "title": "Deploy Apache Superset with MySQL Database Service on Oracle Cloud Infrastructure",
        "excerpt":"It’s easy to deploy solutions on OCI (Oracle Cloud Infrastructure) using Terraform and Resource Manager’s Stack. I’ve published several resources available on this page. Today we will see how easy it is to deploy Apache Superset on OCI using MySQL Data Service. Apache Superset is an open source BI, Reporting, Charting tool that competes with Tableau, Looker, etc. For a list of companies that have deployed Superset, see this page. Superset is loaded with options that make it easy for users of all skill sets to explore and visualize their data, from simple line charts to highly detailed geospatial charts. For a gallery of available charts, go here. MySQL can be used as a backend to store the needed information related to the platform (users, settings, dashboards, …) , this represents 55 tables. MySQL can also be used as source for the data visualization. Preparation The easiest way to install Apache Superset on OCI is to click this button: You can also use the Terraform modules available on this GitHub repo. If you use the Red Pill (by clicking on the image above), you will redirected to OCI’s Dashboard Resource Manager Stack: You need to accept the Oracle Terms of Use and then the stack’s configuration will be loaded. Check that you are in the desired compartment and then click Next: You will be redirected to the second screen, for variables configuration. Some variables are mandatory and self explanatory: You also have the possibility to choose HA for MySQL DB instance, to load superset sample data (the deployment is then longer) and the Shape Selection. If you plan to use HeatWave on that instance, I recommend you to directly choose a HeatWave compatible Shape (default): If you already have a VNC and/or a MDS instance you want to use, you can also use the existing OCI infrastructure you have previously deployed: You need the OCIDs of all the existing resources. When you have performed all the selection you need, you can continue the process… Usually default should be good, you only require to add the MDS admin’s password and if this is the first Apache Superset deployment, I also recommend to load the sample data. Deployment The deployment will start, with the sample data load, and this takes approximately 30 minutes… When ready, the status will change: At the end of the logs section we already have the output variables we need to use to join our deployment: And we can retrieve that info in the Outputs section too: Apache Superset To reach the Apache Superset we just deployed, we paste the supersetpublicip‘s value on a browser and we use the supersetadminusername and supersetadminpassword to connect: Congratulations! Apache Superset is available and working on OCI with MySQL Database Service. If you want to connect to another MDS instance that you would use as a data source for some visualization graphs, you will need to be able to reach it (usually on the same VCN’s subnet or having routing between different VCN’s) and you must use the following syntax, as the MySQL Connector installed is mysql-connector-python: mysql+mysqlconnector://&lt;login&gt;:&lt;password&gt;@&lt;mds IP&gt;:3306/&lt;schema_name&gt; Conclusion Using OCI’s Resource Manager Stack (or Terraform) it is very easy to deploy Apache Superset on OCI using MySQL Database Service (MDS) as a backend. In a few minutes you have an Open Source Data Visualization solution that you can use with your MySQL Database Service instances. Enjoy OCI, MySQL and MySQL Database Service! By lefred","categories": ["cloudapps"],
        "tags": ["data-management","front-end","mysql","oci","analytics"],
        "url": "/tutorials/deploy-apache-superset-with-mds-on-oci",
        "teaser": ""
      },{
        "title": "Deploy Cassandra on Oracle Cloud (OCI) Linux VM",
        "excerpt":" 1 Introduction This tutorial will guide you through the steps needed to set up your environment to run Cassandra in Oracle Cloud Infrastructure. Prerequisites You should have already deployed a VM 2.1 with Oracle Linux 7.9 (OEL7) in Oracle Cloud Infrastructure (OCI). The installation of Oracle Linux 7.9 is using pip3.6 by default Python 3.6 or higher is installed You have access to root, either directly or via sudo. In OCI, the default user is “opc” and has sudo privileges Installing JuypterLab is fairly simple: Set up python Install python components and libraries Lets start with setting up the Python Environment. Begin &raquo; 2 Python setup By default, OEL7 runs Python 3. The first step is to install pip and virtualenv. Install virtualenv The next step is to install virtualenv. virtualenv enables us to create isolated sandboxes to develop Python applications without running into module or library conflicts. It’s easy to install: sudo pip3.6 install virtualenv Create an environment and enable it Create an environment called “myvirtualenv” using the following command: virtualenv -p /usr/bin/python3 myvirtualenv # Activate the env source myvirtualenv/bin/activate Check Python libraries Running the following command will show what Python modules we have installed at this point: (myvirtualenv) [opc@lab1 ~]$ pip3 list Package Version ---------- ------- pip 21.1.3 setuptools 57.1.0 wheel 0.36.2 WARNING: You are using pip version 21.1.3; however, version 21.2.1 is available. You should consider upgrading via the '/home/opc/myvirtualenv/bin/python -m pip install --upgrade pip' command. Upgrade your PIP Environment /home/opc/myvirtualenv/bin/python -m pip install --upgrade pip &laquo; Back Continue &raquo; 3 Jupyterlab setup Install JupyterLab pip3 install jupyterlab Install Python libraries for Machine Learning or ETL Process pip install pandas pip install pandarallel pip install dask pip install seaborn pip install matplotlib pip install plotly pip install -lxml==4.6.3 pip install selenium pip install beautifulsoup4 pip install scikit-learn Install other Python Libraries for Kafka Access and WEB Server Access pip install kafka-python (v2.0.0) pip install Flask pip install gunicorn Install extensiones for Jupyterlab Environment pip install jupytercontribnbextensions jupyter contrib nbextension install --user jupyter nbextension enable execute_time/ExecuteTime &laquo; Back Continue &raquo; 4 Configure Jupyterlab as an OEL7 Linux Service Create a script to automatically instantiate and reboot JupyterLab with the “opc” user. vi /home/opc/launchjupyterlab.sh Add the content below. You must use the “myvirtualenv” virtualenv environment you created previously. You can launch Jupyterlab on a specific port (for example: 8001) and listen on your public IP. #!/bin/bash # Activate myvirtualenv Environment source myvirtualenv/bin/activate cd /home/opc if [ \"$1\" = \"start\" ]; then nohup jupyter-lab --ip=0.0.0.0 --port=8001 &gt; ./nohup.log 2&gt;&amp;1 &amp; echo $! &gt; /home/opc/jupyter.pid else kill $(cat /home/opc/jupyter.pid) fi Make the script executable so it can be executed from the JupyterLab service. chmod 777 /home/opc/launchjupyterlab.sh Connect to the “root” user sudo -i Create a script to start and stop the “jupyterlab” service vi /etc/systemd/system/jupyterlab.service Add the following to launch the “launchjupyterlab.sh” script as the “opc” user [Unit] Description=Service to start jupyterlab for opc Documentation= [Service] User=opc Group=opc Type=forking WorkingDirectory=/home/opc ExecStart=/home/opc/launchjupyterlab.sh start ExecStop=/home/opc/launchjupyterlab.sh stop [Install] WantedBy=multi-user.target Test Jupyterlab Service systemctl start jupyterlab systemctl status jupyterlab systemctl enable jupyterlab &laquo; Back Continue &raquo; 5 Reboot Your machine for a final check Lastly, you’ll need to reboot your machine to ensure the jupyterlab script is available on port 8001. Open port 8001 on your virtual machine VM 2.1 to allow access to JupyterLab using your Public IP. firewall-cmd --permanent --zone=public --list-ports firewall-cmd --get-active-zones firewall-cmd --permanent --zone=public --add-port=8001/tcp firewall-cmd --reload If you’re running directly on a virtual machine and have a browser installed, it should take you directly into the Jupyter environment. Connect using your public IP with the 8001 port, i.e. “http://xxx.xxx.xxx.xxx:8001/”. You should now see the Python Web environment “Jupyterlab”. &laquo; Back By Olivier Francois Xavier Perard","categories": ["cloudapps"],
        "tags": ["oci","python","jupyter","back-end"],
        "url": "/tutorials/deploy-cassandra-on-oci",
        "teaser": ""
      },{
        "title": "How to Deploy a Python Flask Application in a Kubernetes cluster",
        "excerpt":" 1 Introduction In this tutorial, you use an Oracle Cloud Infrastructure (OCI) account to set up a Kubernetes cluster. Then, you create a Python application with a Flask framework. Finally, you’ll deploy your application to your cluster using Cloud Shell. Key tasks include how to: Create a Compartment Set up a Kubernetes cluster on OCI Build a Python application in a Flask framework Create a Docker image Push your image to OCI Container Registry Use Cloud Shell to deploy your Docker application to your cluster Connect to your application from the internet For additional information, see: More on Kubernetes OCI Container Engine OCI Container Registry Cloud Shell Begin &raquo; 2 Before You Begin To successfully perform this tutorial, you’ll need the following: Requirements A Free trial or a paid OCI account. You can sign up here. Cloud Shell or the following JDK 8+ Python 3.6.8+ Kubectl 1.18.10+ Apache Maven 3.5+ Docker 19.0.11+ The advantage of using Cloud Shell is all the required tools to manage your application are already installed and ready to use. &laquo; Back Continue &raquo; 3 Prepare Prepare your environment to create and deploy your application. Check your Service Limits Log in to the Oracle Cloud Infrastructure Console. Open the navigation menu, and click Governance and Administration. Under Governance, click Limits, Quotas and Usage. Find your service limit for Regions: Filter for the following options: Service: Regions Scope: Tenancy Resource: Subscribed region count Compartment: &lt;tenancy-name&gt; (root) Find service limit: Limit Name: subscribed-region-count Service Limit: minimum 2 Find your available Compute core count for the VM.Standard.E2.1 shape: Filter for the following options: Service: Compute Scope: &lt;first-availability-domain&gt;. Example: EMlr:US-ASHBURN-AD-1 Resource: Cores for Standard.E2 based VM and BM Instances Compartment: &lt;tenancy-name&gt; (root) Find available core count: Limit Name: standard-e2-core-count Available: minimum 1 Repeat for Scope: &lt;second-availability-domain&gt; and &lt;third-availability-domain&gt;. Each region must have at least one core available for this shape. Find out if you have 50 GB of Block Volume available: Filter for the following options: Service: Block Volume Scope: &lt;first-availability-domain&gt;. Example: EMlr:US-ASHBURN-AD-1 Resource Volume Size (GB) Compartment: &lt;tenancy-name&gt; (root) Find available core count: Limit Name: total-storage-gb Available: minimum 50 Repeat for Scope: &lt;second-availability-domain&gt; and &lt;third-availability-domain&gt;. Each region must have at least 50 GB of block volume available. Find out how many Flexible Load Balancers are available: Filter for the following options: Service: LBaaS Scope: &lt;your-region&gt;. Example: us-ashburn-1 Resource: Compartment: &lt;tenancy-name&gt; (root) Find the count for the following shapes Limit Name: lb-flexible-bandwidth-count Available: minimum 1 This tutorial creates three compute instances with a VM.Standard.E2.1 shape for the cluster nodes. To use another shape, filter for its core count. For example, filter VM.Standard2.4 for Cores for Standard2 based VM and BM Instances, and get the count. For a list of all shapes, see VM Standard Shapes. This tutorial creates a load balancer with a flexible shape. To use another bandwidth, filter its count. For example 100-Mbps bandwidth or 400-Mbps bandwidth. Create an Authorization Token In the Console’s top navigation bar, click the Profile menu (your avatar). Click your username. Click Auth Tokens. Click Generate Token. Give it a description. Click Generate Token. Copy the token and save it. Click Close. Ensure that you save your token right after you create it. You have no access to it later. Gather Required Information Collect the following credential information from the Oracle Cloud Infrastructure Console: Tenancy name: &lt;tenancy-name&gt; Click your Profile menu (your avatar) and find your Tenancy:. Tenancy namespace: &lt;tenancy-namespace&gt; Click your Profile menu (your avatar). Click Tenancy: . Copy the value for Object Storage Namespace. For some accounts, the tenancy name and namespace values differ. Ensure that you use namespace in this tutorial. Tenancy OCID: &lt;tenancy-ocid&gt; Click your Profile menu (your avatar), then click Tenancy:, and copy OCID. Username: &lt;user-name&gt; Click your Profile menu (your avatar). User OCID: &lt;user-ocid&gt; Click your Profile menu (your avatar), then click User Settings, and copy OCID. Find your region information. Region: &lt;region-identifier&gt; In the Console’s top navigation bar, find your region. Example: US East (Ashburn). Find your Region Identifier from the table in Regions and Availability Domains. Example: us-ashburn-1. Region Key: &lt;region-key&gt; Find your Region Key from the table in Regions and Availability Domains. Example: iad Copy your authentication token from the Create an Authentication Token section. Auth Token: &lt;auth-token&gt; &laquo; Back Continue &raquo; 4 Set Up a Cluster Install and configure management options for your Kubernetes cluster. Later, deploy your application to this cluster. Add Compartment Policy If your username is in the Administrators group, then skip this section. Otherwise, have your administrator add the following policy to your tenancy: allow group &lt;the-group-your-username-belongs&gt; to manage compartments in tenancy With this privilege, you can create a compartment for all the resources in your tutorial. Steps to Add the Policy In the Console’s top navigation bar, open the Profile menu (your avatar). Click your username. In the left pane, click Groups. In a notepad, copy the Group Name that your username belongs. Open the navigation menu and click Identity &amp; Security. Under Identity, click Policies. Click Create Policy. Fill in the following information: Name: manage-compartments Description: Allow the group &lt;the-group-your-username-belongs&gt; to list, create, update, delete and recover compartments in the tenancy. Compartment: &lt;your-tenancy&gt;(root) For Policy Builder, click Show Manual Editor. Paste in the following policy: allow group &lt;the-group-your-username-belongs&gt; to manage compartments in tenancy Click Create. Reference The compartments resource-type in Verbs + Resource-Type Combinations for IAM Create a Compartment Create a compartment for the resources that you’ll create in this tutorial. Log in to the Oracle Cloud Infrastructure Console. Open the navigation menu and click Identity &amp; Security. Under Identity, click Compartments. Click Create Compartment. Fill in the following information: Name: &lt;your-compartment-name&gt; Description: Compartment for &lt;your-description&gt;. Parent Compartment: &lt;your-tenancy&gt;(root) Click Create Compartment. Reference: Create a compartment Add Resource Policy If your username is in the Administrators group, then skip this section. Otherwise, have your administrator add the following policy to your tenancy: allow group &lt;the-group-your-username-belongs&gt; to manage all-resources in compartment &lt;your-compartment-name&gt; With this privilege, you can manage all the resources in your compartment, essentially giving you administrative rights in that compartment. Steps to Add the Policy Open the navigation menu and click Identity &amp; Security. Under Identity, click Policies. Select your compartment from the Compartment list. Click Create Policy. Fill in the following information: Name: manage-&lt;your-compartment-name&gt;-resources Description: Allow users to list, create, update, and delete resources in &lt;your-compartment-name&gt;. Compartment: &lt;your-tenancy&gt;(root) For Policy Builder, select the following choices: Policy use cases: Compartment Management Common policy templates: Let compartment admins manage the compartment Groups: &lt;the-group-your-username-belongs&gt; Location: &lt;your-tenancy&gt;(root) Click Create. Reference Common Policies Create a Cluster with “Quick Create” Create a cluster with default settings and new network resources through the ‘Quick Create’ workflow. Sign in to the Oracle Cloud Infrastructure Console. Open the navigation menu and click Developer Services. Under Containers &amp; Artifacts, clickKubernetes Clusters (OKE). Click Create Cluster. Select Quick Create. Click Launch Workflow. The Create Cluster dialog is displayed. Fill in the following information. Name: &lt;your-cluster-name&gt; Compartment: &lt;your-compartment-name&gt; Kubernetes Version: &lt;take-default&gt; Kubernetes API Endpoint: Public Endpoint The Kubernetes cluster is hosted in a public subnet with an auto-assigned public IP address. Kubernetes Worker Nodes: Private Workers The Kubernetes worker nodes are hosted in a private subnet. Shape: VM.Standard.E2.1 Number of Nodes: 3 Specify a custom boot volume size: Clear the check box. Click Next. All your choices are displayed. Review them to ensure that everything is configured correctly. Click Create Cluster. The services set up for your cluster are displayed. Click Close. Get a cup of coffee. It takes a few minutes for the cluster to be created. You have successfully created a Kubernetes cluster. Configure Cloud Shell to Access to Your Cluster After you create a Kubernetes cluster, set up your local system to access the cluster. Sign in to the Oracle Cloud Infrastructure Console. Open the navigation menu and click Developer Services. Under Containers &amp; Artifacts, clickKubernetes Clusters (OKE). Click the link to &lt;your-cluster&gt;. The information about your cluster is displayed. Click Access Cluster. Click Cloud Shell Access. Follow the steps in the dialog. The following steps are provided for your reference. From the main menu, click the Cloud Shell icon () and start a session. Check your oci CLI version and verify that Cloud Shell is working. oci -v Create your .kube directory if it doesn’t exist. mkdir -p $HOME/.kube Create kubeconfig file for your setup. Use the information from the Access Your Cluster dialog. oci ce cluster create-kubeconfig &lt;use data from dialog&gt; Test your cluster configuration with the following command. kubectl get service If the config file is not stored in its default location (~/.kube/config), you must export theKUBECONFIG environment variable to point to the location. export KUBECONFIG=$HOME//config When working with more than one cluster, you specify a specific config file on the command line. Example: kubectl –kubeconfig=&lt;/path/to/config/file&gt; With your cluster access setup, you are now ready to prepare your application for deployment. &laquo; Back Continue &raquo; 5 Build your Docker Application Next, set up the Flask framework on Cloud Shell. Then, create and run a Python application. Create a Local Application Create your Flask application. Install Flask. pip3 install --user Flask Create a directory for your application. mkdir python-hello-app Change to the python-hello-app directory. cd python-hello-app Create a “Hello, World!” application. Create the file: vi hello.py In the file, input the following text: python from flask import Flask app = Flask(name) @app.route('/') def hello_world(): return '&lt;h1&gt;Hello World from Flask!&lt;/h1&gt;' if name == \"main\": app.run(host=\"0.0.0.0\", port=int(\"5000\"), debug=True) Save the file. Run the Local Application Run your Flask application. Run the Python program. export FLASK_APP=hello.py export FLASK_ENV=development python3 hello.py This produces the following output: Serving Flask app ‘hello’ (lazy loading) Environment: development Debug mode: on Running on all addresses. WARNING: This is a development server. Do not use it in a production deployment. Running on http://x.x.x.x:5000/ (Press CTRL+C to quit) Restarting with stat Debugger is active! Debugger PIN: xxx-xxx-xxx Move the app to the background. Input ctrl-z. Enter the following command: bg Test the app using curl. In the Cloud Shell terminal, enter the following code: curl -X GET http://localhost:5000 Output: &lt;h1&gt;Hello World from Flask!&lt;/h1&gt; Stop the running application. When you’re done testing, get the process ID for your application. ps -ef Stop the process. kill &lt;your-pid&gt; You have successfully created a local Python application with the Flask framework. References: For more information on Flask, check out the Flask Documentation. Build a Docker Image Next, you’ll need to create a Docker image for your Flask application. First, ensure you’re in the python-hello-app directory. Create the configuration file Dockerfile: vi Dockerfile In the file, input the following text and save the file: FROM python:3.9-slim-buster ADD hello.py / COPY . /app WORKDIR /app RUN pip3 install Flask EXPOSE 5000 CMD [ \"python3\", \"./hello.py\" ] Build a Docker image: docker build -t python-hello-app . You get a success message. [INFO] BUILD SUCCESS Successfully tagged python-hello-app:latest Run the Docker image: docker run --rm -p 5000:5000 python-hello-app:latest &amp; Test the application using the curl command: curl -X GET http://localhost:5000 If you see &lt;h1&gt;Hello World from Flask!&lt;/h1&gt;, then the Docker image is running. You can now push the image to Container Registry. Stop the running application. When you are done testing, get the process ID for your application. ps -ef Stop the process. kill &lt;your-pid&gt; Congratulations! You have successfully created a Python Flask Docker image. &laquo; Back Continue &raquo; 6 Deploy Your Docker Image With your Python image created, now you can deploy it. Create a Docker Repository Open the navigation menu and click Developer Services. Under Containers &amp; Artifacts, clickContainer Registry. In the left navigation, select &lt;your-compartment-name&gt;. Click Create Repository. Create a private repository with your choice of repo name: &lt;repo-name&gt; = &lt;image-path-name&gt;/&lt;image-name&gt; Example: flask-apps/python-hello-app You are now ready to push your local image to Container Registry. Before you can push a Docker image into a registry repository, the repository must exist in your compartment. If the repository does not exist, the Docker push command does not work correctly. The slash in a repository name does not represent a hierarchical directory structure. The optional &lt;image-path-name&gt; helps to organize your repositories. Push Your Local Image With your local Docker image created, push the image to the Container Registry. Follow these steps. Open a terminal window. Log in to Container Registry: docker login &lt;region-key&gt;.ocir.io You are prompted for your login name and password. Username: &lt;tenancy-namespace&gt;/&lt;user-name&gt; Password: &lt;auth-token&gt; List your local Docker images: docker images The Docker images on your system are displayed. Identify the image you created in the last section: python-hello-app Tag your local image with the URL for the registry plus the repo name, so you can push it to that repo. docker tag &lt;your-local-image&gt; &lt;repo-url&gt;/&lt;repo-name&gt; Replace &lt;repo-url&gt; with: &lt;region-key&gt;.ocir.io/&lt;tenancy-namespace&gt;/ Replace &lt;repo-name&gt; with &lt;image-folder-name&gt;/&lt;image-name&gt; from the Create a Docker Repository section. Here is an example after combining both: docker tag python-hello-app iad.ocir.io/my-namespace/flask-apps/python-hello-app In this example, the components are: Repo URL: iad.ocir.io/my-namespace/ Repo name: flask-apps/python-hello-app OCI Container Registry now supports creating a registry repo in any compartment rather than only in the root compartment (tenancy). To push the image to the repo you created, combine the registry URL with the exact repo name. OCI Container Registry matches based on the unique repo name and pushes your image. Check your Docker images to ensure that the image is copied. docker images The tagged or the copied image has the same image ID as your local image. The copied image name is: &lt;region-key&gt;.ocir.io/&lt;tenancy-namespace&gt;/&lt;image-folder-name&gt;/&lt;image-name&gt; Push the image to Container Registry. docker push &lt;copied-image-name&gt;:latest Example: docker push iad.ocir.io/my-namespace/flask-apps/python-hello-app:latest Open the navigation menu and click Developer Services. Under Containers &amp; Artifacts, then clickContainer Registry. Find your image in Container Registry after the push command is complete. Deploy the Image With your image in Container Registry, you can now deploy your image and app. Create a registry secret for your application. This secret authenticates your image when you deploy it to your cluster. To create your secret, fill in the information in this template . kubectl create secret docker-registry ocirsecret \\ --docker-server=&lt;region-key&gt;.ocir.io \\ --docker-username='&lt;tenancy-namespace&gt;/&lt;user-name&gt;' \\ --docker-password='&lt;auth-token&gt;' \\ --docker-email='&lt;email-address&gt;' After the command runs, you get a message similar to: secret/ocirsecret created. Verify that the secret is created. Issue the following command: kubectl get secret ocirsecret --output=yaml The output includes information about your secret in the yaml format. Determine the host URL to your registry image using the following template: &lt;region-code&gt;.ocir.io/&lt;tenancy-namespace&gt;/&lt;repo-name&gt;/&lt;image-name&gt;:&lt;tag&gt; Example: iad.ocir.io/my-namespace/flask-apps/python-hello-app:latest On your system, create a file called app.yaml with the following text: Replace the following place holders: * &lt;your-image-url&gt; * &lt;your-secret-name&gt; apiVersion: apps/v1 kind: Deployment metadata: name: app spec: selector: matchLabels: app: app replicas: 3 template: metadata: labels: app: app spec: containers: - name: app image: &lt;your-image-url&gt; imagePullPolicy: Always ports: - name: app containerPort: 5000 protocol: TCP imagePullSecrets: - name: &lt;your-secret-name&gt; --- apiVersion: v1 kind: Service metadata: name: app-lb labels: app: app annotations: service.beta.kubernetes.io/oci-load-balancer-shape: \"flexible\" service.beta.kubernetes.io/oci-load-balancer-shape-flex-min: \"10\" service.beta.kubernetes.io/oci-load-balancer-shape-flex-max: \"100\" spec: type: LoadBalancer ports: - port: 5000 selector: app: app Deploy your application with the following command. kubectl create -f app.yaml Output: deployment.apps/app created In the app.yaml file, the code after the dashes adds a flexible load balancer. Test Your App After you deploy your app, it might take the load balancer a few seconds to load. Check if the load balancer is live: kubectl get service Repeat the command until the load balancer is assigned an IP address. While waiting for the load balancer to deploy, you can check the status of your cluster with these commands: Get each pods status: kubectl get pods Get app status: kubectl get deployment Use the load balancer IP address to connect to your app in a browser: http://&lt;load-balancer-IP-address&gt;:5000 The browser displays: &lt;h1&gt;Hello World from Flask!&lt;/h1&gt; Undeploy your application from the cluster. (Optional) To remove your application run this command: kubectl delete -f app.yaml Output: deployment.apps/python-hello-app deleted service \"python-hello-app-lb\" deleted Your application is now removed from your cluster. &laquo; Back Continue &raquo; 7 What's Next You have successfully created a Hello World Python application, deployed it to a Kubernetes cluster, and made it accessible on the internet using the Flask framework. Check out these resources to explore more information about development with Oracle products: Oracle Developers Portal Oracle Cloud Infrastructure &laquo; Back By ","categories": null,
        "tags": ["flask","kubernetes","oci","open-source","python"],
        "url": "/tutorials/deploy-flask-app-cloud-shell",
        "teaser": ""
      },{
        "title": "Deploying MongoDB in Oracle Cloud (OCI) Linux VM",
        "excerpt":"If you’ve ever wanted a guide for running MongoDB Enterprise/Community in your Oracle Cloud Infrastructure (OCI) environment, we have good news for you! Prerequisites You have deployed a VM 2.1 with Oracle Linux 7.9 (OEL7) in Oracle Cloud Infrastructure. Your install of Oracle Linux 7.9 is using pip3.6 by default. Python 3.6 or higher is installed. You have access to root either directly or via sudo. By default in OCI, you are connected like an “opc” user with sudo privilege. If you don’t yet have an OCI account, you can quickly sign up for one today by registering for an Oracle Cloud Free Tier account. Afterwards, check developer.oracle.com/linux for even more Linux content. MongoDB Installation Create an /etc/yum.repos.d/mongodb-enterprise-5.0. repo file in the yum configuration so that you can install MongoDB Enterprise directly with this command: vi /etc/yum.repos.d/mongodb-enterprise-5.0.repo Paste these lines in that file: [mongodb-enterprise-5.0] name=MongoDB Enterprise Repository baseurl=https://repo.mongodb.com/yum/redhat/$releasever/mongodb-enterprise/5.0/$basearch/ gpgcheck=1 enabled=1 gpgkey=https://www.mongodb.org/static/pgp/server-5.0.asc Execute the next command using the yum file: sudo yum install -y mongodb-enterprise Your result should look like this: Installed: mongodb-enterprise.x86_64 0:5.0.2-1.el7 Dependency Installed: cyrus-sasl.x8664 0:2.1.26-23.el7 cyrus-sasl-gssapi.x8664 0:2.1.26-23.el7 mongodb-database-tools.x8664 0:100.5.0-1 mongodb-enterprise-cryptd.x8664 0:5.0.2-1.el7 mongodb-enterprise-database.x8664 0:5.0.2-1.el7 mongodb-enterprise-database-tools-extra.x8664 0:5.0.2-1.el7 mongodb-enterprise-mongos.x8664 0:5.0.2-1.el7 mongodb-enterprise-server.x8664 0:5.0.2-1.el7 mongodb-enterprise-shell.x8664 0:5.0.2-1.el7 mongodb-enterprise-tools.x8664 0:5.0.2-1.el7 mongodb-mongosh.x8664 0:1.0.5-1.el7 net-snmp.x8664 1:5.7.2-49.el7_9.1 net-snmp-agent-libs.x8664 1:5.7.2-49.el79.1 net-snmp-libs.x8664 1:5.7.2-49.el79.1 Complete! The install is pretty simple using the yum installation script. Let’ s start with setting up the MongoDB Environment MongoDB Setup By default, MongoDB has a daemon configuration file in /etc/mongod.conf and runs using the mongod user account with the following default directories: /var/lib/mongo (the data directory) /var/log/mongodb (the log directory) The package manager creates the default directories during installation. The owner and group name are mongod. Using Non-Default Directories with MongoDB To use a data directory and/or log directory other than the default directories: Create the new directory or directories and then edit the configuration file /etc/mongod.conf and modify the following fields accordingly: storage.dbPath to specify a new data directory path (e.g. /some/data/directory) systemLog.path to specify a new log file path (e.g. /some/log/directory/mongod.log) Ensure that the user running MongoDB has access to the directory or directories: sudo chown -R mongod:mongod &lt;directory&gt; For example, create a “data” directory: sudo mkdir /data/mongodb sudo chown -R mongod:mongod /data/mongodb Change the configuration in /etc/mongod.conf and the dbPath variable. vi /etc/mongod.conf storage: dbPath: /var/lib/mongo dbPath: /data/mongodb journal: enabled: true engine: wiredTiger: If you change the user that runs the MongoDB process, you must give the new user access to these directories. MongoDB SELinux Configuration (Optional) Configure SELinux if enforced. See SELinux. Start MongoDB You can start the mongod process by issuing the following command: sudo systemctl start mongod Verify that MongoDB has started successfully You can verify that the mongod process has started successfully by issuing the following command: sudo systemctl status mongod You can optionally ensure that MongoDB will start following a system reboot by issuing the following command: sudo systemctl enable mongod Stop MongoDB As needed, you can stop the mongod process by issuing the following command: sudo systemctl stop mongod Restart MongoDB. You can restart the mongod process by issuing the following command: sudo systemctl restart mongod You can follow the state of the process for errors or important messages by watching the output in the /var/log/mongodb/mongod.log file. Begin using MongoDB Start a mongosh session on the same host machine as mongod. You can run mongosh without any command-line options to connect to a mongod that is running on your localhost with default port 27017. mongosh The result should look like: Current Mongosh Log ID: 61a0e720ab709eb9973092bc Connecting to: mongodb://127.0.0.1:27017/?directConnection=true&amp;serverSelectionTimeoutMS=2000 Using MongoDB: 5.0.2 Using Mongosh: 1.0.5 For mongosh info see: https://docs.mongodb.com/mongodb-shell/ The server generated these startup warnings when booting: 2021-11-26T13:54:31.358+00:00: Access control is not enabled for the database. Read and write access to data and configuration is unrestricted 2021-11-26T13:54:31.358+00:00: /sys/kernel/mm/transparent_hugepage/enabled is 'always'. We suggest setting it to 'never' Enterprise test&gt; You’re now connected to Mongodb on Oracle Cloud Infrastructure! By Olivier Francois Xavier Perard","categories": null,
        "tags": ["devops","python"],
        "url": "/tutorials/deploy-mongodb-oci-linux-vm",
        "teaser": ""
      },{
        "title": "Deploy Moodle on OCI with MDS",
        "excerpt":"Moodle is the world’s most popular learning management system. Moodle is Open Source and of course it’s compatible with the most popular Open Source Database : MySQL! I’ve already posted an article addressing how to install Moodle before we released MySQL Database Service. In this article we will see how to deploy Moodle very easily in OCI and using MDS. Once again we will use the easiest way to deploy an complete architecture on OCI: Resource Manager. We will then use a stack I’ve created that is available on GitHub This stack includes Terraform code allowing to deploy different architectures that we can use for Moodle. I’ve tried to cover the main possible architecture directly in the stack. It’s also possible to just download the Terraform code and modify it if you need. You also have the possibility to generate again a stack from your modified code. I’ve already multiple stacks you can deploy directly on OCI that allows you to deploy the same architectures as I cover in this article but for other solutions directly from this page: Deploy to OCI. Let’s have a look at some of the possible architectures we can deploy directly by clicking on the “deploy to OCI” button. Simplest Deployment This deployment, is the most simple to deploy. One single MySQL Database Service Instance and one compute instance as the Moodle Web Server. The architecture is composed by the following components: Availability domains: Availability domains are standalone, independent data centers within a region. The physical resources in each availability domain are isolated from the resources in the other availability domains, which provides fault tolerance. Availability domains don’t share infrastructure such as power or cooling, or the internal availability domain network. So, a failure at one availability domain is unlikely to affect the other availability domains in the region. Virtual cloud network (VCN) and subnets: a VCN is a customizable, software-defined network that you set up in an Oracle Cloud Infrastructure region. Like traditional data center networks, VCNs give you complete control over your network environment. A VCN can have multiple non-overlapping CIDR blocks that you can change after you create the VCN. You can segment a VCN into subnets, which can be scoped to a region or to an availability domain. Each subnet consists of a contiguous range of addresses that don’t overlap with the other subnets in the VCN. You can change the size of a subnet after creation. A subnet can be public or private. Internet gateway: the internet gateway allows traffic between the public subnets in a VCN and the public internet. Network security group (NSG): NSGs act as virtual firewalls for your cloud resources. With the zero-trust security model of Oracle Cloud Infrastructure, all traffic is denied, and you can control the network traffic inside a VCN. An NSG consists of a set of ingress and egress security rules that apply to only a specified set of VNICs in a single VCN. MySQL Database Service (MDS): MySQL Database Service is a fully managed Oracle Cloud Infrastructure (OCI) database service that lets developers quickly develop and deploy secure, cloud native applications. Optimized for and exclusively available in OCI, MySQL Database Service is 100% built, managed, and supported by the OCI and MySQL engineering teams. Compute Instance: OCI Compute service enables you provision and manage compute hosts in the cloud. You can launch compute instances with shapes that meet your resource requirements (CPU, memory, network bandwidth, and storage). After creating a compute instance, you can access it securely, restart it, attach and detach volumes, and terminate it when you don’t need it. Apache, PHP and Moodle are installed on the compute instance. Let’s see the different steps to deploy this architecture directly from here: You will redirected the OCI’s dashboard create stack page: As soon as you accept the Oracle Terms of Use, the form will be pre-filled by some default values. You can of course decide in which compartment you want to deploy the architecture: The second screen of the wizard the most important form where we need to fill all the required variables and also change the architecture as we will see later: The second part of the form looks like this. Note that we can enable High Availability for MDS, use multiple Web Server Instances or use existing infrastructure. This means that we have the possibility to use an existing VCN, subnets, etc… And of course we can also specify the Shapes for the compute instances (from a dropdown list of the available shapes in your tenancy and compartment) and for the MDS instance (this one needs to be entered manually). When we click next, we reach the last screen which summarize the choices and we can click on “Create.” By default the Architecture will be automatically applied (meaning all necessary resources will be deployed): Now we need to be a little bit of patience while everything is deployed. Other Possible Architectures As we could see earlier on the second screen of the stack’s creation wizard, we could also specify the use of multiple Web Servers. Then we have the possibility to deploy them on different Fault Domains (default) or use different Availability Domains: It’s possible to also specify if all Moodle servers will use their own database and user or share the same schema in case we want to use a load balancer in front of all the web servers and spread the load for the same site/application. The default architecture with three web servers looks like this: And if you want to enable High Availability for the MDS instance, you just need to check the box: And you will have an architecture like this: ## Finishing Moodle’s Installation When the deployment using the stack is finished, you will the a nice large green square with “SUCCEEDED” and in the log you will also see some important information. This information is also available in the Output section on the left: Now, we just need to open a web browser and enter that public ip to finish the installation of Moodle: And we follow the wizard until the database configuration section: On the screen above, we use the information that we can find in the Stack’s output section. Then we continue the installation process until it’s completed and finally we can enjoy our new Moodle deployment: By Frédéric Descamps","categories": ["frameworks","cloudapps"],
        "tags": ["oci","mysql"],
        "url": "/tutorials/deploy-moodle-on-OCI-with-mds",
        "teaser": ""
      },{
        "title": "Deploying Joomla! on OCI and MDS",
        "excerpt":"The easiest way to deploy Joomla! on Oracle Cloud Infrastructure and MySQL Database Service, is to use OCI Resource Manager’s stack. Let’s see how easy it is to deploy such solution. If you don’t already have an OCI account, you can get started for free. The easiest way to deploy all the required resources (vcn, subnets, gateways, compute and MySQL instances, security lists, …) is to use a stack for Resource Manager. It consists in Terraform modules and information to easily deploy the architecture on OCI. To deploy Joomla! and a MDS instance, visit this GitHub repo and click on the button to deploy on OCI: The Oracle Cloud Dashboard will be open and the stack creation form will be loaded like this: As soon as we accept the Oracle Terms of Use, the form will be automatically filled with the stack’s information: We can click on Next to reach the second form dedicated to the variables: As you can see some variables are already pre-filled. We need to enter some mandatory ones like the password the MDS admin password. We also have the possibility to enable HA for the MySQL Database Service instance, to have multiple webservers in case we want to have multiple Joomla! in front on the same database or multiple Joomla! with multiple dedicated databases… there are different possibilities. We also have the possibility to use existing resources in case you already created an architecture in OCI and you want to use it for Joomla! too. When we are ready, we click Next and the review summary screen appears: When we apply the stack, OCI will create a job to create all the resources and deploy everything that we need. This will take some minutes (+/- 10min): When everything is created and deployed, the job will return a status: All the required information to finish the installation is reported in the Outputs section: Now we can paste the public IP (joomlapublicip) in our Internet browser: After we enter the required information, we click Next to reach the database configuration form: We enter all the information from the stack’s outputs section. As the database is not local to Joomla!, we also need to delete a file for security reasons. To connect to the Joomla! server, we need to use the generate ssh key (you can get it from the Outputs section) and connect to the public IP with the opc user. Then we need to remove the file generated by Joomla!’s installation wizard: When this is done, we can continue with Joomla!’s installation: To finish the installation, we need to delete the installation folder: And here we go, Joomla! is installed: The administration dashboard is also available on /administrator/: As you can see, deploying Joomla! on OCI to use MySQL Database Service is very easy. The most complicate operation if to remove the files and folders during the installation. Enjoy MySQL! By Frédéric Descamps","categories": ["frameworks","modernize"],
        "tags": ["docker","nodejs","mysql"],
        "url": "/tutorials/deploying-joomla-on-oci-and-mds",
        "teaser": ""
      },{
        "title": "Deploying and monitoring a Redis cluster to Oracle Container Engine (OKE)",
        "excerpt":" 1 Introduction In the previous post, you learned how to add a simple extension to the terraform-oci-oke project so that it uses the Redis helm chart to deploy a Redis cluster on Kubernetes. In the current tutorial, you’ll build on this background by exploring how set up Promtheus to monitor your Redis cluster. Key tasks include how to: Deploy a Redis cluster Monitor the Redis cluster with Prometheus Populate the Redis cluster with existing data using Redis Mass Insertion Visualize the mass-insertion process with Grafana Use the Terraform helm provider as an alternate deployment and monitoring procedure For additional information, see: Signing Up for Oracle Cloud Infrastructure Begin &raquo; 2 Before you begin Requirements An Oracle Cloud Infrastructure Free Tier account. Start for Free. A MacOS, Linux, or Windows computer with ssh support installed. Access to Grafana. Access to helm. &laquo; Back Continue &raquo; 3 Deploy Prometheus Operator To get things started, open a terminal and create a namespace for Prometheus: kubectl create namespace monitoring If you are using the terraform-oci-oke module and have provisioned the bastion host, helm is already installed and pre-configured for you! Just login to the bastion host and deploy the Prometheus operator: helm install --namespace monitoring \\ stable/prometheus-operator \\ --name prom-operator \\ --set kubeDns.enabled=true \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\ --set coreDns.enabled=false \\ --set kubeControllerManager.enabled=false \\ --set kubeEtcd.enabled=false \\ --set kubeScheduler.enabled=false By setting serviceMonitorSelectorNilUsesHelmValues to false you ensure that all ServiceMonitors will be selected. Get a list of pods and identify the ones associated with Prometheus: kubectl -n monitoring get pods | grep prometheus The system will echo something similar to the following: alertmanager-prom-operator-prometheus-o-alertmanager-0 2/2 Running 0 18s prom-operator-prometheus-node-exporter-9xhzr 1/1 Running 0 24s prom-operator-prometheus-node-exporter-qtbvv 1/1 Running 0 24s prom-operator-prometheus-node-exporter-wjbfp 1/1 Running 0 24s prom-operator-prometheus-o-operator-79ff98787f-4t4k7 1/1 Running 0 23s prometheus-prom-operator-prometheus-o-prometheus-0 3/3 Running 1 11s In another terminal, set your local KUBECONFIG environment variable and run kubectl port-forward locally to access the Prometheus Expression Browser: export KUBECONFIG=generated/kubeconfig kubectl -n monitoring port-forward prometheus-prom-operator- prometheus-o-prometheus-0 9090:9090 To verify the targets, open your browser and access the Prometheus Expression Browser at http://localhost:9090/targets Configure Grafana Next, you’ll verify that Grafana has been configured properly and already has Prometheus as a datasource. With this set, you’ll be able monitor your Redis installation once it’s been deployed. In a console, get a list of pods and identify the ones associated with Grafana: kubectl -n monitoring get pods | grep grafana This will echo something similiar to the following: grafanaprom-operator-grafana-77cdf86d94-m8pv5 2/2 Running 0 57s Run kubectl port-forward locally to access Grafana: kubectl -n monitoring port-forward prom-operator-grafana-77cdf86d94-m8pv5 3000:3000 Access Grafana by connecting to port 30 on your browser (http://localhost:3000). Login with admin/prom-operator. Use the default username and password if you have not yet changed them. Once you’ve logged in, you should be able to see the default Kubernetes dashboards. &laquo; Back Continue &raquo; 4 Deploy Redis Cluster Create a namespace for Redis: kubectl create namespace redis Use helm to deploy the Redis cluster: helm install --namespace redis \\ stable/redis \\ --name redis \\ --set cluster.enabled=true \\ --set cluster.slaveCount=3 \\ --set master.persistence.size=50Gi \\ --set slave.persistence.size=50Gi \\ --set metrics.enabled=true \\ --set metrics.serviceMonitor.enabled=true \\ --set metrics.serviceMonitor.namespace=monitoring Access the Prometheus Expression Browser again (http://localhost:9090/targets) and verify that Redis is now listed as one of the targets: &laquo; Back Continue &raquo; 5 Import Redis Dashboard for Grafana Login to Grafana (http://localhost:3000). Select + on the left-hand menu to import a dashboard. Enter the dashboard id 2751 in the Grafana.com dashboard field: After the dashboard is loaded, select the Prometheus datasource: Select Import. You should now have a functioning Redis dashboard in Grafana! Mass-insert data into Redis In this step, you’ll learn a neat little time saver that will allow you to import large amounts of data into Redis using a csv file. Format your initial files.csv First, make sure that your file.csv data file is set up with the same format as the one shown below: id, first name, age, gender, nickname, salary 1, John Smith, 40, Male, John, 10000 2, Marco Polo, 43, Male, Marco, 10000 … 1999999, Tom Cruse, 50, Male, Tom, 10001 To import your csv file into Redis, run the following command in your console: awk -F, 'NR &gt; 1{ print \"SET\", \"\\\"employee_\"$1\"\\\"\", \"\\\"\"$0\"\\\"\" }' file.csv | redis-cli --pipe Generate dataset with mimesis In this part, you’ll be using the mimesis package to create a dataset based on the information you provided in file.csv. In a console, install mimesis: pip install mimesis Your initial dataset will need a little tweaking to be ready for use in Redis. So, in this part, you’ll adapt the mimesis schema a little bit to create a new csv file using Python. Create a names.py file with the following content: import csv from mimesis import Person from mimesis.enums import Gender en = Person('en') with open('file.csv',mode='w') as csv_file: field_names = ['id', 'full name', 'age', 'gender', 'username', 'weight'] writer = csv.DictWriter(csvfile, fieldnames=fieldnames) writer.writeheader() for n in range(100000): writer.writerow({'id': str(n), 'first name': en.full_name(), 'age': str(en.age()), 'gender': en.gender(), 'username':en.username(), 'weight':str(en.weight())}) In a console, run the Python script to generate the data: python names.py This will create a file.csv in the current directory. You can configure a PersistentVolume to store and load the data, but for the purpose of this tutorial, you’ll do a quick hack by installing Redis on the bastion. Install and configure Redis on the bastion In your console, run: sudo yum install redis -y This will allow you to use the redis-cli from the bastion where you’ve generated/uploaded your file.csv. On the bastion, get a list of Redis pods: kubectl -n redis get pods The system will echo something similar to the following: NAME READY STATUS RESTARTS AGE redis-master-0 1/1 Running 0 156m redis-metrics-794db76ff7-xmd2q 1/1 Running 0 156m redis-slave-7fd8b55f7-25w8d 1/1 Running 1 156m redis-slave-7fd8b55f7-hvhmc 1/1 Running 1 156m redis-slave-7fd8b55f7-mjq8q 1/1 Running 1 156m Afterwards, use port-forward so you can access the Redis master using the redis-cli: k -n redis port-forward redis-master-0 6379:6379 Redis wil echo something similar to the following: Forwarding from 127.0.0.1:6379 -&gt; 6379 Open a new terminal, login to the bastion and obtain the Redis password: export REDIS_PASSWORD=$(kubectl get secret --namespace redis redis -o jsonpath=\"\" | base64 --decode) Just as a check, do a quick test to see if you can connect to Redis. In a cosole, run: redis-cli -a $REDIS_PASSWORD127.0.0.1:6379&gt; Redis will echo with something similar to the following: ping PONG 127.0.0.1:6379&gt; Prep Grafana for csv import Before you import your csv, you’ll first need to access Grafana (http://localhost:3000). In a new terminal, run kubectl port-forward locally. Also, make sure that you navigate to the Redis Dashboard and set the refresh to every 5 seconds: kubectl -n monitoring port-forward prom-operator-grafana-77cdf86d94-m8pv5 3000:3000 Import your csv: awk -F, 'NR &gt; 1{ print \"SET\", \"\\\"employee\"$1\"\\\"\", \"\\\"\"$0\"\\\"\" }' file.csv | redis-cli -a $REDISPASSWORD --pipe The system will echo with something similar to: All data transferred. Waiting for the last reply... Last reply received from server. errors: 0, replies: 1000000 The Redis cluster is now deployed and actively being monitored by Prometheus! At this point, you may want to watch the Redis dashboard in Grafana. You can see the immediate jump in Network IO, the number of items in the DB as well as the amount of memory used. &laquo; Back Continue &raquo; 6 Installing the Prometheus Operator and Redis cluster using the Terraform helm provider Earlier in this tutorial, you learned how to manually install the Prometheus Operator and Redis Cluster through the cli, but this isn’t the only option available to you. You can also achieve the same results using the Terraform helm provider, but there are a few important things to keep in mind while doing so. As you’re enabling monitoring on Redis, you’ll now need to ensure that the relevant custom resource definitions (CRDs) are created. Previously, the manual steps you performed made certain that the CRDs were created and in the proper order. However, when you use Terraform to do the provisioning, you’ll need to explicitly set the order as follows: resource \"helm_release\" \"prometheus-operator\" { ... ... ... } resource \"helm_release\" \"redis\" { dependson = [\"helmrelease.prometheus-operator\"] ... ... ... } Congratulations! After performing these steps, you’ve ensured that the prometheus-operator release is created first, along with all the necessary CRDs that the Redis release will need (like Alertmanager, Prometheus, PrometheusRule, and ServiceMonitor) for Prometheus to be able to monitor the Redis cluster. &laquo; Back Continue &raquo; 7 What's Next You have successfully deployed a Redis cluser and enabled monitoring with Prometheus. To explore more information about development with Oracle products: Oracle Developers Portal Oracle Cloud Infrastructure &laquo; Back By Ali Mukadam","categories": null,
        "tags": ["back-end","oci"],
        "url": "/tutorials/deploying-monitoring-redis-oke",
        "teaser": ""
      },{
        "title": "Deploying the Argo CD on Oracle Container Engine for Kubernetes (OKE)",
        "excerpt":" In early 2020, the Argo Project was accepted as an incubator-level project in CNCF’s stack and comprises a set of Kubernetes-native tools for running and managing jobs and applications on Kubernetes. As a brief introduction, the Argo Project has 4 main components: Prerequisites In order to successfully complete this tutorial, you’ll need: A MacOS, Linux, or Windows computer with ssh support installed. The kubectl command-line tool. A kubeconfig file (default location is ~/.kube/config). Getting started In this tutorial, we’ll cover how to: Create a test OKE cluster for Argo Install Argo CD Connect to the Argo CD API Server Deploy applications using the Argo CD UI Deploy Argo Rollouts So, without wasting any time, let’s take Argo CD for a spin! Creating a test OKE cluster for Argo Clone the terraform-oci-oke repo or use the published terraform OKE module on the Terraform registry to create an OKE Cluster. NOTE: You can also use the Quick Create feature to create your cluster if you don’t want to use Terraform. Ensure you use the following parameters in your terraform.tfvars: label_prefix = \"argo\" region = \"us-phoenix-1\" vcndnslabel = \"oke\" vcn_name = \"oke\" createbastionhost = true create_operator = true admininstanceprincipal = true controlplanetype = \"private\" node_pools = { np1 = { shape = \"VM.Standard.E4.Flex\", ocpus = 1, memory = 16, nodepoolsize = 2, bootvolumesize = 150 } } In the console, run: terraform init terraform apply -auto-approve Once Terraform has finished, ssh to the operator by copying the ssh command from the output. For example: ssh -i ~/.ssh/id_rsa -J opc@XXX.XXX.XXX.XXX opc@10.0.1.10 Argo CD Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. It allows: Application definitions, configurations, and environments to be declarative and version controlled Application deployment and lifecycle management to be automated, auditable, and easy to understand In this section we’ll follow Argo CD’s getting started guide. Install Argo CD kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml Download and install the Argo CD CLI: curl -sLO https://github.com/argoproj/argo-cd/releases/download/v2.1.3/argocd-linux-amd64 chmod +x argocd-linux-amd64 sudo mv argocd-linux-amd64 /usr/local/bin/argocd Access the Argo CD API server Using port-forwarding Establish an SSH tunnel to operator: ssh -L 8080:localhost:8080 -i ~/.ssh/idrsa -J opc@&lt;bastionpublicip&gt; opc@&lt;operatorprivate_ip&gt; Then we port-forward to the Argo CD service: kubectl port-forward --address 0.0.0.0 svc/argocd-server -n argocd 8080:443 We can now access the ArgoCD UI in our browser at https://localhost:8080 Using the Load Balancer Or, we can change the service type to Load Balancer and use the IP Address of the Load Balancer to access the UI: kubectl patch svc argocd-server -n argocd -p '{\"spec\": {\"type\": \"LoadBalancer\"}}' Argo CD login Whichever way we use to connect to the Argo CD API server, we’ll be warned of a potential security risk. That’s because we didn’t install certificates and take other security precautions. However, we can still do that later by using Let’s Encrypt and cert-manager and then use this together with an Ingress Controller like the NGINX Ingress Controller. Make sure you read Argo’s documentation on using Ingress. Skip security warning for now: Since our goal is to get things set up and we know that we can postpone these tasks until later, we’ll just skip passed these warnings and select: Advanced &gt; Accept the Risk and Continue log in: The ArgoCD login page will appear and will require a password. Use the following to log in: login: admin password: kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"\" | base64 -d Since the initial password is just saved as plain text in the argocd-initial-admin-secret secret, we can use this to retrieve the information we need. Creating apps via the UI Once you’re connected to the Argo CD API server, follow the rest of the instructions in creating apps via UI. Deploy application Once the application is created, select Sync and watch the application being deployed as Kubernetes works its magic to create the various resources (deployment, service, ReplicaSet, pods, etc.). Acquaint yourself with the UI Once the application is deployed, take a moment to poke around the Argo UI and the Kubernetes resources. While you’re taking the tour, there are definitely some destinations you should visit. Let’s take a quick look at one of them: Select the guestbook-ui service and then select Edit. Change the service type from ClusterIP to LoadBalancer and then save. Once the OCI Load Balancer is provisioned, its public IP address appears. Awesome stuff! You can make a quick check on the OCI Console to verify. From here, you can experiment with other applications such as the sock-shop or using other tools such as helm or kustomize. You can find more examples in this example apps repo. Argo Rollouts Argo Rollouts provides additional deployment strategies such as Blue-Green and Canary to Kubernetes. In this section, we’ll dive right in and follow Rollouts’ getting started guide to do a Blue-Green deployment. Deploy Blue-Green To install Blue-Green, run: kubectl create namespace argo-rollouts kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml Install the Argo Rollouts kubectl plugin by running the following commands: curl -LO https://github.com/argoproj/argo-rollouts/releases/download/v1.0.7/kubectl-argo-rollouts-linux-amd64 chmod +x kubectl-argo-rollouts-linux-amd64 sudo mv kubectl-argo-rollouts-linux-amd64 /usr/local/bin/kubectl-argo-rollouts To test the plugin: kubectl argo rollouts version Switch from Kubernetes Switching from the default Kubernetes Deployment to Rollout is very easy: Change the apiVersion from apps/v1 to argoproj.io/v1alpha1 Change the kind from Deployment to Rollout Add a deployment strategy to the Rollout object Deploy a Rollout Create a bluegreen.yaml file (copied from the Argo CD documentation and example) on the operator host: apiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: rollout-bluegreen spec: replicas: 2 revisionHistoryLimit: 2 selector: matchLabels: app: rollout-bluegreen template: metadata: labels: app: rollout-bluegreen spec: containers: - name: rollouts-demo image: argoproj/rollouts-demo:blue imagePullPolicy: Always ports: - containerPort: 8080 strategy: blueGreen: # activeService specifies the service to update with the new template hash at time of promotion. # This field is mandatory for the blueGreen update strategy. activeService: rollout-bluegreen-active # previewService specifies the service to update with the new template hash before promotion. # This allows the preview stack to be reachable without serving production traffic. # This field is optional. previewService: rollout-bluegreen-preview # autoPromotionEnabled disables automated promotion of the new stack by pausing the rollout # immediately before the promotion. If omitted, the default behavior is to promote the new # stack as soon as the ReplicaSet are completely ready/available. # Rollouts can be resumed using: kubectl argo rollouts promote ROLLOUT autoPromotionEnabled: false --- kind: Service apiVersion: v1 metadata: name: rollout-bluegreen-active spec: type: LoadBalancer selector: app: rollout-bluegreen ports: - protocol: TCP port: 80 targetPort: 8080 --- kind: Service apiVersion: v1 metadata: name: rollout-bluegreen-preview spec: type: LoadBalancer selector: app: rollout-bluegreen ports: - protocol: TCP port: 80 targetPort: 8080 Deploy it: kubectl apply -f bluegreen.yaml Verify deployment Verify that we have 2 pods created: kubectl get pods List the ReplicaSet: NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR rollout-bluegreen-6565b74f44 1 1 1 83s rollouts-demo argoproj/rollouts-demo:blue app=rollout-bluegreen,rollouts-pod-template-hash=6565b74f44 We can see that the image deployed is of the blue variety. Similarly, if get Argo Rollouts to print thing for us: kubectl argo rollouts get rollout rollout-bluegreen -w Sample output: Name: rollout-bluegreen Namespace: default Status: ✔ Healthy Strategy: BlueGreen Images: argoproj/rollouts-demo:blue (stable, active) Replicas: Desired: 2 Current: 2 Updated: 2 Ready: 2 Available: 2 NAME KIND STATUS AGE INFO ⟳ rollout-bluegreen Rollout ✔ Healthy 21m └──# revision:1 └──⧉ rollout-bluegreen-6565b74f44 ReplicaSet ✔ Healthy 20m stable,active ├──□ rollout-bluegreen-6565b74f44-qps4m Pod ✔ Running 19m ready:1/1 └──□ rollout-bluegreen-6565b74f44-twx2x Pod ✔ Running 4m19s ready:1/1 Dashboard We can also use the Argo Rollouts dashboard to visualize things. NOTE: If you’re logged in the operator host, exit and log in again: ssh -L 3100:localhost:3100 -i ~/.ssh/id_rsa -J opc@132.226.28.30 opc@10.0.0.14 Access the dashboard To start up the dashboard, run: kubectl argo rollouts dashboard Use your browser to access the Rollout dashboards: Example - load balancers Finally, since we deployed both services as type=LoadBalancer, we will have 2 Load Balancers. Look up public IP addresses - You can look up their respective public IP addresses in the OCI console or use kubectl to look them up in the EXTERNAL-IP column when you run: kubectl get svc Use you browser to access them: NOTE: Both the active and preview will be blue. Quick test - upgrade from blue to green: Upgrade - Let’s now patch to upgrade from blue to green: kubectl patch rollout rollout-bluegreen --type merge -p '{\"spec\": {\"template\": { \"spec\": { \"containers\": [{\"name\": \"rollouts-demo\",\"image\": \"argoproj/rollouts-demo:green\"}]}}}}' And we can see effect immediately: Preview - If we access the preview for active Load Balancers, we’ll see the preview is green and active is still blue. Promotion - Let’s give the rollout a promotion. We can use command line as thus: kubectl promote rollout-bluegreen or if you have Argo Rollouts Dashboard still open, you can use that too. If we now access both load balancers, they’ll both show up as green. You can keep switching between them to simulate upgrading to newer versions of your application. What’s next At this point, we’ll pause here and leave Argo Events for a future article. Hopefully this tutorial has shown you that if you were considering running the Argo project on your Kubernetes cluster, OKE will work quite nicely with it. NOTE: This article was originally included as part of the Oracle developers series on April, 2020. It has been updated to focus on ArgoCD and Rollouts and also to reflect the changes in the terraform-oci-oke project. To explore more information about development with Oracle products: Oracle Developers Portal Oracle Cloud Infrastructure By Ali Mukadam","categories": ["cloudapps","opensource"],
        "tags": ["open-source","oke","kubernetes","terraform","devops"],
        "url": "/tutorials/deploying-the-argo-project-on-oke",
        "teaser": ""
      },{
        "title": "Deploying Verrazzano on Oracle Container Engine for Kubernetes (OKE)",
        "excerpt":" Oracle recently released Verrazzano, an “end-to-end container platform to deploy cloud native and traditional applications in multi-cloud and hybrid environments.” If that’s a lot to take in, it’s because Verrazzano, (v8o for short) packs a lot. In this post, we will explore deploying Verrazzano on OKE (Oracle Container Engine). The single cluster deployment model is easy: Create a Kubernetes cluster Install the Verrazzano platform operator Install Verrazzano After this, you can deploy your application of choice. Remember, if you don’t yet have an OCI account, you can quickly sign up for one today by registering for an Oracle Cloud Free Tier account. Creating the OKE cluster We will start by creating the OKE cluster using Terraform OKE module. Since we are only taking Verrazzano for a spin, we only need the bare minimum features. Follow the quickstart guide, create the providers and create a copy of the terraform.tfvars.example and rename the copy to terraform.tfvars. Ensure the following features/resources are enabled/created: createbastionhost = true bastion_access = [\"anywhere\"] create_operator = true enableoperatorinstance_principal = true node_pools = { np1 = { shape = \"VM.Standard.E4.Flex\", ocpus = 2, memory = 32, nodepoolsize = 2, bootvolumesize = 150} } Follow the rest of the quickstart to run terraform init and apply. Once the cluster is created, use the convenient output to copy the command to ssh to the operator host: sshtooperator = \"ssh -i ~/.ssh/id_rsa -J opc@xyz.xyz.xyz.xyz opc@10.0.0.12\" From here onwards, all kubectl commands are executed on the operator host. Installing the Verrazzano operator Let’s first install the Verrazzano operator: $ kubectl apply -f https://github.com/verrazzano/verrazzano/releases/download/v1.0.1/operator.yaml and wait for the deployment to complete: $ kubectl -n verrazzano-install rollout status deployment/verrazzano-platform-operator Waiting for deployment “verrazzano-platform-operator” rollout to finish: 0 of 1 updated replicas are available… Give it a couple of minutes and the operator should have deployed by then. Verify that the operator is running: $ kubectl -n verrazzano-install get pods NAME READY STATUS RESTARTS AGE verrazzano-platform-operator-5f788568fd-w8cz7 1/1 Running 0 80s Installing Verrazzano We can now install Verrazzano. We will use the dev profile for this exercise: kubectl apply -f - &lt;&lt;EOF apiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: hello-verrazzano spec: profile: dev EOF We need to wait for Verrazzano to install: kubectl wait \\ --timeout=20m \\ --for=condition=InstallComplete \\ verrazzano/ Accessing Verrazzano In order to access Verrazzano, you need to get the console URL: $ kubectl get vz -o yaml You will get a list of URLs printed. For example, my Verrazzano console URL is https://verrazzano.default.168.138.102.88.nip.io. Access this url in your browser and you will be prompted to login: The username is verrazzano and you can obtain the password by issuing the following command: kubectl get secret \\ --namespace verrazzano-system verrazzano \\ -o jsonpath= | base64 \\ --decode; echo You should now be able to access the Verrazzano console: Deploy an application to Verrazzano We will deploy the hello-helidon application. First, create a namespace: kubectl create namespace hello-helidon and add labels to identify the namespace as managed by Verrazzano and enabled for Istio: kubectl label namespace hello-helidon verrazzano-managed=true istio-injection=enabled Next, deploy the Verrazzano component: kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-comp.yaml Then create the Application Configuration: kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-app.yaml You can now get the name of your pod: $ kubectl get pods -n hello-helidon NAME READY STATUS RESTARTS AGEhello-helidon-deployment-54979d7d74-6c9nw 1/1 Running 0 2m18s And check if the application is ready: $ kubectl wait — timeout=300s — for=condition=Ready -n hello-helidon pod/hello-helidon-deployment-54979d7d74–6c9nw pod/hello-helidon-deployment-54979d7d74-6c9nw condition met Lookup the hostname of the load balancer: HOST=$(kubectl get gateway hello-helidon-hello-helidon-appconf-gw \\ -n hello-helidon \\ -o jsonpath='') You can then test the application: $ curl -sk \\ -X GET \\ \"https://${HOST}/greet\" This should return you the following: {\"message\":\"Hello World!\"} Observability Now, that we’ve got our application running and accessible, we want to also look at its logs and metrics. Verrazzano has got you covered in the form of the ELK stack for logging and the combination of Prometheus and Grafana for metrics and performance monitoring. Let’s look at Grafana first. On the main page of the Verrazzano console, you will see a link to Grafana. You can use the same combination of username and password you used to log into Grafana. Once logged in, click on “Home” and select the “Helidon Monitoring Dashboard”: Similarly, access the Kibana dashboard and click on Visualize icon in the left menu. You will be prompted to create an index pattern. Select the verrazzano* and follow the wizard to add the index pattern. Search for hello-helidon and you should be able to see the following: From here, you can create your own visualizations and dashboards. What if we want to peek at the Kubernetes cluster itself? Again, Verrazzano has got you covered. From the Verrazzano console, locate the link to Rancher and click on it. The default username is “admin” and you can retrieve the password as follows: kubectl get secret \\ --namespace cattle-system rancher-admin-secret \\ -o jsonpath= | base64 \\ --decode; echo Once logged in, you will land on the cluster page and you will see an Explorer button. Click on it and you will be able to view your Kubernetes cluster: Summary Verrazzano packs a nice set of capabilities that helps you with the operational side of of Kubernetes. From monitoring to logging and security, there is a lot productivity that a Kubernetes or an application administrator can gain. I hope you find this article helpful. In future, we will explore other features of Verrazzano, including multi-cluster deployment and network security among others. By Ali Mukadam","categories": ["cloudapps","opensource"],
        "tags": ["open-source","oke","kubernetes","terraform","devops"],
        "url": "/tutorials/deploying-verrazzano-on-oke",
        "teaser": ""
      },{
        "title": "Deploying Verrazzano on Oracle Container Engine for Kubernetes (OKE)",
        "excerpt":" You may have been following Oracle’s open-source development of Verrazzano and were curious to know what it was about. Technically speaking, Verrazzano is an “end-to-end container platform to deploy cloud native and traditional applications in multi-cloud and hybrid environments.” If that’s a lot to take in, it’s because Verrazzano, (v8o for short) packs a lot in! But in essence, Verrazzano is a bridge between on-premises and the cloud, enabling you to deploy your container applications to any of the Kubernetes clusters where Verrazzano is installed. In this first part of the series, we’ll first cover the single-cluster deployment of Verrazzano on the Oracle Container Engine (OKE) and then learn how to deploy an example application and monitor its activity. Topics include learning how to: Create a Kubernetes cluster Install the Verrazzano platform operator Install and access Verrazzano Deploy, access, and monitor an example application For additional information, see: Signing Up for Oracle Cloud Infrastructure Introducing Verrazzano Enterprise Verrazzano quickstart guide Before you begin To successfully complete this tutorial, you will need to have the following: An Oracle Cloud Infrastructure Free Tier account. Start for Free A MacOS, Linux, or Windows computer with ssh support installed Git Terraform 1.0.0 or later Creating the OKE cluster Time to dig in! Let’s start by creating the OKE cluster using the Terraform OKE module. Fortunately, since we’re only taking Verrazzano for a spin, we just need the bare minimum of features. For this, we’ve prepared a quickstart guide to get you going. Begin by following the instructions to create: the providers, and a copy of the file, terraform.tfvars.example NOTE: Be sure that you rename the copy to terraform.tfvars. Before moving ahead, confirm that the following features/resources are enabled/created: createbastionhost = true bastion_access = [\"anywhere\"] create_operator = true enableoperatorinstance_principal = true node_pools = { np1 = { shape = \"VM.Standard.E4.Flex\", ocpus = 2, memory = 32, nodepoolsize = 2, bootvolumesize = 150} } Now that we have the environment just as we need it, continue with the rest of the quickstart to run terraform init and tarraform apply. Once the cluster is created, we can use the output to conveniently copy the command that tells us how to ssh to the operator host: sshtooperator = \"ssh -i ~/.ssh/id_rsa -J opc@xyz.xyz.xyz.xyz opc@10.0.0.12\" NOTE: For the rest of the tutorial, all kubectl commands are executed on the operator host. Installing the Verrazzano operator Verrazzano provides a Kubernetes operator to manage the life cycle of Verrazzano installations. In this section, we’ll learn how to install this operator. Deploy the Verrazzano operator by running: kubectl apply -f https://github.com/verrazzano/verrazzano/releases/download/v1.0.1/operator.yaml Wait for the deployment to complete: $ kubectl -n verrazzano-install rollout status deployment/verrazzano-platform-operator Waiting for deployment \"verrazzano-platform-operator\" rollout to finish: 0 of 1 updated replicas are available... NOTE: Be patient! The operator make take a couple of minutes to deploy. Verify that the operator is running: $ kubectl -n verrazzano-install get pods NAME READY STATUS RESTARTS AGE verrazzano-platform-operator-5f788568fd-w8cz7 1/1 Running 0 80s Installing Verrazzano Now that we have all of the preliminary set up out of the way, we’re ready to install Verrazzano. For this exercise, install Verrazzano with the dev profile: $ kubectl apply -f - &lt;&lt;EOF apiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: hello-verrazzano spec: profile: dev EOF Wait for Verrazzano to install: $ kubectl wait \\ --timeout=20m \\ --for=condition=InstallComplete \\ verrazzano/ Accessing Verrazzano We’ve successfully installed Verrazzano, so how do we access it? We’ll first need to determine the Verrazzano console URL. To obtain the console URL, run: kubectl get vz -o yaml This command will return a list of URLs. For example, a Verrazzano console URL may look similar to: https://verrazzano.default.168.138.102.88.nip.io. Once you have the console URL, copy it into your browser. At this point, you’ll be prompted to log in: username: The username is: verrazzano password: To get the password, run: $ kubectl get secret \\ --namespace verrazzano-system verrazzano \\ -o jsonpath= | base64 \\ --decode; echo You should now be able to access the Verrazzano console: Deploy an example application to Verrazzano With Verrazzano installed and configured, we’re ready to deploy our first application. We’ll keep it simple to start by creating a version of an old friend, the hello-world example. The Hello World Helidon application returns a friendly and familiar “Hello World” response when invoked. Create a namespace for the hello-helidon application: kubectl create namespace hello-helidon Add labels to identify the namespace as managed by Verrazzano and enabled for Istio: kubectl label namespace hello-helidon verrazzano-managed=true istio-injection=enabled Deploy the Verrazzano component: kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-comp.yaml Create the Application Configuration: kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-app.yaml You can now get the name of your pod: $ kubectl get pods -n hello-helidon NAME READY STATUS RESTARTS AGEhello-helidon-deployment-54979d7d74-6c9nw 1/1 Running 0 2m18s To check to see if the application is ready, run: $ kubectl wait — timeout=300s — for=condition=Ready -n hello-helidon pod/hello-helidon-deployment-54979d7d74–6c9nw pod/hello-helidon-deployment-54979d7d74-6c9nw condition met Look up the hostname of the load balancer: $ HOST=$(kubectl get gateway hello-helidon-hello-helidon-appconf-gw \\ -n hello-helidon \\ -o jsonpath='') Test the deployment The moment of truth! You can then test the application by running: $ curl -sk \\ -X GET \\ \"https://${HOST}/greet\" This should return you the following: {\"message\":\"Hello World!\"} Success! You’ve deployed your first application in Verrazzano. Observability Now that we have our application running and accessible, we’ll want to keep track of how its performing and have a look at its logs and metrics. In both cases, Verrazzano has you covered! Verrazano provides the ELK stack for logging and a combination of Prometheus and Grafana for metrics and performance monitoring. Grafana On the main page of the Verrazzano console, you’ll see a link to Grafana. The same combination of username and password you used for Verrazzano will work here to connect to Grafana. Once logged in, select Home and then the Helidon Monitoring Dashboard: Similarly, access the Kibana dashboard and then select the Visualize icon in the left-hand menu. You’ll be prompted to create an index pattern. Select verrazzano* and then follow the wizard to add the index pattern. Search for our example application, hello-helidon, and you should see the following: From here, you can create your own visualizations and dashboards. Checking in on the Kubernetes cluster with Rancher What if we want a peek at the Kubernetes cluster itself? Again, Verrazzano has your back. From the Verrazzano console, locate the link to Rancher and select it. username: The default username is: admin password: To retrieve the password, run: $ kubectl get secret \\ --namespace cattle-system rancher-admin-secret \\ -o jsonpath= | base64 \\ --decode; echo Once logged in, you’ll land on the cluster page and see an Explorer button. Select it to view your Kubernetes cluster: What’s next Verrazzano packs in a nice set of capabilities that helps you with the operational side of Kubernetes. From monitoring to logging and security, there’s a lot productivity that a Kubernetes or an application administrator can gain. Hopefully, you’ve found this article helpful and piqued your interest in what Verrazzano has to offer. In the next part of this series, we’ll begin exploring other features of Verrazzano, including multi-cluster deployment and network security. For more information about development with Oracle products: Oracle Developers Portal Oracle Cloud Infrastructure By Ali Mukadam","categories": ["cloudapps","opensource"],
        "tags": ["open-source","oke","kubernetes","verrazzano","terraform","devops"],
        "url": "/tutorials/multi-cluster-verrazzano-oke/deploying-verrazzano-on-oke",
        "teaser": ""
      },{
        "title": "DevOps and Automation on OCI",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/use-cases/devops",
        "teaser": ""
      },{
        "title": "Enterprise Cloud Native Development",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/use-cases/enterprise",
        "teaser": ""
      },{
        "title": "Extending Terraform OKE with a helm chart",
        "excerpt":" 1 Introduction When designing the Terraform OKE provisioning scripts, one of the things we wanted to do was make it reusable. So, what does that translate to here? In this context, it means extending the base sample repo and adding in our own extensions. In this tutorial, we’ll deploy a Redis Cluster to OKE using helm charts. Terraform conveniently provides a helm provider, so we’ll use that for our purposes. Topics covered in this tutorial: Adding the helm provider and repository Adding Redis with a helm release Interacting with the Redis Cluster Inspecting the new Redis Cluster Updating your release after deployment For additional information, see: Signing Up for Oracle Cloud Infrastructure Getting started with Terraform Getting started with OCI Cloud Shell Begin &raquo; 2 Prerequisites To successfully complete this tutorial, you’ll need the following: An Oracle Cloud Infrastructure Free Tier account. Start for Free. A MacOS, Linux, or Windows computer with ssh support installed. OCI Cloud Shell - It provides a great platform for quickly working with Terraform as well as a host of other OCI interfaces and tools. Access to Terraform. &laquo; Back Continue &raquo; 3 Getting started First, clone the repo as we did before: git clone https://github.com/oracle/sample-oke-for-terraform.git tfoke Then, navigate into the tfoke directory: cd tfoke And finally, follow the instructions to create your Terraform variable file. &laquo; Back Continue &raquo; 4 Adding the helm provider and repository In the OKE module, create a redis.tf file. First, we need to configure the helm provider. Since we already have the kubeconfig file, we’ll use the File Config method: Add the following to redis.tf: provider \"helm\" { kubernetes { config_path = \"${path.root}/generated/kubeconfig\" } } Add a helm repository: data \"helm_repository\" \"stable\" { name = \"stable\" url = \"https://kubernetes-charts.storage.googleapis.com\" } &laquo; Back Continue &raquo; 5 Adding Redis with a helm release In this section, we’ll use the Redis helm chart to create a helm release. However, we want helm to deploy only after the worker nodes become active, so we’ll have to make sure to check their status before proceeding. Let’s get started setting up our release. In the sample repo, there’s a nullresource isworker_active that you can use to set an explicit dependency. To make use of this dependency, add the following to redis.tf: resource \"helmrelease\" \"redis\" { dependson = [\"nullresource.isworkeractive\", \"localfile.kubeconfigfile\"] provider = \"helm\" name = \"oke-redis\" repository = \"${data.helm_repository.stable.metadata.0.name}\" chart = \"redis\" version = \"6.4.5\" set { name = \"cluster.enabled\" value = \"true\" } set { name = \"cluster.slaveCount\" value = \"3\" } set { name = \"master.persistence.size\" value = \"50Gi\" } } yaml file If you prefer to customize your helm release using a yaml file, we’ll quickly walk through setting that up here: Create a folder called resources under the oke module. Copy the file, values.yaml from the redis chart repo to redis_values.yaml: curl -o modules/oke/resources/redis_values.yaml https://raw.githubusercontent.com/helm/charts/master/stable/redis/values.yaml Remove the individual settings in the redis release from the terraform code and add the following instead: values = [ \"${file(\"${path.module}/resources/redis_values.yaml\")}\" ] Your release should then look like this: resource \"helmrelease\" \"redis\" { dependson = [\"nullresource.isworkeractive\", \"localfile.kubeconfigfile\"] provider = \"helm\" name = \"my-redis-release\" repository = \"${data.helm_repository.stable.metadata.0.name}\" chart = \"redis\" version = \"6.4.5\" values = [ \"${file(\"${path.module}/resources/redis_values.yaml\")}\" ] } Note: You can also combine the two approaches above, but in general it’s not a bad idea to keep the configurations in a single location for easy updating. Also, you can change the values in the yaml file if you want to. For example, a good working pair of settings might be: default cluster.slaveCount = 3 persistence.size = 50Gi Download the helm provider and check status Run terraform init to download the helm provider and then apply again: terraform init terraform apply -auto-approve Log in to the bastion and do a helm list: helm list NAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE oke-redis 1 Wed Apr 24 12:05:40 2019 DEPLOYED redis-6.4.5 4.0.14 default Get the notes provided by the redis chart: helm status &laquo; Back Continue &raquo; 6 Interacting with the Redis Cluster After you’ve run helm status (see previous section), the following are available to you: Get the Redis password: export REDIS_PASSWORD=$(kubectl get secret --namespace default oke-redis -o jsonpath=\"\" | base64 --decode) Run a Redis pod: kubectl run --namespace default oke-redis-client --rm --tty -i --restart='Never' \\ --env REDISPASSWORD=$REDISPASSWORD \\ --image docker.io/bitnami/redis:4.0.14 -- bash Connect using the Redis cli: redis-cli -h oke-redis-master -a $REDIS_PASSWORD Type a redis command: oke-redis-master:6379&gt; ping PONG &laquo; Back Continue &raquo; 7 Inspecting your cluster Recall that in the yaml file, we set the number of redis slaves to 3. Let’s verify that this is still the case: kubectl get pods Your output should look something like this: NAME READY STATUS RESTARTS AGE oke-redis-master-0 1/1 Running 0 42m oke-redis-slave-79c45c57d8-67bxj 1/1 Running 1 42m oke-redis-slave-79c45c57d8-s6znq 1/1 Running 0 42m oke-redis-slave-79c45c57d8-wnfrh 1/1 Running 0 42m From this, you can see that there are 3 pods running the redis slaves. &laquo; Back Continue &raquo; 8 Updating your release Let’s consider a real-world example. Let’s say we want to update the helm release to change some settings. For example, we need to reduce the number of slaves from 3 to 2. We actually have a couple of different ways we can do this. Change settings (2 methods) helm cli - Perform the setting change manually using the helm cli: helm upgrade oke-redis stable/redis --set cluster.slaveCount=2 yaml file - Or, change the settings in the redis_values.yaml and then run terraform apply again. In the case where we reduced the number of slaves from 3 to 2 the output of the terraform apply command should be something like: .. .. .. module.oke.helm_release.redis: Still modifying… (ID: oke-redis, 10s elapsed) module.oke.helm_release.redis: Still modifying… (ID: oke-redis, 20s elapsed) module.oke.helm_release.redis: Still modifying… (ID: oke-redis, 30s elapsed) module.oke.helm_release.redis: Still modifying… (ID: oke-redis, 40s elapsed) module.oke.helm_release.redis: Still modifying… (ID: oke-redis, 50s elapsed) module.oke.helm_release.redis: Still modifying… (ID: oke-redis, 1m1s elapsed) module.oke.helm_release.redis: Modifications complete after 1m9s (ID: oke-redis) Apply complete! Resources: 1 added, 1 changed, 1 destroyed. In the meantime, from another terminal, we can watch the number of pods being updated: kubectl get pods -w Your output should be something like: oke-redis-master-0 0/1 Terminating 0 61s oke-redis-slave-6bd9dc8d89-jdrs2 0/1 Running 0 3s oke-redis-slave-6bd9dc8d89-kvc8r 0/1 Running 0 3s oke-redis-slave-6fdd8c4b56-44qpb 0/1 Terminating 0 63s &laquo; Back Continue &raquo; 9 Next steps In future articles, we’ll look at other ways to extend the terraform-oci-oke module to deploy software on OKE. Check out these sites to explore more information about development with Oracle products: Oracle Developers Portal Oracle Cloud Infrastructure &laquo; Back By ","categories": null,
        "tags": ["kubernetes","devops","terraform"],
        "url": "/tutorials/extending-terraform-oke-helm-chart",
        "teaser": ""
      },{
        "title": "Creating flexible OCI Load Balancers with OKE",
        "excerpt":"Until recently, the OCI Load Balancer shapes were restricted to a handful of options: 100 Mbps 400 Mbps 8000 Mbps What’s more, if you had to change the shape, this would involve recreating the load balancer. Not anymore! A few more options have been created for new load balancer shapes: 10 Mbps-Micro 10 Mbps Flexible And now, [load balancer] shapes are updatable without having to destroy and recreate them. So, let’s see how we can create them with OKE! For more information, see: Signing Up for Oracle Cloud Infrastructure Getting started with Terraform Getting started with OCI Cloud Shell Overview of load balancing Comparing OCI Load Balancers Load Balancer management Prerequisites To successfully complete this tutorial, you will need the following: An Oracle Cloud Infrastructure Free Tier account. Start for Free. A MacOS, Linux, or Windows computer with ssh support installed. OCI Cloud Shell Creating Load Balancer Shapes First, let’s see what load balancer shapes are available in our tenancy: oci lb shape list --compartment-id ocid1.compartment.oc1.. Output should show the following: $ oci lb shape list --compartment-id ocid1.compartment.oc1.. { \"data\": [ { \"name\": \"100Mbps\" }, { \"name\": \"10Mbps\" }, { \"name\": \"10Mbps-Micro\" }, { \"name\": \"400Mbps\" }, { \"name\": \"8000Mbps\" }, { \"name\": \"flexible\" } ] } As you can see, all the shapes are available. We could use a simple service to have the load balancer created, but the goal here is to show that these work equally well with ingress controllers. So, let’s use the NGINX Ingress Controller to create one. Creating and updating a Load Balancer with an Ingress Controller Let’s first add an Ingress Controller: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update helm install nginx ingress-nginx/ingress-nginx By default, this will create a load balancer with a of shape 100 Mbps: oci lb load-balancer get --load-balancer-id ocid1.loadbalancer....\"shape-name\": \"100Mbps\",... Example - change shape to 400 Mbps: \"shape-name\": \"100Mbps\", ... Change shape - Let’s say we want to change the shape to 400 Mbps. We can do this with a load balancer annotation and a helm upgrade: helm upgrade nginx ingress-nginx/ingress-nginx \\ --set controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/oci-load-balancer-shape\"=\"400Mbps\" This can get to be really cumbersome and unwieldy, so if you want to avoid the horrible escapes and \\, use the values.yaml file provided by the chart. All you would need to do is traverse to the annotations section and add the following: service.beta.kubernetes.io/oci-load-balancer-shape: \"400Mbps\" Check status - After the upgrade is done, we can check on the shape again as before. We can see it’s now been upgraded to 400 Mbps: ... \"shape-name\": \"400Mbps\", ... Example - create a flexible shape with bandwidth limits: Now, let’s say we want to create a load balancer with flexible shape and also take the opportunity to set bandwidth limits. We can do this by passing the annotations described below. Check shape - When we check on the shape, we see the following: helm install nginx ingress-nginx/ingress-nginx \\ --set controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/oci-load-balancer-shape\"=\"flexible\" \\ --set controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/oci-load-balancer-shape-flex-min\"=100 \\ --set controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/oci-load-balancer-shape-flex-max\"=200 Change the bandwidth - To dynamically change the bandwidth: helm upgrade nginx ingress-nginx/ingress-nginx --set controller.service.annotations.\"service\\.beta\\.kubernetes\\.$ io/oci-load-balancer-shape\"=\"flexible\" \\ --set controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/ oci-load-balancer-shape-flex-min\"=10 \\ --set controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/ oci-load-balancer-shape-flex-max\"=500 Recheck shape: Now when we check the shape, we can see the changes reflected: \"shape-details\": { \"maximum-bandwidth-in-mbps\": 500, \"minimum-bandwidth-in-mbps\": 10 }, $ helm upgrade nginx ingress-nginx/ingress-nginx --set controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/oci-load-balancer-shape\"=\"flexible\" \\ --set controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/oci-load-balancer-shape-flex-min\"=10 \\ --set controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/oci-load-balancer-shape-flex-max\"=500 \"shape-details\": { \"maximum-bandwidth-in-mbps\": 500, \"minimum-bandwidth-in-mbps\": 10 }, \"shape-name\": \"flexible\", Where to find the Load Balancer annotations Finally, all the OCI Load Balancer annotations can be found in the Load Balancer Annotations document. These annotations allow you to control the behavior of the load balancers created by OKE. What’s next To explore more information about development with Oracle products: Oracle Developers Portal Oracle Cloud Infrastructure By ","categories": ["clouddev"],
        "tags": ["kubernetes"],
        "url": "/tutorials/flexible-load-balancers-oke",
        "teaser": ""
      },{
        "title": "Top Frameworks for Top Languages",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/use-cases/frameworks",
        "teaser": ""
      },{
        "title": "Free Tier: Install WordPress on an Ubuntu Instance",
        "excerpt":" 1 Introduction In this tutorial, use an Oracle Cloud Infrastructure Free Tier account to set up an Ubuntu instance. Next, install an Apache web server, PHP 7, MySQL, and finally WordPress. After installation, access your new WordPress installation from the internet. This tutorial covers all the steps necessary to set up a virtual network, a compute instance, and connect the host to the internet. Key tasks include how to: Set up a compartment for your development work. Install your Ubuntu Linux instance and connect it to your Virtual Cloud Network (VCN). Set up an Oracle Cloud Infrastructure virtual cloud network and related network services required for your host to connect to the internet. Set up ssh encryption keys to access your Ubuntu Linux Server. Configure ingress rules for your VCN. Configure Apache, PHP 7, MySQL, and WordPress on your VM. Connect to your instance from the internet. Here’s a simplified diagram of the setup for your Linux VM. For additional information, see: Start for Free Free Tier: Install Apache and PHP on an Ubuntu Instance Begin &raquo; 2 Before you Begin To successfully complete this tutorial, you must have the following: Requirements An Oracle Cloud Infrastructure Free Tier account. Start for Free. A MacOS, Linux, or Windows computer with ssh support installed. &laquo; Back Continue &raquo; 3 1. Set up a Compartment for Development Configure a compartment for your development. Create a compartment Create a compartment for the resources that you create in this tutorial. Log in to the Oracle Cloud Infrastructure Console. Open the navigation menu and click Identity &amp; Security. Under Identity, click Compartments. Click Create Compartment. Fill in the following information: Name: Description: Compartment for . Parent Compartment: (root) Click Create Compartment. Reference: Create a compartment &laquo; Back Continue &raquo; 4 2. Install your Ubuntu Linux Instance Use the Create a VM Instance wizard to create a new compute instance. The wizard does several things when installing the instance: Creates and installs a compute instance running Ubuntu Linux. Creates a VCN with the required subnet and components needed to connect your Ubuntu Linux instance to the internet. Creates an ssh key pair you use to connect to your instance Review Installation Steps To get started installing your instance with the Create a VM Instance wizard, follow these steps: From the main landing page, select Create a VM Instance wizard. The Create Compute Instance page is displayed. It has a section for Placement, Image and shape, Networking, Add SSH keys, and Boot volume. Choose the Name and Compartment. Initial Options: Name: Create in compartment: Enter a value for the name or leave the system supplied default. Review the Placement settings, and click the Show advanced options link. Take the default values. Your data might look similar to the following: Availability domain: Availability domain: AD-1 Capacity type: On-demand capacity Fault domain: Let Oracle choose the best fault domain For Free Tier, use Always Free Eligible option for availability domain. Review the Image and shape settings. Select the latest Ubuntu image. Click Change Image. Select the latest Ubuntu image. Click Select Image. Your image is displayed, for example your data looks similar to the following: Image: Canonical Ubuntu 20.04 Image build: 2020.12.11-0 Take the default values for Shape. For example, your data looks similar to the following: Shape: VM.Standard.E2.1.Micro OCPU count: 1 Memory (GB): 1 Network bandwidth (Gbps): 0.48 For Free Tier, use Always Free Eligible shape options. Review the Networking settings. Take the default values provided by the wizard. The following is sample data. The actual values change over time or differ in a different data center. Virtual cloud network: vcn-- Subnet: vcn-- Assign a public IPv4 address: Yes Review the Add SSH keys settings. Take the default values provided by the wizard. Select the Generate a key pair for me option. Click Save Private Key and Save Public Key to save the private and public SSH keys for this compute instance. If you want to use your own SSH keys, select one of the options to provide your public key. Put your private and public key files in a safe location. You cannot retrieve keys again after the compute instance has been created. Review the Configure boot volume settings. Take the default values provided by the wizard. Leave all check boxes unchecked. Click Create to create the instance. Provisioning the system might take several minutes. You have successfully created an Ubuntu Linux instance. &laquo; Back Continue &raquo; 5 3. Enable Internet Access The Create a VM Instance wizard automatically creates a VCN for your VM. You add an ingress rule to your subnet to allow internet connections on port 80. Create an Ingress Rule for your VCN Follow these steps to select your VCN’s public subnet and add the ingress rule. Open the navigation menu and click Networking, and then click Virtual Cloud Networks. Select the VCN you created with your compute instance. With your new VCN displayed, click subnet link. The public subnet information is displayed with the Security Lists at the bottom of the page. A link to the Default Security List for your VCN is displayed. Click the Default Security List link. The default Ingress Rules for your VCN are displayed. Click Add Ingress Rules. An Add Ingress Rules dialog is displayed. Fill in the ingress rule with the following information. Stateless: Checked Source Type: CIDR Source CIDR: 0.0.0.0/0 IP Protocol: TCP Source port range: (leave-blank) Destination Port Range: 80 Description: Allow HTTP connections Click Add Ingress Rule. Now HTTP connections are allowed. Your VCN is configured for Apache server. Click Add Ingress Rule. Now HTTP connections are allowed. Your VCN is configured for Apache server. You have successfully created an ingress rule that makes your instance available from the internet. &laquo; Back Continue &raquo; 6 4. Install and Configure Apache, PHP 7, MySQL, and WordPress Next install and configure Apache web server and PHP to run on your Ubuntu Linux instance. Configure the Ubuntu Firewall Connect to your Ubuntu instance and configure your firewall settings. Follow these steps: Log into your free tier account. Open the navigation menu and click Compute. Under Compute, click Instances. Click the link to the instance you created in the previous step. From the Instance Access section, write down the Public IP Address the system created for you. You use this IP address to connect to your instance. Open a Terminal window. Change into the directory where you stored the ssh encryption keys you created in part one. Connect to your VM with this SSH command. ssh -i &lt;your-private-key-file&gt; ubuntu@&lt;your-public-ip-address&gt; Since you identified your public key when you created the VM, this command logs you into your VM. You can now issue sudo commands to install and start your server. Update firewall settings. Next, update your iptables configuration to allow HTTP traffic. To update iptables, run the following commands. sudo iptables -I INPUT 6 -m state --state NEW -p tcp --dport 80 -j ACCEPT sudo netfilter-persistent save The commands add a rule to allow HTTP traffic and saves the changes to the iptables configuration files. Install Apache Server Install Apache Server. sudo apt update sudo apt -y install apache2 Next, start Apache. sudo systemctl restart apache2 You can now test your server. You can test your server from the command line with curl localhost. Or, you can connect your browser to the public IP address assigned to your VM: http://&lt;your-public-ip-address&gt;. The page looks similar to: Install PHP Install PHP and then some helpful modules with the following commands. sudo apt -y install php sudo apt -y install php-mysql php-curl php-gd php-zip Verify installation and restart Apache. php -v sudo systemctl restart apache2 Add a PHP test file to your VM. sudo vi /var/www/html/info.php In the file, input the following text and save the file: &lt;?php phpinfo(); ?&gt; Connect to http://&lt;your-public-ip-address&gt;/info.php. The browser produces a listing of PHP configuration on your VM. You have successfully installed Apache and PHP 7 on your Oracle Cloud Infrastructure instance. After you are done testing, delete the info.php file. Configure your Apache HTML Directory Set up your Apache server to read and write from the /var/www/html directory. Add your username to the www-datagroup so you can edit the /var/www/html directory. sudo adduser $USER www-data Now change the ownership of the content directory. sudo chown -R www-data:www-data /var/www/html Change permissions on the files and directory. sudo chmod -R g+rw /var/www/html Reboot your machine for changes to take effect. Install and Configure MySQL Server and Client Next, you install and configure the MySQL server and client so it can be used with WordPress. Install the MySQL Server package. sudo apt -y install mysql-server This step can take some time. Next, perform a secure configuration of MySQL. sudo mysqlsecureinstallation Produces this output: Securing the MySQL server deployment. Connecting to MySQL using a blank password. Turn on Password Validation: VALIDATE PASSWORD COMPONENT can be used to test passwords and improve security. It checks the strength of password and allows the users to set only those passwords which are secure enough. Would you like to set up VALIDATE PASSWORD component? Press y|Y for Yes, any other key for No: Select Y. Select the password validation level. There are three levels of password validation policy: LOW Length &gt;= 8 MEDIUM Length &gt;= 8, numeric, mixed case, and special characters STRONG Length &gt;= 8, numeric, mixed case, special characters and dictionary file Please enter 0 = LOW, 1 = MEDIUM and 2 = STRONG: Select a level. Set the root password. Please set the password for root here. New password: Re-enter new password: Estimated strength of the password: 100 Do you wish to continue with the password provided?(Press y|Y for Yes, any other key for No) : Select the remaining security options. Remove anonymous users? (Press y|Y for Yes, any other key for No) : Disallow root login remotely? (Press y|Y for Yes, any other key for No) : Remove test database and access to it? (Press y|Y for Yes, any other key for No) : Reload privilege tables now? (Press y|Y for Yes, any other key for No) : Success. All done! Log in to MySQL. sudo mysql You are given a MySQL prompt. List the default databases. mysql&gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | +--------------------+ 4 rows in set (0.01 sec) Create a user for MySQL. mysql&gt; CREATE USER '&lt;your-user-name&gt;'@'localhost' IDENTIFIED BY '&lt;your-password&gt;'; Query OK, 0 rows affected (0.01 sec) Make the user an admin. mysql&gt; GRANT ALL PRIVILEGES ON . TO '&lt;your-user-name&gt;'@'localhost'; Query OK, 0 rows affected (0.01 sec) Create your WordPress database. mysql&gt; create database wpdb; Query OK, 1 row affected (0.01 sec) Check the result. mysql&gt;show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | | wpdb | +--------------------+ 5 rows in set (0.00 sec) Flush privileges to clear cached memory. mysql&gt; FLUSH PRIVILEGES; Query OK, 0 rows affected (0.00 sec) mysql&gt; exit; Bye Install and Configure WordPress Download and follow these steps to install WordPress on your server. Open a terminal window and create a tmp directory. Download the WordPress Linux zip from https://wordpress.org/download/ and unzip. wget &lt;url-for-download&gt;.gz tar xvfz &lt;download-file-name&gt;.gz The command creates a wordpress directory with the PHP code for WordPress in it. Copy the contents of the wordpress directory to the /var/www/html directory. cp -R /home/&lt;your-username&gt;/tmp/wordpress/* /var/www/html The contents of the wordpress directory are copied into the /var/www/html directory. This command is a sample. Your command differs depending on the name of your directories. Change into to the /var/www/html directory. cd /var/www/html Rename the default index.html file. mv index.html index.html.bk Now index.php is loaded by default when your root directory is accessed. Rename the wp-config-sample.php file. mv wp-config-sample.php wp-config.php Update the values for your MySQL set up. vi wp-config.php Run the installation script by opening a browser and this URL: http://&lt;your-public-ip-address&gt;/wp-admin/install.php Create an administrator account for your WordPress blog. Ensure you write down the information from the install page. You need it to log into your WordPress blog. Open your new blog at: http://&lt;your-public-ip-address&gt; Finish any other configuration you need for WordPress. Here is a link to help: First Steps with WordPress You have set up a WordPress blog on an Oracle Cloud Infrastructure (OCI) compute instance. &laquo; Back Continue &raquo; 7 What's Next You have successfully installed and deployed an Apache web server on Oracle Cloud Infrastructure using a Linux instance. To explore more information about development with Oracle products, check out these sites: Oracle Developers Portal Oracle Cloud Infrastructure &laquo; Back By Docs @ Oracle","categories": ["modernize"],
        "tags": ["ubuntu","back-end"],
        "url": "/tutorials/freetier-wordpress-ubuntu",
        "teaser": ""
      },{
        "title": "Video Games, Servers, and Development",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/use-cases/games",
        "teaser": ""
      },{
        "title": "Get Started with your own LAMP stack application on Oracle Cloud",
        "excerpt":"I’ve written several articles about how to deploy popular Open Source applications on Oracle Cloud Infrastructure and MySQL Database Service. Now we will see how you can deploy your own LAMP stack application using the same technique where L will stand for a compute instance (and why not the Ampere always free trier?), A stays Apache and will run in that compute instance. M stands for MySQL Database Service and P is for PHP. As usual we start by deploying a Stack by just clicking on the deploy button from GitHub Then we are redirected to OCI’s dashboard and we need to accept the Terms of Use: As soon as we accept the Terms of Use, we see the information being updated, and we can directly click on Next: On the next screen, we can set all the variables. Some are mandatory and the others are already pre-filled: This is also where you can choose which version of PHP you want to use: You validate everything, and then Create the stack and deploy the architecture on OCI: As you can see, we have all the generated and required information in the Outputs section. We can already use the public IP in a browser and we should see the following page: Now the Web server is ready to get our code. From the stack’s outputs, we already know the IP, the username, and password to use to connect to MySQL Database Service. We also need the ssh key that we can copy locally to ssh to the Web Server to deploy our code, or we can use the Cloud Shell from OCI’s dashboard. Let’s use it: We create a file for the ssh key (php.key) and we paste its content in it: We change the permission of the key’s file and we use it to connect to our web server using its public IP: As an application, we will use this gist file which is a PHP script that connects to MDS and we will place it in /var/www/html: We edit it, and there are 3 variables to modify using the values from the Stack’s outputs: When done, we can refresh the page on the browser and we will see our code being processed: The Web server box already contains git and certbot. Now you are able to deploy your own LAMP stack easily to OCI, enjoy! By lefred","categories": ["cloudapps"],
        "tags": ["data-management","front-end","mysql","oci","orm"],
        "url": "/tutorials/get-started-with-lamp-on-oci",
        "teaser": ""
      },{
        "title": "Install Node Express on an Oracle Linux Instance",
        "excerpt":" 1 Introduction In this tutorial, we’ll use an Oracle Cloud Infrastructure Free Tier account to set up an Oracle Linux Compute instance. We’ll install a Node Express application and access your new app from the internet. Finally, we’ll cover all the steps necessary to set up a virtual network for your host and connect the host to the internet. Key tasks include how to: Set up a compartment for your development work Install your Oracle Linux instance and connect it to your Virtual Cloud Network (VCN) Set up an Oracle Cloud Infrastructure Virtual Cloud Network and related network services required for your host to connect to the internet Set up ssh encryption keys to access your Oracle Linux Server Configure ingress rules for your VCN Configure NodeJS with an Express framework on your instance Here is a simplified diagram of the setup for your Linux instance. For additional information, see: Signing Up for Oracle Cloud Infrastructure Launch your first Linux VM Begin &raquo; 2 Before You Begin To successfully complete this tutorial, you must have the following: An Oracle Cloud Infrastructure Free Tier account: Start for Free A MacOS, Linux, or Windows computer with ssh support installed &laquo; Back Continue &raquo; 3 Set up a Compartment for Development Configure a compartment for your development. Create a Compartment Create a compartment for the resources that you create in this tutorial. Log in to the Oracle Cloud Infrastructure Console. Open the navigation menu and click Identity &amp; Security. Under Identity, click Compartments. Click Create Compartment. Fill in the following information: Name: &lt;your-compartment-name&gt; Description: Compartment for &lt;your-description&gt;. Parent Compartment: &lt;your-tenancy&gt;(root) Click Create Compartment. Reference: Create a compartment &laquo; Back Continue &raquo; 4 Install your Oracle Linux Instance Use the Create a VM Instance wizard to create a new compute instance. The wizard does several things when installing the instance: Creates and installs a compute instance running Oracle Linux Creates a VCN with the required subnet and components needed to connect your Oracle Linux instance to the internet Creates an ssh key pair you use to connect to your instance Review Installation Steps To get started with installing your instance using the Create a VM Instance wizard, follow these steps: From the main landing page, select Create a VM Instance. The Create Compute Instance page is displayed. It has a section for Placement, Image and shape, Networking, Add SSH keys, and Boot volume. Choose the Name and Compartment. Initial Options Name: Create in compartment: Enter a value for the name or leave the default. Review the Placement settings. Accept the default values provided by the wizard. The following is sample data. The actual values change over time or differ based on data center. Placement Availability domain: AD-1 Capacity type: On-demand capacity. Fault domain: Oracle chooses the best placement. For Free Tier, use Always Free Eligible option for availability domain. Review the Image and shape settings. Accept the default values provided by the wizard. The following is sample data. The actual values change over time or differ based on data center. Image Image: Oracle Linux 7.9 Image build: 2020.11.10-1 Shape Shape: VM.Standard.E2.1.Micro OCPU count: 1 Memory (GB): 1 Network bandwidth (Gbps): 0.48 Note: For Free Tier, use Always Free Eligible shape options. Review the Networking settings. Accept the default values provided by the wizard. The following is sample data. The actual values change over time or differ based on data center. Virtual cloud network: vcn-&lt;date&gt;-&lt;time&gt; Subnet: vcn-&lt;date&gt;-&lt;time&gt; Assign a public IPv4 address: Yes Review the Add SSH keys settings. Take the default values provided by the wizard. Select the Generate a key pair for me option Click Save Private Key and Save Public Key to save the private and public SSH keys for this compute instance If you want to use your own SSH keys, select one of the options to provide your public key. Put your private and public key files in a safe location. You cannot retrieve keys again after the compute instance has been created. Review the Boot volume settings. Accept the default values provided by the wizard. Leave all check boxes unchecked. Click Create to create the instance. Provisioning the system might take several minutes. You have successfully created an Oracle Linux instance. &laquo; Back Continue &raquo; 5 Enable Internet Access The Create a VM Instance wizard automatically creates a VCN for your instance. We’ll add an ingress rule to the subnet to allow internet connections on port 3000. Create an Ingress Rule for your VCN Follow these steps to select your VCN’s public subnet and add the ingress rule: Open the navigation menu and click Networking, then click Virtual Cloud Networks Select the VCN you created with your compute instance With your new VCN displayed, click &lt;your-subnet-name&gt; subnet link. The public subnet information is displayed with the Security Lists at the bottom of the page. A link to the Default Security List for your VCN is displayed. Click the Default Security List link The default Ingress Rules for your VCN are displayed. Click Add Ingress Rules An Add Ingress Rules dialog is displayed Fill in the ingress rule with the following information: Stateless: Checked Source Type: CIDR Source CIDR: 0.0.0.0/0 IP Protocol: TCP Source port range: (leave-blank) Destination Port Range: 3000 Description: Allow HTTP connections Click Add Ingress Rule Now HTTP connections are allowed. Your VCN is configured for Node Express, and you have successfully created an ingress rule that makes your instance available from the internet. &laquo; Back Continue &raquo; 6 Create a Node Express Application Next, set up an Express framework on your Oracle Linux instance, then create and run a NodeJS application. Install and Set up Node Express Follow these steps to set up your instance and build your application: Open the navigation menu and click Compute. Under Compute, click Instances Click the link to the instance you created in the previous step From the Instance Details page, look in the Instance Access section. Copy the public IP address the system created for you. You use this IP address to connect to your instance. Open a Terminal or Command Prompt window Change into the directory where you stored the ssh encryption keys you created for this tutorial Connect to your instance with this SSH command $ ssh -i &lt;your-private-key-file&gt; opc@&lt;x.x.x.x&gt; Since you identified your public key when you created the instance, this command logs you into your instance. You can now issue sudo commands to install and start your server. Enable HTTP connection on port 3000 $ sudo firewall-cmd --permanent --add-port=3000/tcp $ sudo firewall-cmd --reload Install the latest version of NodeJS $ sudo yum update $ sudo yum install -y oracle-nodejs-release-el7 $ sudo yum install -y nodejs $ node --version Create a directory for your application $ mkdir node-hello-app Change to the node-hello-app directory $ cd node-hello-app Use npm to create a package.json file $ npm init Enter information as follows: name: node-hello-app version: 1.0.0 description: Node Express Hello application entry point: app.js (Don’t use the default.) test command: (leave-blank) git repository: git://github.com/username/repository.git (or replace with a valid git repository) keywords: (leave-blank) author: Example User username@example.com license: UPL-1.0 Preview what you get in package.json About to write to /home/opc/node-hello-app/package.json: { \"name\": \"node-hello-app\", \"version\": \"1.0.0\", \"description\": \"Node Express Hello application\", \"main\": \"app.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" &amp;&amp; exit 1\" }, \"repository\": { \"type\": \"git\", \"url\": \"git://github.com/username/repository.git\" }, \"author\": \"Example User username@example.com\", \"license\": \"UPL-1.0\", \"bugs\": { \"url\": \"https://github.com/username/repository/issues\" }, \"homepage\": \"https://github.com/username/repository#readme\" } Enter yes to approve your answers. Install Express and save it in the dependencies list of package.json $ npm install express --save Verify that express is added as a dependency in package.json $ cat package.json \"dependencies\": { \"express\": \"^4.17.1\" } Create a “Hello, World!” application Create the file: $ vi app.js In the file, input the following text and save the file: const express = require('express') const app = express() app.get('/', function (req, res) { res.send('Hello World!') }) app.listen(3000, function() { console.log('Hello World app listening on port 3000!'); }) Run the NodeJS program $ node app.js Test the application using the command line or a browser To test with curl, from a new terminal, connect to your Ubuntu instance with your SSH keys, and then on the command line enter: curl -X GET http://localhost:3000 From a browser, connect to the public IP address assigned to your instance: http://&lt;x.x.x.x&gt;:3000 The Node app returns Hello World! in your instance or your browser. You have successfully created a local NodeJS application in an Express framework on an Oracle Cloud Infrastructure instance. References: For more information on Express, see: Installing Express Getting Started with Express &laquo; Back Continue &raquo; 7 What's Next You have successfully installed and deployed a Node Express app on Oracle Cloud Infrastructure using a Linux instance. To explore more information about development with Oracle products: Oracle Developers Portal Oracle Cloud Infrastructure &laquo; Back By ","categories": ["frameworks","cloudapps"],
        "tags": ["open-source","oci","always-free","nodejs","javascript","typescript","front-end"],
        "url": "/tutorials/get-started-with-node-express-on-oracle-cloud",
        "teaser": ""
      },{
        "title": "Get started with Ruby on Rails on Oracle Cloud",
        "excerpt":" Introduction Ruby on Rails is a rapid development framework. It’s used by over a million websites, including Airbnb, Bloomberg, GitHub, GitLab, CouchSurfing, and many more. The Ruby language is easy to understand as it’s close to English, making it great for beginners. In this tutorial we’ll show you how to get started with a new Ruby on Rails project and how to bring your existing Ruby on Rails application to Oracle Cloud Infrastructure (OCI). Set up environment automatically Don’t have time to read the whole article? Use the “Deploy Button” to provision the Ruby on Rails environment with a few clicks. In this environment you can build a new application or bring your existing Ruby on Rails application to OCI. The button takes you to Oracle Resource Manager which runs Terraform code that will provision the full Ruby on Rails environment. The following resources will be created: # Service Name Additional Info 1 Virtual Cloud Network   1 Public subnet   1 Private subnet   1 VM - Ubuntu 1 OCPU, 8 Gb Memory 1 OCPU. can be scaled up to 1024 GB Memory and 128 OCPUs 1 MySQL Database Service can be scaled up to run in HA with up to 3 nodes 1 Bastion Host can optionally be replaced with bastion service 1 Load balancer 10 mpbs flex loadbalancer upto 4000 mbps The public subnet will contain the following resources: Load balancer Bastion Host The private subnet will contain the following resources: Ruby on Rails VM(s) Managed MySQL Database Software packages installed on the VMs: RBENV Package manager for ruby to make it easy to manage different ruby versions and gem packs Node.js The bootstrap script in the GitHub repo shows how the VMs are configured and all the packages that are installed: You can check out the script on Github. If you have deployed the environment using the deploy button, you can skip the next section. The bootstrap script has been run automatically when the VMs were provisioned. How to setup environment automatically Click the “Deploy to Oracle” button below. If you aren’t already signed in, when prompted, enter the tenancy and user credentials. Review and accept the terms and conditions. Select the region where you want to deploy the stack. Follow the on-screen prompts and instructions to create the stack. After creating the stack, click Terraform Actions, and select Plan. Wait for the job to be completed, and review the plan. To make any changes, return to the Stack Details page, click Edit Stack, and make the required changes. Then, run the Plan action again. If no further changes are necessary, return to the Stack Details page, click Terraform Actions, and select Apply. How to setup environment manually If you prefer to setup the environment manually you need to provision the resources and then install the Ruby on Rails environment. Provisioning the VM and MySQL database is not covered in this guide. Once you have a VM and MySQL database ready you can proceed to the next step to install Ruby on Rails. Minimum requirements VM running Ubuntu 20.04 MySQL Database How to install Ruby on Rails on Ubuntu 20.04 with rbenv SSH to your VM and run the following commands. Make sure to run the command with a user that has sudo rights. On Ubuntu the ubuntu user is usually fine: Update apt: sudo apt update Install ruby manager, mysql-client and other dependencies: sudo apt-get install -y build-essential git libsqlite3-dev libssl-dev libzlcore-dev mysql-client libmysqlclient-dev git-core zlib1g-dev build-essential libssl-dev libreadline-dev libyaml-dev libsqlite3-dev sqlite3 libxml2-dev libxslt1-dev libcurl4-openssl-dev software-properties-common libffi-dev nodejs npm Install yarn: curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | sudo apt-key add echo \"deb https://dl.yarnpkg.com/debian/ stable main\" | sudo tee /etc/apt/sources.list.d/yarn.list sudo apt update sudo apt install -y yarn Install the ruby package manager to help install and manage different ruby versions: cd git clone https://github.com/rbenv/rbenv.git ~/.rbenv echo 'export PATH=\"$HOME/.rbenv/bin:$PATH\"' &gt;&gt; ~/.bash_profile echo 'eval \"$(rbenv init - bash)\"' &gt;&gt; ~/.bash_profile source ~/.bash_profile git clone https://github.com/rbenv/ruby-build.git ~/.rbenv/plugins/ruby-build echo 'export PATH=\"$HOME/.rbenv/plugins/ruby-build/bin:$PATH\"' &gt;&gt; ~/.bash_profile source ~/.bash_profile Now we are ready to install ruby. If you need a different version, change the version number: rbenv install 3.0.1 Check that the correct version of ruby is installed: ruby -v Use version 3.0.1 and make it default: rbenv global 3.0.1 rbenv local 3.0.1 Reload the environment variables to ensure the correct ruby binaries are run: source ~/.bash_profile Let’s install Rails framework and bundler gem: gem install rails gem install bundler Congratulations, you have now installed Ruby on Rails on your VM! Create your Ruby on Rails application In this section we will create a rails application using the built-in scaffolding methods of rails. We will connect the rails app to the database provisioned. Let’s create a directory where our app will reside and make the ubuntu user the owner of the directory: sudo mkdir /opt/apps sudo chown ubuntu:ubuntu /opt/apps Create a new app that is preconfigured with a mysql adapter: cd /opt/apps rails new myapp -d mysql Open up port 8080 for incoming HTTP requests: sudo iptables -I INPUT 6 -m state --state NEW -p tcp --dport 8080 -j ACCEPT #SAVE STATE Edit the database config file and add your MySQL username and MySQL database name and MySQL host URL: nano /opt/apps/myapp/config/database.yml Create a database schema. If you have already created a database schema you can skip this: rake db:create Run all migration scripts to create the tables in the DB. This step needs to run every time you makes change to table definitions: rake db:migrate Start the rails server as a background process, listen on port 8080, bind all interfaces, and send log output to startup.log: rails s -p 8080 -b 0.0.0.0 &gt;&gt; ./log/startup.log &amp; Accessing the application If you configured your VM manually you should now be able to access your new rails application at http://my-vm-public.ip:8080. If you provisioned your environment automatically using the “Deploy Button” you can access the application through the load balancer, e.g. http://my-load-balancer-ip. Check Oracle Resource Manager output for the IP or check the load balancer page. The load balancer is configured to listen on port 80 and forwards all traffic to the VM where the Ruby on Rails application is running. The load balancer will forward traffic to all the VMs if you provisioned more than one VM. Summary A new Rails App is created in the directory /opt/apps/myapp. A database schema is created on the MySQL server. Database schema name: myapp The app is configured to connect to the database. Configuration can be found in /opt/apps/app/config/database.yml. How to deploy your existing application Before deleting the app we will take a backup of the database.yml file. We’ll need it later. cp /opt/apps/myapp/config/database.yml /opt/apps/bck_database.yml Delete the existing vanilla Ruby on Rails app created in /opt/apps/myapp rm -rf /opt/apps/myapp Copy your application to the server using scp scp myapp.zip ubuntu@server-ip:/opt/apps/ Unpack the application unzip myapp.zip ./myapp/ Copy the generated database config file cp /opt/apps/bck_database.yml ./myapp/config/database.yml Or edit the existing database config file (database.yml) username: password: url: Migrate your data to the MySQL Database Install dependencies bundle install Run Rails migrations rake db:migrate Start the server rails s -b 0.0.0.0 -p 8080 How to change Ruby version The default ruby version installed is 3.0.1. If you need a different ruby version simply run: rbenv install 3.0.2 You can see available Ruby version by running: rbenv install list By Hassan Ajan","categories": ["frameworks","cloudapps"],
        "tags": ["ruby","rails","MySQL","RubyOnRails"],
        "url": "/tutorials/get-started-with-rubyonrails",
        "teaser": ""
      },{
        "title": "Getting Started with Oracle Cloud Infrastructure (OCI)",
        "excerpt":" Over the last two decades, Cloud Computing has revolutionized the technological landscape, allowing companies to rent infrastructure instead of building rapidly-obsolescing hardware on-site. During this time, the architectural approach integrating infrastructure orchestration and application code into cloud services has evolved with it. Today, cloud users have a choice between technology stacks for virtual hosts, master-slave architectures, and container clusters. Each stack wraps application code differently, relies on different methods to launch servers, and provides different mechanisms to automate fail-over and scaling processes. For Enterprise IT organizations managing a large variety of applications, a single stack is usually insufficient to address the broad variety of functional and non-functional requirements. Hence, spreading workloads across multiple cloud providers is a common strategy to address this constraint. However, deploying private workloads across multiple public infrastructure stacks increases operational complexity significantly and comes with certain vulnerabilities. This makes adopting a second generation infrastructure service (IaaS) like Oracle Cloud Infrastructure (OCI) an attractive alternative. Purpose-built hardware enables cloud providers to separate the orchestration layer from the hosts and allows cloud customers to build private data centers on pre-built infrastructure pools. Programming interfaces allow operators to build extensible service delivery platforms and service owners to modernize applications in incremental steps. The bare-metal approach allows customers to run enterprise applications in a traditional way. Cloud orchestrators remain optional, and can be added as managed service, but even then the user remains in control over the infrastructure resources. Oracle operates a fast-growing network of data centers to provide access to pre-built cloud infrastructure in more than 30 regions. In addition, Oracle builds private infrastructure pools on-demand, and offers to extend these pools with edge compute or database nodes. In every data center, dedicated compute and storage resources are isolated on a native layer three network. Orchestrators, including hypervisor, container, and network functions, remain private by default and also exist in shared pools. Combining open-source orchestration technologies with cloud-agnostic monitoring and management services allows operators to build a control center for hybrid cloud services. End-to-end programmability ensures fast and flexible resource deployments. Platform components like middleware, integration, and database infrastructure, provided either as managed or as unmanaged services, offer a choice between convenience and control. At the same time, standard hardware controls like the integrated lights out manager (ILOM) and off-box virtualization allow customers to address regional privacy regulations and compliance requirements. Oracle Cloud Infrastructure (OCI) is a deep and broad platform of public cloud services that enables customers to build and run a wide range of applications in a scalable, secure, highly available, and high-performance environment. For on-premises requirements, OCI is available with the new Dedicated Region Cloud@Customer located both behind a company’s private firewall and in their data center. A detailed “Getting Started Guide” is part of our documentation and available here: Getting Started with OCI. Oracle Cloud Infrastructure benefits Autonomous services Oracle Cloud Infrastructure (OCI) is the exclusive home of the Oracle Autonomous Database and its self-repairing, self-optimizing autonomous features. Leveraging machine learning to automate routine tasks, Autonomous Database delivers higher performance, better security, and improved operational efficiency while freeing up more time for you to focus on building enterprise applications. For more information see: Gartner: Critical Capabilities for Operational Database Management Systems Easily migrate enterprise apps, reduce costs, and enhance performance OCI is built for enterprises seeking higher performance, lower costs, and easier cloud migration for existing on-premises applications all while offering better price-performance for cloud native workloads. For more information see: Learn why Oracle apps run best on OCI Best support for hybrid architecture Traditional, on-premises workloads that enterprises rely on to run their business are easier to migrate to Oracle Cloud. Designed to deliver bare-metal compute services, network traffic isolation, and the only service-level guarantees for performance, Oracle Cloud enables rapid migration and faster time to innovation. Build new value around migrated applications faster with Autonomous Database, data science tools, and our cloud native development tools. For more information see: Learn why Oracle apps run best on OCI Start migrating your custom apps to OCI Best Support for Hybrid Architecture Deploy your cloud applications and databases anywhere with a wide choice of options, ranging from our public regions to edge devices. In addition to our public cloud region, we offer full private Dedicated Regions in customer data centers, edge-computing Oracle Roving Edge devices, and our blazingly-fast Oracle Exadata Cloud@Customer with Autonomous Database services delivered behind your firewall. With full support for VMware environments in the customer tenancy as well, Oracle offers cloud computing that works the way you expect. For more information see: Learn about hybrid, multi-cloud, and inter-cloud deployment options Oracle Brings the Cloud to You (PDF) Read about customer experiences with OCI If you would like to learn more about how customers have moved away from AWS to embrace OCI, check out some of these articles below: Compare against AWS Read Gartners perspective on Oracles public cloud What’s next Cloud operations engineering is a relatively new field which extends the scope of IT service management (ITSM). It represents an important step towards more agility and flexibility in service operation. The concept of “Infrastructure as Code” replaces runbook tools and has become an enabler of self-service delivery, even for complex solution architectures. Operators empower service owners and developers to add, change, or delete infrastructure on demand with deployment templates for logical resources. Service consumers gain flexibility to provision virtual infrastructure, while resource operators remain in control of the physical infrastructure. This series aims to provide a path for IT organizations introducing cloud engineering. We start with a short introduction of Infrastructure as Code (IaC), show how to define logical resources for application and database services, and end with an example showing how to consolidate infrastructure and application services in a self-service catalogue. We’ll build on the official Oracle Cloud Architect training which prepares administrators for the Oracle Architect Associate exam, but extend the Learning Path with tool recommendations and code examples for cloud engineers. This introduction targets future operation engineers and is structured as follows: Automating with Terraform Base Configuration Database Infrastructure Application Infrastructure Workload Deployment Governance Vizualizer Next up, automating OCI with Terraform! By Malte Menkhoff","categories": ["iac","opensource"],
        "tags": ["open-source","terraform","iac","devops","get-started"],
        "url": "/tutorials/oci-iac-framework/getting-started-with-oci-intro",
        "teaser": ""
      },{
        "title": "Automating OCI with Terraform",
        "excerpt":" Introducing Oracle Cloud Infrastructure (OCI) for service delivery allows operators to represent physical and virtual infrastructure in the form of code. System administrators ensure server uptime and service levels, responding to infrastructure monitoring alerts and resolving incidents programmatically. Adopting “Infrastructure as Code” (IaC) helps operators to connect events and state changes with automated provisioning processes. While requesting a resource from a cloud provider is a matter of opening the console, providing input values, and launching the server, addressing functional and non-functional requirements of an IT operation during the launch process is more complex. Provisioning plans help to determine the correct framework, reflect the operational and management tools, and configure resources according to security and compliance regulations. In OCI, we rely on Terraform to automate provisioning processes. The widely-adopted, open-source tool allows engineers to translate “declarative” syntax written in JSON or Hashicorp’s Configuration Language (HCL) into API calls. Getting Started One of the key benefits of using Terraform is that it’s orchestrator agnostic. When needed, the execution environment can be dynamically extended with hypervisor-specific interfaces: Terraform templates comprise the physical infrastructure, network functions and orchestrators for distributed systems. Topology templates comprise configuration scripts that initiate service deployments upon the successful creation of a resource. Terraform’s local-exec and remote-exec blocks run configuration scripts that install software components for service instances. Operators combine application code and/or binaries with virtualization and an orchestration environment of their choice into comprehensive build plans. The process of provisioning infrastructure is merged with the continuous integration and deployment chain for application code. Combining Terraform with OCI, enterprise IT departments benefit from: Increased flexibility and agility in operation Relying on automated deployment allows you to centralize topology management in code repositories. While Terraform is creating resources, state is maintained. This helps to prevent confusion among engineering teams. The desired state for all resources is compared to the current state and the delta defines the provisioning sequence for the requested changes. This allows operators to launch virtual data center in more than 30 locations and rely on a single script source to maintain company-wide security and compliance standards. Integrated cloud services Database and application infrastructure is separated in OCI. The terraform service provider combines deployment scripts for database services with provisioning scripts for applications relying on open source orchestrators and commercial hypervisors like Kubernetes and VMWare vSphere. Engineers merge cloud-native services and traditional enterprise applications into comprehensive deployment templates. Combining Oracle’s orchestrator-agnostic approach for the applications stack with separate infrastructure for multi-model databases on a native layer three network, service owners modernize existing applications incrementally and adopt cloud native services in digestible steps. Doing so, they avoid rewriting entire applications before migrating to a managed infrastructure stack. Shadow IT prevention Terraform separates code and execution environments and allows you to maintain security and cost-control while enabling self-service application deployments. Defining resources in code allows you to define launch parameters and user credentials programmatically, take dependencies during the provisioning process into account, and define the sequence of resources that are deployed. Engineers describe a target topology and terraform determines the steps required to create a resource automatically. Leveraging flags like the region argument, modules are executed in different regions and availability/security domains. This allows modules to be defined which represent services in a self-service portal, with admin expertise and configuration dependencies reflected in the predefined deployment process. We refer to resources that are programmatically defined as custom or logical resources. The learning curve of adding orchestrator-specific service providers to the OCI core services is not very steep, because syntax patterns that are used to code infrastructure on orchestrators remain the same. Command Line Access One of the quickest and easiest ways to start is to obtain an OCI account by signing up for the Free Tier. Because Oracle assigns dedicated resources to every user, new registrations can be prevented by a lack of resources in a specific region. Selecting a different region for the registration usually helps to overcome the hurdle. After obtaining an account we log into the web console and start the Cloud Shell with the ** &gt;_ ** button. The cloud shell launches in the region that is active in the cloud console. For the initial setup, the home region of a tenant should be selected. When executing deployment plans, Terraform depends on the directory structure to identify files that belong to a plan. mkdir ~/project In the project directory we create a file with the name hello.tf. cd ~/project &amp;&amp; touch hello.tf Terraform code is stored in a text file that ends with .tf. We open the file with an editor of choice. The cloud shell comes with vim and nano preinstalled. nano ~/project/hello.tf Hello World Terraform uses a configuration language called HashiCorp Configuration Language (HCL) for templates that characterize components of a topology. HCL is a declarative language and instructions are stored in blocks. The syntax of HCL looks as follows: &lt;BLOCK TYPE&gt; \"&lt;BLOCK project&gt;\" { &lt;IDENTIFIER&gt; = &lt;EXPRESSION&gt; } Infrastructure components are defined adopting an input-output model. Input parameter are passed to a build plan, resources definitions describe the provision process and output parameters return the results. This principle can be explained using a simple “Hello World” example. First we define an input variable. The input block contains type constraints as arguments for interpretation. Simple types are string, number, and bool, while complex types are list and object. The resource block remains empty for now, because we’re not aiming to provision any resources. The output block returns the execution results. Referring to an output variable instructs Terraform to execute the referenced block, so the sequence inside the code isn’t actually relevant. # Input variable \"input\" { type = string default = \"Hello World\" } # Function resource \"null_resource\" \"nothing\" {} # Output output \"greeting\" { value = var.input } Since HCL scripts don’t include a shebang, we need to call terraform explicitly. We store the file as ~/project/hello.tf and run the terraform command. Before executing terraform, we need to initialize the working directory: cd ~/project &amp;&amp; terraform init The command allows for additional subcommands using the format: terraform &lt;subcommand&gt; -options The -auto-approve option forces terraform to execute the command without further confirmation: terraform apply -auto-approve The command only returns with an output of the variable: greeting = Hello World Note: After this example, the ~/project/hello.tf is no longer needed. Provider Configuration In order to create resources in OCI, we need to configure terraform. We can do this by creating a basic configuration file: rm ~/project/hello.tf &amp;&amp; nano ~/project/config.tf The first block is the provider block used to load the latest service provider. A service provider is a plugin for the provisioning API and translates HCL code into API calls. Since the API is continuously evolving, adding a provider block forces terraform to load the latest version before executing any scripts. In the cloud shell, an empty block is sufficient since all of the necessary arguments are stored as the default parameter. # Setup the oci service provider provider \"oci\" { } Next, we instruct terraform to address a specific account, referring to the Oracle Cloud Resource Identifier (OCID). We define the tenancy_ocid as an empty input parameter and retrieve the OCID from the command line. # Input parameter to request the unique identifier for an account variable \"tenancy_ocid\" { } With that, the communication is established and we can use the cloud controller as data source. A data block sends a request to the controller and returns account specific information in JSON format. # Retrieve meta data for tenant data \"ociidentitytenancy\" \"account\" { tenancyid = var.tenancyocid } Here, we’ll define an output block by setting the result as a parameter and echoing it to the console: # Output tenancy details output \"tenancy\" { value = data.ociidentitytenancy.account } Now, we close the file and test the integration with the plan command. Because we left the input variable for the tenancy ID empty, we need to refer to a environment setting. The unix command printenv unveils the respective environment variable: printenv | grep TENANCY With the -var argument, we provide the expected input and run terraform: cd ~/project &amp;&amp; terraform init &amp;&amp; terraform plan -var tenancyocid=$OCITENANCY Home Region OCI is available in multiple regions, with every region representing one or more a physical data centers. Multi-data-center regions like FRA, PHX, IAD, and LHR are managed as a single resource pool with multiple availability domains (AD). All regions are managed through a single API, which makes global service roll-outs simple. We can target a specific region using the region argument inside a block. In the console, the active region is shown in the upper-right corner, and in the cloud shell the command prompt shows the active region. Some resources like compartments are defined once and replicated into every region. These are referred to as global resources. Global resources should be created in the “home” region. We use the locals block to define a function that determines the home region for a tenant: # Create a region map locals { region_map = { for city in data.ociidentityregions.global.regions : city.key =&gt; city.name } home_region = lookup( local.region_map, data.ociidentitytenancy.account.homeregionkey ) } Region identifiers in OCI are provided either in a long format that includes the AD (e.g., eu-frankfurt-1) or as a simple three-letter key (e.g., FRA.) Even though HCL is a templating language, expressions allow us to use scripts as primitives. We use this functionality in a local block to construct the right input format: # Get the list of regions data \"ociidentityregions\" \"global\" { } The ociidentitytenancy block in setting.tf delivers the home-region key that we match with the long format in map that contains all OCI regions. The map is retrieved as a “global” data block. The result is an identifier we can use as argument in the provider block. Terraform Workflow Executing a build plan is reflected in a sequence of terraform commands which validate block definitions and quantify and provision the required resources. The steps in the Terraform workflow are closely related to the lifecycle of resources on cloud platforms. As you recall from earlier, these steps are orchestrator-agnostic, meaning that the commands are valid for creating, updating, and destroying resources on any given orchestrator. validate Before executing terraform commands, we validate the HCL and JSON syntax. Terraform validate is a parser that checks the syntax structure for obvious errors: terraform validate init When the code is error free, we run the init command to instruct terraform to check and load the referenced service provider plug-ins from the provider registry and store the plugin in the .terraform sub directory. Third party service providers can be added by defining additional provider blocks: terraform init plan After that, we run a plan command to check the list of resource changes before executing a deployment plan. The plan command indicates which resources will be created, updated, or deleted in order to adjust the current architecture to match the desired architecture. The -out config.tfplan extension instructs terraform to store the execution plan for a later apply: terraform plan -var tenancyocid=$OCITENANCY -out config.tfplan apply The apply command executes the script. We are not provisioning any resources yet and so use the -auto-approve flag to skip the explicit confirmation: terraform apply \"config.tfplan\" -auto-approve Executing the script instructs Terraform to provision the required resources and to return the output variables in the shell. Once confirmed, it takes a few seconds to complete the creation of the resources on OCI. The completion of the command is indicated with the output messages. State Management Terraform is a state-aware deployment tool, meaning it generates a file with the apply command that reflects the deployment status. The state file allows operators to track the current state of their resources. State information is maintained in syntax similar to JSON. The file is located in a hidden folder .terraform/terraform.tfstate. Editing the state file directly is not recommended. cat ~/training/terraform.tfstate This file captures terraform the results of every deployment process. State awareness sets Terraform apart from most configuration management systems used for cloud deployments, because it allows for “declarative” deployments. The admin only defines the desired architecture in the build plan and when Terrafrom is executed, the program automatically determines the difference between the as-is and to-be architecture, either deploying or destroying the required resources. Terraform.tfstate file example: # Terraform.tfstate file example { \"version\": 3, \"terraform_version\": \"0.11.8\", \"lineage\": \"35a9fcf6-c658-3697-9d74-480408535ce6\", \"modules\": [ { \"path\": [\"root\"], …………………………………… \"depends_on\": [] } ]&amp; } Terraform provides the show command for operators to review the current state. This command retrieves all information captured in the state file. Specifying the -json option transfers the data into a valid json format: terraform show -json With a JSON parser like jq we can filter infrastructure information for further use. In case of an error, the json validator allows us to check the JSON structure and the jq playground to develop the filter syntax before applying it: terraform show -json | jq .values.outputs.tenancy.value.homeregionkey -r This example shows how to use jq to filter the home region of your tenant from json output. It returns the three letter value, e.g., FRA for the home region. Output With terraform output will retrieve the defined output parameter from the state file. Adding the name of a particular block, the response returns only the data of a specific block: terraform output home We can use this command to analyze an existing tenancy from the command line (i.e, resources can only be deployed in regions that have been activated by the account owner). One option is to leverage the OCI command line interface to retrieve a list of regions have already been activated: oci iam region-subscription list --query 'data[*]' Terraform allows operators to use the same API but provide a list of regions in JSON format. We create a small module in a subdirectory. This modules creates a JSON output containing all regions that a tenancy has subscribed to: mkdir ~/project/subscriptions &amp;&amp; nano ~/project/subscriptions/region.tf The method ociidentityregion_subscriptions retrieves the list of activated regions form the service provider. With the output block we reformat the list into a list of provider arguments: variable \"tenancy_ocid\" { } output \"subscriptions\" { value = [ for subscription in data.ociidentityregionsubscriptions.activated.regionsubscriptions:{ \"alias\" = subscription.region_key \"region\" = subscription.region_name } ] } data \"ociidentityregionsubscriptions\" \"activated\" { tenancyid = var.tenancy_ocid } After an initial apply, we use the terraform output -json command and parse the list with jq to create a terraform input file. In this example, the list contains a set of regional providers. We then use a bash command to transfer the result into ~/project/provider.tf.json file: terraform output -json | jq '[{provider: {oci: .providers.value[]}}]' &gt; p &amp;&amp; mv ./p ../provider.tf.json Next up, the Service Delivery Framework. By Malte Menkhoff","categories": ["iac","opensource"],
        "tags": ["open-source","terraform","iac","devops","get-started"],
        "url": "/tutorials/oci-iac-framework/getting-started-with-oci-step-1-provider",
        "teaser": ""
      },{
        "title": "Service Delivery Framework",
        "excerpt":" At it’s core, Oracle Cloud Infrastructure (OCI) is a programable data center, providing dedicated infrastructure in more than 30 locations worldwide. The share-nothing design allows IT operators to launch private clouds on demand and enables enterprises to adopt managed services into an existing operation. This framework is inspired by the Center for Internet Security (CIS) landing zone and helps Information Technology Infrastructure Library (ITIL) oriented organizations build and launch both private or public cloud services. Prerequisites Customizing the framework enables the application provider to manage multi-tenant services with clients shielded on the network layer. We recommend that you study the following material before approaching this tutorial: Compartments, Group-, Policy- and User-Templates Documentation Video Virtual Cloud Network Documentation Video Key Vault Documentation Video Object Store Documentation Video Code Structure We employ Infrastructure as Code (IaC) to combine dedicated resources with managed cloud- and orchestration services into custom resources. The code is separated into multiple definition files that Terraform merges into one deployment plan at the time of execution. The following structure uses compartments to reflect shared service center for independent businesses or business units that are separated on the network layer. Nr. Domain File Resources   1 Applications app.tf Hosts (VM &amp; BM), instance groups and container cluster optional 2 Database Infrastructure db.tf CDB or PDB optional 3 Network Topology net.tf Virtual Cloud Network, Layer-3 Gateways required 4 Operations and Security ops.tf Monitoring and management required 5 Operations and Security global.tf Global variables, data sources and naming conventions required 6 Operations and Security default.tfvars Default parameter for a project required Templates In the background, we build on a modular code structure that uses Terraform modules to employ OCI resources and services. Templates for a predefined network topology and isolated database infrastructure extend the application oriented DevOps processes with customized resources. Using declarative templates provides operators with the flexibility to adjust their service delivery platform as requirements evolve. Global input parameters help maintain readability of the code and avoid repeating definitions. We use the ~/project/default.tfvars file to define global input parameters for an entire project: variable \"tenancy_ocid\" { } variable \"organization\" { type = string description = \"provide a string that identifies the commercial owner of a service\" default = \"org\" # Define a name that identifies the project validation { condition = length(regexall(\"^A-Za-z{1,7}$\", var.owner)) &gt; 0 errormessage = \"The servicelabel variable is required and must contain alphanumeric characters only, start with a letter and 5 character max.\" } } variable \"project\" { type = string description = \"provide a string that refers to a project\" default = \"name\" # Define a name that identifies the project validation { condition = length(regexall(\"^A-Za-z{1,7}$\", var.project)) &gt; 0 errormessage = \"The servicelabel variable is required and must contain alphanumeric characters only, start with a letter and 8 character max.\" } } variable \"stage\" { type = string description = \"define the lifecycle status\" default = \"dev\" # Lifecycle stage for the code base validation { condition = length(regexall(\"^A-Za-z{1,7}$\", var.stage)) &gt; 0 errormessage = \"The servicelabel variable is required and must contain alphanumeric characters only, start with a letter and 3 character max.\" } } variable \"region\" { default = \"us-ashburn-1\" validation { condition = length(trim(var.region,\"\")) &gt; 0 error_message = \"The region variable is required.\" } } variable \"owner\" { type = object({ user_ocid = string api_fingerprint = string apiprivatekey_path = string privatekeypassword = string }) description = \"refers to the technical owner of the tenancy\" default = { \"user_ocid\" : \"\", \"api_fingerprint\" : \"\", \"apiprivatekey_path\" : \"\", \"privatekeypassword\" : \"\" } } global.tf The ~/project/global.tf file contains common datasources and functions that can be utilized throughout the entire stack. provider \"oci\" { region = var.region tenancyocid = var.tenancyocid userocid = var.root.userocid fingerprint = var.root.fingerprint privatekeypath = var.root.privatekeypath privatekeypassword = var.root.privatekeypassword } provider \"oci\" { alias = \"home\" region = local.regionsmap[local.homeregion_key] tenancyocid = var.tenancyocid userocid = var.root.userocid fingerprint = var.root.fingerprint privatekeypath = var.root.privatekeypath privatekeypassword = var.root.privatekeypassword } ## --- data sources --- data \"ociidentityregions\" \"global\" { } # Retrieve a list OCI regions data \"ociidentitytenancy\" \"ocloud\" { tenancyid = var.tenancyocid } # Retrieve meta data for tenant data \"ociidentityavailabilitydomains\" \"ads\" { compartmentid = var.tenancy_ocid } # Get a list of Availability Domains data \"ociidentitycompartments\" \"root\" { compartmentid = var.tenancyocid } # List root compartments data \"ociobjectstoragenamespace\" \"ns\" { compartmentid = var.tenancyocid } # Retrieve object storage namespace data \"ocicloudguardtargets\" \"root\" { compartmentid = var.tenancy_ocid } data \"templatefile\" \"adnames\" { # List AD names in home region count = length(data.ociidentityavailabilitydomains.ads.availabilitydomains) template = lookup(data.ociidentityavailabilitydomains.ads.availabilitydomains[count.index], \"name\") } ## --- input functions --- # Define the home region identifier locals { # Discovering the home region name and region key. regionsmap = {for rgn in data.ociidentity_regions.global.regions : rgn.key =&gt; rgn.name} # All regions indexed by region key. regionsmapreverse = {for rgn in data.ociidentityregions.global.regions : rgn.name =&gt; rgn.key} # All regions indexed by region name. homeregion = data.ociidentitytenancy.ocloud.homeregion_key # Home region key obtained from the tenancy data source regionkey = lower(local.regionsmap_reverse[var.region]) # Region key obtained from the region name # Setting network access parameters anywhere = \"0.0.0.0/0\" validservicegatewaycidrs = [\"oci-${local.regionkey}-objectstorage\", \"all-${local.region_key}-services-in-oracle-services-network\"] # Service label dns_label = format(\"%s%s%s\", substr(var.owner, 0, 3), substr(var.project, 0, 5), substr(var.stage, 0, 3)) displayname = upper(\"${var.owner}${var.project}_${var.stage}\") } ## --- global output parameter --- output \"account\" { value = data.ociidentitytenancy.ocloud } output \"namespace\" { value = data.ociobjectstoragenamespace.ns.namespace } output \"adnames\" { value = sort(data.templatefile.ad_names.*.rendered) } # List of ADs in the selected region Network Design Before provisioning any compute or storage resources we’ll first need to set up a basic network. To do this, we’ll start with the compartment for network operation. One of the unique features of OCI is the virtual layer 2 network design. Compared to the common network overlays in public clouds, this design provides the necessary control to create an isolated data center on a shared infrastructure pool. Packet encapsulation shields private traffic on a shared network backbone by defining overlapping IP ranges. This allows for a multi-tenant design on the infrastructure layer and frees developers and operators from relying on complex procedures to build and maintain multi-tenant applications. The following diagram exemplifies the topology in a multi data centre region. Virtual Cloud Network A Virtual Cloud Network (VCN) contains a private “Classless Inter-Domain Routing (CIDR)” and can be extended with publicly addressable IP addresses: Even though we need to distinguish the physical topology of single- and multi-data center regions, the logical network layer remains the same, because the data centers are connected through a close network and packet forwarding relies on host routing mechanisms. Regional subnets enable operators to launch multi-data center networks for both private and public cloud services. In addition to the CIDR, the VCN definition contains the Dynamic Routing Gateway (DRG). The DRG acts as both an IP peer and host for network functions like Internet Connectivity, Network Address Translation (NAT), or Private-Public Service Communication. CIDR Subnet Mask Total IP Usable IP /32 255.255.255.255 1 1 /31 255.255.255.254 2 0 /30 255.255.255.252 4 2 /29 255.255.255.248 8 6 /28 255.255.255.240 16 14 /27 255.255.255.224 32 30 /26 255.255.255.192 64 62 /25 255.255.255.128 128 126 /24 255.255.255.0 256 254 /23 255.255.254.0 512 510 /22 255.255.252.0 1024 1022 /21 255.255.248.0 2048 2046 /20 255.255.240.0 4096 4094 /19 255.255.224.0 8192 8190 /18 255.255.192.0 16,384 16,382 /17 255.255.128.0 32,768 32,766 /16 255.255.0.0 65,536 65,534 /15 255.254.0.0 131,072 131,070 /14 255.252.0.0 262,144 262,142 /13 255.248.0.0 524,288 524,286 /12 255.240.0.0 1,048,576 1,048,574 /11 255.224.0.0 2,097,152 2,097,150 /10 255.192.0.0 4,194,304 4,194,302 /09 255.128.0.0 8,388,608 8,388,606 /08 255.0.0.0 16,777,216 16,777,214 /07 254.0.0.0 33,554,432 33,554,430 /06 252.0.0.0 67,108,864 67,108,862 /05 248.0.0.0 134,217,728 134,217,726 /04 240.0.0.0 268,435,456 268,435,454 /03 224.0.0.0 536,870,912 536,870,910 /02 192.0.0.0 1,073,741,824 1,073,741,822 /01 128.0.0.0 2,147,483,648 2,147,483,646 /00 0.0.0.0 4,294,967,296 4,294,967,294 VCN definition: We start the VCN definition with the network parameter: # VCN parameters variable \"create_net\" { default = false } variable \"cidr\" { default = \"10.0.0.0/16\" } variable \"enable_routing\" { default = true } variable \"enable_internet\" { default = false } variable \"enable_nat\" { default = true } variable \"private_service\" { default = false } hostname: In the sources file we define a valid hostname that refers to the owner and the lifecycle stage of an infrastructure platform: # Create a valid hostname locals { hostname = \"${var.project}_${var.stage}\" } Resource blocks: Next, we define the following resource blocks in the file, network.tf: The network compartment. Group and policy block - Here we’ll need a group and policy block that allows administrators to read all the resources in the tenancy and manage all the networking resources, except for security lists, internet gateways, IPSec VPN connections, and customer-premises equipment. VCN - For the VCN definition, we rely on a terraform module that combines the layer three gateways with the CIDR. # Create a compartment network management resource \"ociidentitycompartment\" \"net\" { #Required compartment_id = var.net name = \"${var.project}_network\" description = \"Compartment to manage network for ${var.project}\" #Optional enable_delete = false // true will cause this compartment to be deleted when running terrafrom destroy # defined_tags = {\"terraformed\": \"yes\", \"budget\": 0, \"stage\": var.stage} freeform_tags = {\"source\": \"/code/setup\", \"Parent\"=\"root\"} } # Create the network administrator role resource \"ociidentitygroup\" \"netops\" { #Required compartmentid = var.tenancyocid name = \"${var.project}_netops\" description = \"Group for the network administrator role\" #Optional # defined_tags = {\"terraformed\": \"yes\", \"budget\": 0, \"stage\": var.stage} freeform_tags = {\"source\": \"/code/setup\", \"Parent\"=\"root\"} } # Define a the administration policies for network administrators resource \"ociidentitypolicy\" \"netops\" { name = \"netops\" description = \"Policies for the network administrator role\" compartmentid = var.tenancyocid statements = [ \"ALLOW GROUP ${ociidentitygroup.netops.name} to manage vcns IN TENANCY\", \"ALLOW GROUP ${ociidentitygroup.netops.name} to manage subnets IN TENANCY\", \"ALLOW GROUP ${ociidentitygroup.netops.name} to manage route-tables IN TENANCY\", \"ALLOW GROUP ${ociidentitygroup.netops.name} to manage dhcp-options IN TENANCY\", \"ALLOW GROUP ${ociidentitygroup.netops.name} to manage drgs IN TENANCY\", \"ALLOW GROUP ${ociidentitygroup.netops.name} to manage cross-connects IN TENANCY\", \"ALLOW GROUP ${ociidentitygroup.netops.name} to manage cross-connect-groups IN TENANCY\", \"ALLOW GROUP ${ociidentitygroup.netops.name} to manage virtual-circuits IN TENANCY\", \"ALLOW GROUP ${ociidentitygroup.netops.name} to manage vnics IN TENANCY\", \"ALLOW GROUP ${ociidentitygroup.netops.name} to manage vnic-attachments IN TENANCY\", \"ALLOW GROUP ${ociidentitygroup.netops.name} to manage load-balancers IN TENANCY\", \"ALLOW GROUP ${ociidentitygroup.netops.name} to use virtual-network-family IN TENANCY\", \"ALLOW GROUP ${ociidentitygroup.netops.name} to read all-resources IN TENANCY\", ] } # Launch the base network module \"vcn\" { source = \"oracle-terraform-modules/vcn/oci\" version = \"2.2.0\" # required inputs compartmentid = ociidentity_compartment.net.id drgdisplayname = \"${var.project}${var.stage}DRG\" region = local.home_region vcndnsproject = local.hostname vcnname = \"${var.project}${var.stage}_VCN\" #internetgatewayrouterules = list(object({ destination = string destinationtype = string networkentityid = string description = string })) #natgatewayrouterules = list(object({ destination = string destinationtype = string networkentityid = string description = string })) # optional inputs createdrg = var.enablerouting internetgatewayenabled = var.enable_internet project_stage = local.hostname lockdowndefaultseclist = true natgatewayenabled = var.enable_nat servicegatewayenabled = var.private_service vcn_cidr = var.cidr tags = { \"module\": \"oracle-terraform-modules/vcn/oci\", \"terraformed\": \"yes\", \"budget\": 0, \"stage\": var.stage } } output.tf - In the output.tf file we add the reference to the module output: # VCN parameter returns output \"vcnid\" { value = module.vcn.vcnid } output \"igrouteid\" { value = module.vcn.igrouteid } output \"natgatewayid\" { value = module.vcn.natgatewayid } output \"natrouteid\" { value = module.vcn.natrouteid } Service Operation Compartments denote a demarcation for administrator domains in OCI, and compartment membership determines the privilege to add, change, or delete resources. Compartment structure When we define our compartment structure we need to keep the ITIL model in mind. The first compartment defines the working environment for service operators and enables processes like incident or problem management. While ITIL distinguishes between technical management services and application management services, we rely on Infrastructure as a Service and separate network- and database-managers in distinct compartments. On the application layer, we distinguish between application management and application development. The later compromises platform services and allows us to define our own code chain, while application managers receive the necessary rights to deploy and manage binaries. The definitions are captured in the ~/starter/operation.tf template. Operator roles First, we create a set of roles with privileged access to operation data and tools. Cloud operators make sure that services are delivered effectively and efficiently. This includes fulfilling user requests, resolving service failures, fixing problems, as well as carrying out routine operational tasks. These roles get provisioned in the form of groups. Group policies allow us to define the different administrator roles on a granular level. Initially, we’ll stick to four groups: a cloud account administrator, security manager, user manager, and “readonly” (e.g., for auditors). HCL In HashiCorp Configuration Language (HCL), we use a complex variable type, a map, to describe the different roles: # the base set of operator roles variable \"operator\" { type = map default = { \"cloudops\" = [ \"ALLOW GROUP tenant to read users IN TENANCY\", \"ALLOW GROUP tenant to read groups IN TENANCY\", \"ALLOW GROUP tenant to manage users IN TENANCY\", \"ALLOW GROUP tenant to manage groups IN TENANCY where target.group.name = 'Administrators'\", \"ALLOW GROUP tenant to manage groups IN TENANCY where target.group.name = 'secops'\", ] \"iam\" = [ \"ALLOW GROUP userid to read users IN TENANCY\", \"ALLOW GROUP userid to read groups IN TENANCY\", \"ALLOW GROUP userid to manage users IN TENANCY\", \"ALLOW GROUP userid to manage groups IN TENANCY where all {target.group.name ! = 'Administrators', target.group.name ! = 'secops'}\", ] \"secops\" = [ \"ALLOW GROUP security to manage security-lists IN TENANCY\", \"ALLOW GROUP security to manage internet-gateways IN TENANCY\", \"ALLOW GROUP security to manage cpes IN TENANCY\", \"ALLOW GROUP security to manage ipsec-connections IN TENANCY\", \"ALLOW GROUP security to use virtual-network-family IN TENANCY\", \"ALLOW GROUP security to manage load-balancers IN TENANCY\", \"ALLOW GROUP security to read all-resources IN TENANCY\", ] \"readonly\" = [ \"ALLOW GROUP read_only to read all-resources IN TENANCY\" ] } } Modify group resources for each role We modify the group resource to reflect the list of roles. In a later stage, we’ll use our own resource to assign user accounts to one of these roles. From a Terraform perspective, we introduce a loop type to help automate the role assignment process. With count we create an ordered list and we can use the index to refer to the stored value. While each.key loops through the user list, [0] refers to the first group. Create a service operation compartment # Create a service operation compartment resource \"ociidentitycompartment\" \"operation\" { provider = oci.home #Required compartmentid = var.tenancyocid name = \"${var.project}${var.stage}_ops\" description = \"Compartment to manage ${var.project} ${var.stage} services\" #Optional enable_delete = false // true will cause this compartment to be deleted when running terraform destroy # defined_tags = {\"terraformed\": \"yes\", \"budget\": 0, \"stage\": var.stage} freeform_tags = {\"source\": \"/code/setup\", \"Parent\"=\"root\"} } resource \"ociidentitygroup\" \"operators\" { provider = oci.home for_each = var.operator #Required compartmentid = var.tenancyocid name = each.key description = \"group for the ${each.key} role\" #Optional # defined_tags = {\"terraformed\": \"yes\", \"budget\": 0, \"stage\": var.stage} freeform_tags = {\"source\": \"/code/setup\", \"Parent\"=\"root\"} } resource \"ociidentitypolicy\" \"operation\" { provider = oci.home for_each = var.operator #Required compartmentid = var.tenancyocid name = each.key description = \"Policies for the ${each.key} operator\" statements = each.value } output file In the output file, we create a map containing the name and respective OCID for the new defined roles: output \"operator\" { value = { for operator in ociidentitygroup.operators: operator.name =&gt; operator.id } } Key Vault Vault is a cloud service that allows operators to manage encryption keys that secure resource access and protect both data and secret credentials. Vaults also store the master encryption keys and secrets that are used in configuration files and/or code. A secret can be anything that requires controlled access, such as API keys, passwords, certificates, or cryptographic keys. Technical- and application manager compartments After that, we create the compartments for both the technical- and application manager. The compartment’s associated tree structure is created using the compartmentid argument. While the resource compartments refer to the tenancyocid, the service compartments refer to the parent compartment ID. This enables us to use loops, counts and conditionals. Using lists helps avoid the creation of multiple blocks and allows us to adjust the tree compartment structure without rewriting the code: // Create a vault to store secrets output \"key_id\" { value = ocikmskey.main.id } resource \"ocikmsvault\" \"ops\" { compartmentid = var.compartmentid displayname = \"${var.project}opsvault\" vaulttype = \"DEFAULT\" # or \"VIRTUALPRIVATE\" } resource \"ocikmskey\" \"main\" { #Required compartmentid = var.compartmentid displayname = \"${var.project}${var.stage}_key\" managementendpoint = data.ocikmsvault.ops.managementendpoint key_shape { #Required algorithm = \"AES\" length = 32 } } // Gets the detail of the vault. data \"ocikmsvault\" \"ops\" { #Required vaultid = ocikms_vault.ops.id } data \"ocikmskeys\" \"ops\" { #Required compartmentid = var.compartmentid managementendpoint = data.ocikmsvault.ops.managementendpoint filter { name = \"display_name\" values = ocikmskey.main.display_name } } Management Bucket Within the ops compartment, we define a storage bucket to store files that need to be accessible to all operators. Some examples include log files or Terraform state files. To do this, we add the following resource blocks to the ops.tf template: resource \"ociobjectstoragebucket\" \"ops\" { provider = oci.home #Required compartmentid = var.tenancyocid name = \"${var.project}${stage}tfstate\" namespace = \"${var.project}${stage}ops\" #Optional accesstype = var.bucketaccess_type # defined_tags = {\"terraformed\": \"yes\", \"budget\": 0, \"stage\": var.stage} freeform_tags = {\"source\": \"/code/setup\", \"Parent\"=\"root\"} kmskeyid = ociobjectstoragekms_key.main.id } Note: The complete template is stored in the code directory. In the compartment structure we break the technical management domain up into network and data management services. In addition, a tree structure for application management services enables operators to integrate multiple in- and external application owner, without giving up control over the main digital assets. Data Management Next, we’ll need to define 2 properties to establish our overall data management behavior. The first will be the creation of a data management domain to maintain data gravity across the four infrastructure deployment models for the application developer and service operator. And the second will be a boolean variable to enable/disable the creation of the data compartment: variable \"create_data\" { default = false } Create a data management compartment Initially, data.tf contains only one resource definition. We use the count method to enable/disable the compartment creation: # Create a data management compartment resource \"ociidentitycompartment\" \"data\" { count = var.create_data ? 1 : 0 provider = oci.home #Required compartmentid = var.tenancyocid name = \"${var.project}datadomain\" description = \"Compartment to manage persistent data for ${var.project}\" #Optional enable_delete = false // true will cause this compartment to be deleted when running terraform destroy # defined_tags = {\"terraformed\": \"yes\", \"budget\": 0, \"stage\": var.stage} freeform_tags = {\"source\": \"/code/setup\", \"Parent\"=\"root\"} } Define policies and roles Admin policies define the tasks that a user can perform. We’ll create groups for database administrators and file system managers, as well as assign policies to them. Group memberships enable domain administrators to perform the tasks associated with these policies. # Create a file system administrator role resource \"ociidentitygroup\" \"fsadmin\" { provider = oci.home #Required compartmentid = var.tenancyocid name = \"${var.project}${var.stage}fs_administrator\" description = \"Group for manager of network attached storage\" #Optional # defined_tags = {\"terraformed\": \"yes\", \"budget\": 0, \"stage\": var.stage} freeform_tags = {\"source\": \"/code/setup\", \"Parent\"=\"root\"} } # Define a the administration policies for storage administrators resource \"ociidentitypolicy\" \"fsadmin\" { name = \"${var.project}${var.stage}fsadministratorpolicy\" description = \"Policies for manager of network attached storage\" compartmentid = var.tenancyocid statements = [ \"ALLOW GROUP ${ociidentitygroup.fsadmin.name} to manage object-family IN TENANCY\", \"ALLOW GROUP ${ociidentitygroup.fsadmin.name} to manage volume-family IN TENANCY\", \"ALLOW GROUP ${ociidentitygroup.fsadmin.name} to read all-resources IN TENANCY\", ] } # Create the database administrator role resource \"ociidentitygroup\" \"dbadmin\" { provider = oci.home #Required compartmentid = var.tenancyocid name = \"${var.project}${var.stage}database_admin\" description = \"Group for the network administrator role\" #Optional # defined_tags = {\"terraformed\": \"yes\", \"budget\": 0, \"stage\": var.stage} freeform_tags = {\"source\": \"/code/setup\", \"Parent\"=\"root\"} } # Define a the administration policies for database administrators resource \"ociidentitypolicy\" \"database_admin\" { name = \"${var.project}${var.stage}databaseadminpolicy\" description = \"Policies for the database administrator role\" compartmentid = var.tenancyocid statements = [ \"ALLOW GROUP ${ociidentitygroup.dbadmin.name} manage database-family IN TENANCY\", \"ALLOW GROUP ${ociidentitygroup.dbadmin.name} read all-resources IN TENANCY\", ] } List root compartments A data block that we call trunk creates a list of compartments attached to the root compartment. # List root compartments data \"ociidentitycompartments\" \"trunk\" { #Required compartmentid = var.tenancyocid #Optional access_level = \"ANY\" //ANY or ACCESSIBLE # applies only when you perform listCompartments on the tenancy #compartmentidin_subtree = \"ANY\" } These data blocks are referenced in output.tf. # Output compartment details for resource compartments output \"resources\" { value = data.ociidentitycompartments.resources } Application Management For the application management domain, we add a variable containing list of application domains: variable \"create_app\" { default = false } The app.tf file contains two resource definitions, one for the main app compartment and one for the sub-compartments. The compartment structure is reflected using a complex variable type that enables us to create list of sub-compartments. # Create a main compartment application management (https://wiki.en.it-processmaps.com/index.php/ITILApplicationManagement) resource \"ociidentitycompartment\" \"app\" { count = var.create_app ? 1 : 0 provider = oci.home #Required compartmentid = var.tenancyocid name = \"${var.project}_applications\" description = \"Compartment to manage applications for ${var.project}\" #Optional enable_delete = false // true will cause this compartment to be deleted when running terraform destroy # defined_tags = {\"terraformed\": \"yes\", \"budget\": 0, \"stage\": var.stage} freeform_tags = {\"source\": \"/code/setup\", \"Parent\"=\"root\"} } # Create the system operator role resource \"ociidentitygroup\" \"sysadmin\" { provider = oci.home #Required compartmentid = var.tenancyocid name = \"${var.project}${var.stage}sysadmin\" description = \"Group for the system operator role\" #Optional # defined_tags = {\"terraformed\": \"yes\", \"budget\": 0, \"stage\": var.stage} freeform_tags = {\"source\": \"/code/setup\", \"Parent\"=\"root\"} } resource \"ociidentitypolicy\" \"sysadmin\" { name = \"${var.platformstage}${var.environmentstage}systemadministratorpolicy\" description = \"Policies for the system operator role\" compartmentid = var.tenancyocid statements = [ \"ALLOW GROUP ${ociidentitygroup.sysadmin.name} to manage instance-family IN TENANCY where all {target.compartment.name=/*/, target.compartment.name!=/${var.project}_network/}\", \"ALLOW GROUP ${ociidentitygroup.sysadmin.name} to manage object-family IN TENANCY where all {target.compartment.name=/*/, target.compartment.name!=/${var.project}_network/}\", \"ALLOW GROUP ${ociidentitygroup.sysadmin.name} to manage volume-family IN TENANCY where all {target.compartment.name=/*/ , target.compartment.name!=/${var.project}_network/}\", \"ALLOW GROUP ${ociidentitygroup.sysadmin.name} to use load-balancers IN TENANCY where all {target.compartment.name=/*/ , target.compartment.name!=/${var.project}_network/}\", \"ALLOW GROUP ${ociidentitygroup.sysadmin.name} to use subnets IN TENANCY where target.compartment.name=/${var.project}_network/\", \"ALLOW GROUP ${ociidentitygroup.sysadmin.name} to use vnics IN TENANCY where target.compartment.name=/${var.project}_network/\", \"ALLOW GROUP ${ociidentitygroup.sysadmin.name} to use vnic-attachments IN TENANCY where target.compartment.name=/${var.project}_network/\", \"ALLOW GROUP ${ociidentitygroup.sysadmin.name} to manage compartments in Tenancy where all {target.compartment.name=/*/ , target.compartment.name!=/${var.project}network/, target.compartment.name!=/${var.project}applications/}\", \"ALLOW GROUP ${ociidentitygroup.sysadmin.name} to read all-resources IN TENANCY\", ] } List app compartments We loop over our platform- and the services block, using the count method. Count refers to keys using the respective index number [count.index]. The ${ } construct allows to use variables inside a string. Freeform tags provide non-harmonized context information. After that, we create data blocks in sources.tf which return the identifier for the compartments defined in the deployment plan: # List app compartments data \"ociidentitycompartments\" \"apps\" { #Required compartmentid = ociidentity_compartment.app.id #Optional access_level = \"ANY\" //ANY or ACCESSIBLE # applies only when you perform ListCompartments on the tenancy #compartmentidin_subtree = \"ANY\" } These data blocks are referenced in output.tf: # Output compartment details for app compartments output \"apps\" { value = data.ociidentitycompartments.apps } A complete compartment.tf example file is stored in the code directory. User Management After defining the compartment structure, we create the initial admin users, leveraging the user.tf template with the basic resource block and the user data block. Next, we’ll define a variable that represents the admin profile and insert the variable at the top of our template. Information like the name, email and description is captured in a tuple, another complex variable type. User profiles allow tenant administrators to: manage user information manage privilege, application, and service access grant users self-management for their own accounts and services. variable \"user\" { description = \"user definition\" type = tuple([string, string, string, bool]) default = [ \"user_name\", \"ocilabs@mail.com\", \"ITIL Administrator\", true ] } ociidentityuser this resource block creates the admin users ociidentityui_password this block creates the password ociidentityusercapabilitiesmanagement this block sets the user capabilities resource \"ociidentityuser\" \"user_name\" { provider = oci.home #Required compartmentid = var.tenancyocid description = var.user[2] name = var.user[0] #Optional #defined_tags = {\"Operations.CostCenter\"= \"42\"} email = var.user[1] freeform_tags = {\"Framework\" = \"itil\"} } resource \"ociidentityuipassword\" \"usersecret\" { provider = oci.home userid = ociidentityuser.username.id } resource \"ociidentityusercapabilitiesmanagement\" \"user_name\" { provider = oci.home userid = ociidentityuser.username.id canuseapi_keys = false canuseauth_tokens = false canuseconsole_password = var.user[3] canusecustomersecretkeys = false canusesmtp_credentials = false } Assigning Roles We defined a number of groups to manage the roles and responsibilities. A common definition of roles and responsibilities is provided by the ITIL framework. While the complete model is pretty broad, our specific concern is service operation and initially created the following administrator roles: Group Permissions cloud account - Manage users - Manage the Administrators and Netsecopss groups Note: Oracle creates the Administrators group when you subscribe to Oracle Cloud. The users in this group have full access to all the resources in the tenancy, including managing users and groups. Limit the membership to this group. security - Read all the resources in the tenancy - Manage security lists, internet gateways, customer-premises equipment, IPSec VPN connections, and load balancers - Use all the virtual network resources user - Manage users - Manage all the groups except Administrators and Netsecopss network - Read all the resources in the tenancy - Manage all the networking resources, except security lists, internet gateways, IPSec VPN connections, and customer-premises equipment system - Read all the resources in the tenancy - Manage the compute and storage resources - Manage compartments - Use load balancers, subnets, and VNICs storage - Read all the resources in the tenancy - Manage the object storage and block volume resources database - Read all the resources in the tenancy - Manage the database resources Auditor (ReadOnly) - View and inspect the tenancy Note: This group is for users who aren’t expected to create or manage any resources (for example, auditors and trainees). Role assignment: resource \"ociidentityusergroupmembership\" \"operator\" { provider = oci.home compartmentid = var.tenancyocid foreach = var.usernames userid = ociidentity_user.admins[each.key].id groupid = ociidentity_group.itsm[0].id } ociidentityui_password this data block retrieves all information related to the password resource before we create the output block for the user details. data \"ociidentityuipassword\" \"username\" { #Required userid = ociidentityuser.username.id } output \"user_details\" { value = ociidentityuipassword.usersecret } This template will return the user details including the UI password. Passwords will be generated and shown at the terraform apply stage. When we use the terraform output command, Terraform will not return the passwords. The complete template is stored in the code directory. What’s next Next up, Database Infrastructure in Step 3! By Malte Menkhoff","categories": ["iac","opensource"],
        "tags": ["open-source","terraform","iac","devops","get-started"],
        "url": "/tutorials/oci-iac-framework/getting-started-with-oci-step-2-base",
        "teaser": ""
      },{
        "title": "Database Infrastructure",
        "excerpt":" Overview of the Oracle Database Cloud Service Database System Overview The Oracle Database Cloud Service offers autonomous and co-managed Oracle Database cloud solutions. Autonomous databases are preconfigured, fully-managed environments that are suitable for either transaction processing or data warehouse workloads. Co-managed solutions are virtual machine and Exadata DB systems that you can customize with the resources and settings that meet your needs. You can quickly provision an Autonomous Database or co-managed DB system. You have full access to the features and operations available with the database, but Oracle owns and manages the infrastructure. You can also extend co-managed database services into your data center by using Exadata Cloud@Customer, which applies the combined power of Exadata and Oracle Cloud Infrastructure while enabling you to meet your organization’s data-residency requirements. For details about each offering, start with the following overview topics: Autonomous Databases The Database service offers Oracle’s Autonomous Databases with transaction processing and data warehouse workload types. Co-managed Systems Virtual Machine DB Systems Exadata Cloud Service Exadata Cloud@Customer Database Cloud Service on Virtual Machine Database Cloud Service offers full-featured Oracle Database cloud instances: Enterprise Edition or Standard Edition 2 Multiple Oracle Database versions, including 21c 4 tiers of Oracle Database License Included options or Bring Your Own License Enhanced with Cloud automation features In addition to our Oracle Database Cloud Solution (DBCS), we also offer managed MySQL Cloud Services and other Data Management Cloud Services. Cloud automation under customer control - provisioning, patching, backup, disaster recovery DBCS Features Database Cloud Service Provisioning using Terraform and OCI Resource Manager Database Cloud Service on VM is an exceptional entry into the world of cloud-supported database services. First, you keep full control of your database servers and databases. Plus, the service includes many convenient functions which simplify and accelerate the creation and configuration of database systems. But not only that, with just a few clicks your database can be enhanced with additional features to accommodate growing demands or needed adjustments. These include CPU and storage scaling as well as the creation of a standby database through Data Guard. When creating a Database Cloud Service on VM all the cloud resources you need to get a fully operational database instance going are provisioned: Compute instance instantiates a Real Application Cluster (RAC) Block storage is made available to the database nodes Object Store is used to store automated and manual backups Notes: All DBCS resources are only accessible through the Database System resource, not through the individual categories as Compute Instances or Object Store. All DBCS components are created as part of the Database System resource. Therefore you won’t find them under the Compute Instance or Object Storage Categories in the OCI console. Prepare the OCloud Landing Zone Before we create a DBCS on VM resource, we’ll set up a Compartment, a Virtual Cloud Network (VCN), and a Subnet. You can do this either through the OCI console or through Terraform using OCI’s Rest API. The latter method is used for this session. Reference: Instructions on how to deploy OCloud Landing Zone can be found in this article. DBCS architecture DBCS Architecture Policies for the Database Compartment allow members of the dbops group to create database subnets and to manage database-family resources: - ALLOW GROUP &lt;label&gt;dbops manage database-family in compartment &lt;label&gt;database_compartment - ALLOW GROUP &lt;label&gt;dbops read all-resources in compartment &lt;label&gt;database_compartment - ALLOW GROUP &lt;label&gt;dbops manage subnets in compartment &lt;label&gt;database_compartment OCloud Remote Stack Now, to create the database system resource the OCI Rest API requires a target compartment and a target subnet. There are three different ways to collect information in OCI: By default, the Terraform stack defines the required parameters itself, but they can also be entered by the user when the stack is created in OCI Resource Manager (ORM). Both the Database Compartment OCID and VNC OCID are queried from OCI as data elements: # In this example a list of all compartments within a Tenant is returned which is filtered by the database compartment name data \"ociidentitycompartments\" \"db_compartment\" { compartmentid = &lt;tenancyocid&gt; compartmentidin_subtree = true state = \"ACTIVE\" filter { name = \"name\" values = [ &lt;Datenbank Compartment Name&gt; ] } } Terraform supports direct access to the output data of previously provisioned stacks through ociresourcemanagerstacktfstat and terraformremotestate resources. For this object-relational mapping (ORM), Terraform extracts the desired information from the Terraform tfstate file which is stored as part of a successfully deployed OCI Stack: data \"ociresourcemanagerstacktfstate\" \"stack1tfstate\" { stack_id = &lt;stack id&gt; local_path = \"stack1.tfstate\" } # Load the pulled state file into a remote state data source data \"terraformremotestate\" \"externalstackremote_state\" { backend = \"local\" config = { path = \"${data.ociresourcemanagerstacktfstate.stack1tfstate.local_path}\" } } Note: In practice, several of these methods will likely be used in combination. Creating the Database Subnet After validating the prerequisites for a DBCS on VM deployment, we’ll now provision the resources for the DBCS on a VM stack. For this use case, all members of the dbops group should be able to create subnets and database resources within the Tenant’s Database Compartment’s limits. For creating subnets, we use a dedicated Terraform module that is provided by the terraform-oci-ocloud-landing-zone repository. Reference: Network domain The network_domain module, which is also used as part of the landing zone provisioning, creates a private subnet as well as all required Security List Policies to communicate with the database system. When sizing the subnet, you should pay attention to the minimum required IP addresses for a certain deployment type (single node vs RAC). The OCloud landing zone defines the bigger subnets for each service, but if the database architecture requires one large subnet, the Terraform function cidrsubnets(prefix,newbits,netum) allows you to split the address space into smaller chunks: servicesegmentsubnets key subnet app 10.0.0.0/26 db 10.0.0.64/26 pres 10.0.1.0/26 k8s 10.0.0.128/25 Below, you can see an example where CIDIR block 10.0.0.64/26 is split into four subnets by adding two additional bits (newbits): &gt; cidrsubnets(\"10.0.0.64/26\",2,2,2,2) tolist([ \"10.0.0.64/28\", ---&gt; cidrsubnet(\"10.0.0.64/26\",2,0) \"10.0.0.80/28\", ---&gt; cidrsubnet(\"10.0.0.64/26\",2,1) \"10.0.0.96/28\", ---&gt; cidrsubnet(\"10.0.0.64/26\",2,2) \"10.0.0.112/28\", ---&gt; cidrsubnet(\"10.0.0.64/26\",2,3) ]) The Terraform module network_domain creates a private subnet for a given database compartment and VNC. It also sets all ingress rules to allow ssh access to the Database nodes and to communicate with the database itself. Last but not least, it sets all egress rules to access both Object Storage and YUM Repository on the Service Network. It’s important to note here that the db_domain module doesn’t define its own Bastion Service since it’s available through the application subnet. However, after provisioning the database, a couple of Bastion Sessions are created (ssh, sqlnet) aiming to validate database system connectivity. Once the Time-to-Live has been exceeded for the Bastion Sessions, they will be terminated automatically. Resource Schema module \"db_domain\" { … source = \"github.com/oracle-devrel/terraform-oci-ocloud-landing-zone/component/network_domain\" config = { service_id = &lt;Container Compartment ID&gt; compartment_id = &lt;Database Compartment ID&gt; vcn_id = &lt;VNC ID&gt; anywhere = “0.0.0.0/0” defined_tags = null freeform_tags = {\"framework\" = \"ocloud\"} } # Subnet Requirements # DB System Type, # Required IP Addresses, Minimum Subnet Size # 1-node virtual machine, 1 + 3 reserved in subnet = 4, /30 (4 IP addresses) # 2-node RAC virtual machine, (2 addresses * 2 nodes) + 3 for SCANs + 3 reserved in subnet = 10, /28 (16 IP addresses) subnet = { # Select the predefined name per index domain = &lt;predefined subnet postfix (See OCloud Landing Zone, module service_segment)&gt; # Select the predefined range per index cidrblock = &lt;predefined subnet cidr block (See OCloud Landing Zone, module servicesegment)&gt;&gt; # Create subnet as private prohibitpubliciponvnic = true # Creates a private subnet dhcpoptionsid = null routetableid = &lt;Routing Table ID for Oracle Service Network connectivity which is created by the OCloud Landing Zone&gt; } bastion = { create = false # Determine whether a bastion service will be deployed and attached clientallowcidr = [] maxsessionttl = null } # Security List Policies tcp_ports = { // [protocol, source_cidr, destination port min, max] ingress = [ [\"ssh\", “0.0.0.0/0”, 22, 22], # DBnode access [\"http\", “0.0.0.0/0”, 80, 80], # APEX access [\"https\", “0.0.0.0/0”, 443, 443], # APEX access [\"tcp\", “0.0.0.0/0”, 1521, 1522], # DB Access where 1521 is used for DBCS and 1522 for Autonomous DB [\"tcp\", “0.0.0.0/0”, 5500, 5500], # Enterprise Manager Express access [\"tcp\", “0.0.0.0/0”, 6200, 6200] # Enables the Oracle Notification Services (ONS) to communicate about Fast Application Notification (FAN) events ] } Database System Provisioning Now that all of our prerequisite resources are created, we’re ready to set up the final components. The OCI resource ocidatabasedbsystem provisions a database system, database nodes, and an initial CDB and PDB all in one step. Convenient, no? Note that the ocidatabasedbsystem comes with many additional parameters to support other flavors of provisioning a database (e.g, creating a database instance from a backup or as clone from an existing database system). For this scenario, we’ll focus on a fresh database install. Refer to the ocidatabasedb_system resource documentation for further details. resource \"ocidatabasedbsystem\" \"dbaasdb_system\" { availability_domain = &lt;Availability Domain&gt; compartment_id = &lt;Database Compartment&gt; database_edition = &lt;Database Edition, i.e. ENTERPRISE EDITION&gt; db_home { database { admin_password = &lt;SYS PASSWORD&gt; db_name = &lt;CDB Name&gt; character_set = &lt;Character Set&gt; ncharacter_set = &lt;International Character Set&gt; db_workload = &lt;Workload Type, OLTP or DW&gt; pdb_name = &lt;PDB Name&gt; tdewalletpassword = &lt;TDE Wallet Password if it is different to the admin_password&gt; dbbackupconfig { autobackupenabled = &lt;Is automated backup to Object Storage enabled?&gt; autobackupwindow = &lt;Two hour time slot within 24 hour when the backup can take place&gt; recoverywindowin_days = &lt;Retention Period&gt; } } db_version = &lt;Database Version, whereas 19.0.0.0.0 corresponds to the latest available version, i.e. 19.12.0.0.0 display_name = &lt;&gt; shape = &lt;Database Node Shape which defines the number of OCPUs and Memory&gt; subnet_id = &lt;Target Database Subnet&gt; sshpublickeys = [&lt;Public Key for ssh access&gt;] display_name = &lt;OCI Display Name&gt; hostname = &lt;DB Node Hostname Prefix&gt; datastoragesizeingb = &lt;Initial Database Storage&gt; license_model = &lt;Database License is either included or transferred from On Premise &gt; node_count = &lt;Database Node Count, 1 or 2 for a Real Application Cluster&gt; cluster_name = &lt;RAC Cluster name&gt; nsg_ids = &lt;Optional Network Security Group&gt; dbsystemoptions { storage_management = &lt;LVM or ASM&gt; } } OCI Resource Manager To use DBCS on a VM stack with Resource Manager, you’ll need to use one of the following methods: download the code as a zip-file After extracting the archive, select Deploy to Oracle Cloud within the Github repository’s Readme for further information. create your own cloned repository on Gitlab or Github Note that the DBCS stack references networkdomain, so if you are using a customized version of the landing zone stack you might have to update the source parameter of the db_domain module. Deploy to Oracle Cloud For this session, we’ll use the Deploy to Oracle Cloud method from the stack’s Readme. This option automatically redirects you to the OCI Console login, and after authentication opens Resource Manager. Package URL points to the stack artifact which is stored within the Github repository. Finally, confirm the Terms of Use. Setting up Resource Manager Configure manually Name the DBCS stack and select a compartment - To get started, let’s enter a meaningful name for your DBCS stack and choose a Compartment where your stack resource will be created. Note: The stack compartment may differ from the Compartment where the actual infrastructure resources reside. Database configuration - On the next page, you’ll finalize the database system configuration. Enter the landing zone’s Stack OCID, which can be found in Resource Manager next to the landing zone stack. This avoids the need to re-enter a lot of the parameters and enables access to Output artifacts from the landing zone. Database System display name - Enter a Database System display name. Then, either accept the shown default values or adjust them to you needs. For a default deployment, the following values are used: Configuration Database Version Oracle Database Software Edition Shape OCPUs Storage(GB) Small 19c Oracle Enterprise Edition VM.Standard2.2 2 512 Deployment Type Storage Management Software Auto Backup enabled? Node Count Fast Provisioning LVM false 1 Authentication - Finally, enter the admin or sys password and the public part of your ssh key. Make sure that you update the admin password after installation as the initial password will show up in the Terraform tfstate file. Configure using a terraform.tfvars file Another way to preconfigure parameters is to manually download the Terraform code from the repository and add a terraform.tfvars file. The respective values are shown as read-only parameters in the stack configuration. Example: dbsystemdisplay_name = \"OCI Database System Display Name\" dbsystemsshpublickeys = \"ssh-rsa …\" dbsystemdbhomedatabaseadminpassword = \"Password\" stack_id = \"ocid1.ormstack.oc1.eu-frankfurt-1.aaaaaaaaqyekvuodrozodmn23zxi…\" Alternative methods for deploying a database system OCloud DBCS on VM Stack supports other ways to deploy a database system, but they’re not the subject of this session: Deploy a Database System into an existing subnet Overwrite Organization, Project and Environment labels which were defined by the corresponding OCloud Landing Zone Customize all stack parameters For further information, refer to the OCloud DBCS on VM stack’s Readme. Plan but don’t apply Create the stack but don’t check Run Apply because you should always plan your deployment first. This step is very important since it checks the stack code for syntax errors and determines exactly which OCI resources are going to be added, updated, or destroyed. Since OCI resources contain both updatable and non-updatable parameters, it’s recommended that the planning task be made a required part of the set up routine since updating a non-updatable value may result in destruction of the resource. Sample plan output: Plan: 9 to add, 0 to change, 0 to destroy. After verifying the expected results, the stack can be applied. Note: For a “Small” configuration set up with “Fast Provisioning” it can take between 20 and 25 minutes before the database is available. Verify DB Node and Database access Once the Database System, Container Database, and Pluggable Database are available you can verify connectivity. Since our target resource is in a private subnet, the Apps Compartments Bastion Service needs to be used as the Bastion Sessions were created as part of the DBCS stack. Protocol Session type IP Address Port Maximum session time-to-live (min)   ssh SSH Port Forwarding Session Host IP 22 1800 ssh access to database node(s) sqlnet SSH Port Forwarding Session Host IP 1521 1800 sqlplus, sqlcl or sqldeveloper SSH: Copy the SSH command from OCI console (Bastion Session) to create an SSH tunnel from localhost:localport to &lt;database node ip address&gt;:22. ssh -i &lt;private key file path&gt; -N -L &lt;local port&gt;:&lt;database node ip address&gt;:22 -p 22 &lt;bastion session OCID&gt;@host.bastion.eu-frankfurt-1.oci.oraclecloud.com To ssh to the DB Node, use the following command: ssh -i &lt;private key file&gt; opc@localhost -p &lt;local port&gt; Oracle SQL Developer: For an Oracle SQL Developer connection to either the CDB or PDB, copy the SSH command from OCI console (Bastion Session) to create an ssh tunnel from localhost:localport to &lt;database node ip address&gt;:1521: ssh -i &lt;private key file&gt; -N -L &lt;local port&gt;:&lt;database node ip address&gt;:1521 -p 22 &lt;bastion session OCID&gt;@host.bastion.eu-frankfurt-1.oci.oraclecloud.com Open sqldeveloper and create a new database connection: Parameter Value Username sys Password admin password Role SYSDBA Connection Type Basic Hostname localhost Port [local sql port] Service Name CDB or PDB SERVICE_NAME from DBCS output SQLcl and SQLplus: For a sqlcl or sqlplus connection, copy the SSH command from OCI console (Bastion Session) to create a SSH tunnel from localhost:localport to &lt;database node ip address&gt;:1521. ssh -i &lt;private key file&gt; -N -L &lt;local port&gt;:&lt;database node ip address&gt;:1521 -p 22 &lt;bastion session OCID&gt;@host.bastion.eu-frankfurt-1.oci.oraclecloud.com Execute: sql sys@localhost:1523/&lt;FQ Database Name&gt; AS sysdba or sqlplus sys@&lt;FQ Database Name&gt; AS sysdba And now you’ve logged into the database, congratulations! What’s next We successfully provisioned a Database Cloud Service on VM Database instance. From here, you can apply further adjustments or add additional stacks to your OCI Infrastructure in your Tenancy. Happy building! Up next, App Infrastructure. By Malte Menkhoff","categories": ["iac","opensource"],
        "tags": ["open-source","terraform","iac","devops","get-started"],
        "url": "/tutorials/oci-iac-framework/getting-started-with-oci-step-3-database-infrastructure",
        "teaser": ""
      },{
        "title": "Application Infrastructure",
        "excerpt":" Oracle Cloud Infrastructure (OCI) allows different deployment models within a shared network using the same Infrastructure-as-Code (IaC) methods. When setting up our operation, we distinguish between core and orchestration service API. Core services represent the physical infrastructure in our data center while orchestration services refer to software that runs outside the core service portfolio, interacts with the application code, and manipulates the behavior of virtual instances. Both application developers and service operators alike need to delineate the following four infrastructure deployment models when designing a multi-server architecture: Dedicated Server - Virtual Machines (VM) or bare metal server that maintain stateful communication interfaces on layer three. Elastic Compute Cluster - One or more VM that scales automatically and maintain a stateless communication interface on layer three. Container Cluster - One or more dedicated server that host lightweight stand-alone, executable user space images including code, runtime, system tools, system libraries, settings to run on a linux kernel. Miscellaneous - Functions, ephemeral, single purpose, self contained, or stateless container without an API surface, invoked via network protocols like HTTP. While public cloud providers also offer these instance types as products, OCI provides the ability to defines logical resources such as respective orchestrators. We can rely on managed services for open source orchestrators or choose commercial third-party offerings. Either way, we invoke an orchestrator and write modules for the resource manager. In this article, we’ll focus on these three different models: host, node, and container. Host model The host model is the one most known from on-premise environments. On a dedicated physical machine, Virtual Machines (VMs) can be deployed that run either stateless or stateful applications. OCI offers both ways here: You can deploy a bare metal host, install the hypervisor, and deploy the VMs on top of it. Here, you are responsible for the VMs and the hypervisor layer as well as the Operating System (O/S) of the bare metal host. You will have full root access to the O/S of the bare metal server and it will be inside a Virtual Cloud Network (VCN) that you own. You can deploy a Dedicated VM Host and deploy the VMs on top of it. This is the approach that we use here: You can use Terraform to fully deploy both the Dedicated VM Host as well as the VMs on top of it. Each VM will be instantiated with its own Virtual Network Interface Card (VNIC) which can be individually placed into VCNs and subnets that you own. Since the Dedicated VM Host itself is fully managed by Oracle, you won’t have any O/S access to it and the Dedicated VM Host won’t be placed in any VCN. You can use tools like Packer to first build a custom image with all applications and data you need on your VMs before applying Terraform to instantiate the VMs. The cloud-init option of Terraform gives the opportunity to apply a shell script on the instantiated VMs to add either individual pieces of data or installations immediately after the instantiation. The shell script is added as a base64-encoded attribute to the resource definition of the instance. Through metadata key-value pairs, Terraform can pass parameters to the instance that can be used inside the cloud-init shell script to parameterize the actual shell execution. The Terraform stack consists of a dedicatedHost.tf file which can be used to create a Dedicated VM Host. By default, this code is commented out because the Service Limits of many demo-tenants do not allow the creation of Dedicated VM Hosts. However, this capability can easily be activated by removing the comments start/end lines. Node model The Node model applies the cloud principle to adapt the number of available nodes to the current amount of workload. Here, we have primary workloads running that control secondary workloads on top which will be scaled in and out based on on-demand capacity rather than capacity from a Dedicated VM Host in order to optimize the costs. The secondary workloads should be stateless in practice since scaling in means that those nodes might be terminated by the Cloud Control at any time if the overall workload could be sufficiently executed by less nodes. OCI has the following artifacts to create this scenario, which can be fully deployed by Terraform: Instance Configuration - An Instance Configuration that acts as the blueprint for the pool of secondary workloads VMs. Here, you define: A Custom Image that should be used (can be built using Packer and you can use cloud-init provider for further work). The Shape of the pool instances (e.g., VM.Standard2.1, which means a 1 OCPU Intel X7 VM with network-attached storage). The public part of the ssh key pair to access the O/S of the instance. Instance Pool - The Instance Pool object refers to an Instance Configuration. This Instance Configuration adds information about in which Availability Domain it will be in, as well as in which subnet the instance pool’s instances’ VNICs should be placed. Furthermore, you define how many VMs should be started. You can add a load balancer to the instance pool definition in a way that any created instances inside the pool will be part of this load balancer’s backend set, so that incoming requests are forwarded to the instance pool instances (e.g., in a round-robin-manner). Load balancers also support cookie-based session stickiness in case this is needed by stateful applications running in the instance pool instances. Autoscaling Configuration - The Autoscaling Configuration refers to an Instance Pool and adds policies for when new instances should be automatically added or removed. You define the incremental and decremental step size (number of instances to be added or removed when a scale-in or scale-out event occurs) as well as the minimum and maximum number of instances. Two autoscaling policies are supported: Schedule-based Autoscaling: Here, the scaling-out and scaling-in rules are defined based on fixed schedules similar to definitions in cron jobs. This is feasible if regular workload peaks are to be expected like loading data into a Data Warehouse or providing Analytic reporting at certain times during a day, week, or month. Metrics-based Autoscaling: Here, the scaling-out and scaling-in rules are based on overall instance pool metrics that the instances report using agents to the Cloud Control. OCI allows the following metrics to be used in this option: CPU Utilization (in percent) Memory Utilization (in percent) Note: In the Autoscaling Configuration, you define the percent threshold value above which the pool will scale out (add an instance or instances if the maximum number is not yet reached) and the percent threshold value below which the pool will scale in (terminate an instance or instances if the minimum number is not yet reached). In this scenario, we trigger a 100% CPU utilization process with each new instance pool upon creation (using cloud-init) whose duration in minutes can be set by a user-defined variable as part of the Terraform stack definition. This way we can optionally demonstrate the scaling-in and scaling-out according to an CPU-utilization based auto-scaling policy. Further, we deploy a https server along with a static page for each instance pool instance (including a timestamp of the instance’s creation). This stateless “application” is exposed to the public internet by a load balancer, so you can directly see the round-robin-fashioned forwarding of requests to the instance pool instances by reloading the page in the browser. The corresponding public load balancer endpoint is displayed as part of the Terraform out parameters. In this stack, the load balancer exposes the “application” with https, using a self-signed certificate that is also created inside the Terraform stack. Container Model The Container model is the preferred cloud model for stateless applications like Functions. OCI offers a fully managed Kubernetes Cluster, the OCI Container Engine (OKE). Again, this can be deployed using Terraform. OKE consists of: The Kubernetes Cluster which provides the Kubernetes API endpoint as well as the Scheduler and Controller Manager. These components are fully managed by Oracle and visible to the customer only using the Kubernetes API (e.g., either by using kubectl or by deploying Helm charts). The customer doesn’t have O/S access to this instance and is also free of charge. The Kubernetes Node Pool that contains the worker nodes. The customer has full root access using ssh but has to pay for these VMs. The charges are the same for Linux VMs of the respective shapes, meaning that there is no surcharge for their role as a Kubernetes worker node. Additional elements are added and terminated according to Kubernetes deployments., e.g. when deploying a Load Balancer service to a Kubernetes cluster like: kubectl expose deployment myapplication --type=LoadBalancer --name=myapplicationservice An OCI Load Balancer is automatically deployed and configured with the worker nodes in its backend set. .kube/config When the cluster is ready, the .kube/config file (which contains the network details like the Kubernetes Cluster’s API endpoint’s IP address and the authorization certificate) can be downloaded to a client using the following OCI Command Line Interface (OCI CLI) command: oci ce cluster create-kubeconfig --cluster-id ocid1.cluster.oc1.eu-frankfurt-1.aaaaathekubernetesclusterocidlqs27a --file $HOME/.kube/config --region eu-frankfurt-1 --token-version 2.0.0 export KUBECONFIG=$HOME/.kube/config The Terraform stack also creates an Kubernetes Cluster along with a worker node pool. The contents of the .kube/config file can be directly accessed by a corresponding parameter of the Terraform Output. Then, the client can, for example, apply kubectl to inspect, create, and destroy Kubernetes artifacts: kubectl get nodes,pods Output may look similar to: NAME STATUS ROLES AGE VERSION node/10.0.10.166 Ready node 7h11m v1.19.7 NAME READY STATUS RESTARTS AGE pod/myapplication-588cf6ff66-cq684 1/1 Running 0 6h13m pod/myapplication-588cf6ff66-hwtgd 1/1 Running 0 6h13m pod/myapplication-588cf6ff66-q4228 1/1 Running 0 6h15m Registry service OCI also offers a registry service (the OCI registry, OCIR) where container images can be stored and retrieved to be deployed to the Kubernetes cluster. OCIR allows registries to both be public (free access to anyone) or private (downloading images requires presenting a SWIFT-compliant API Key, a so-called OCI Auth Token that is created individually for each OCI User). Besides deploying Kubernetes artifacts like pods, deployments, services, or replicasets, Terraform provides a Kubernetes provider to deploy these artifacts as part of the terraform apply process using a kubectl client. The okeServiceDeployment.tf shows the steps to take here: Get the OKE Cluster’s config file and extract the CA certificate as well as the OCI CLI command (along with the necessary arguments) to create an ExecCredential. This OCI CLI command is executed so Terraform can authenticate to the Kubernetes API endpoint for further operations. Create a new namespace in Kubernetes. Define additional resources like kubernetes_service to deploy artifacts. Kubernetes artifacts are defined by yaml documents and those Terraform resources basically reformat these yaml documents to match the HashiCorp Configuration Language (HCL) standard. Example stack In this example stack, we deploy a standard NGINX server to the new generated Kubernetes Cluster. We take the standard NGINX image from the official Docker registry, but you can also deploy your own pods from stored docker images (e.g., in the OCI registry (OCIR)). Next, we deploy this NGINX server as a kubernetes_service with “Load Balancer” as the type using Terraform. The advantage of using Terraform instead of a local kubectl client for deploying Kubernetes services is that these services are removed when destroying the Terraform stack. This is important because deploying a Kubernetes service as a “Load Balancer” means that an OCI Load Balancer with the Kubernetes deployment of pods in its backend is created outside of the Kubernetes Cluster. So you need to delete the Kubernetes service first when destroying the Terraform stack in order to properly remove this load balancer. The complete network topology along with the compute instances, load balancers, and Kubernetes resources that will be created by running this stack can be seen in the image below: After the Terraform stack has been successfully applied, you should see the following Kubernetes artifacts (e.g., by using the cloud shell): kubectl get pods,deployments,replicasets,services --namespace nginx Output should look something like: NAME READY STATUS RESTARTS AGE pod/nginx-5c48f8956d-84456 1/1 Running 0 41m pod/nginx-5c48f8956d-wwq8s 1/1 Running 0 41m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx 2/2 2 2 41m NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-5c48f8956d 2 2 2 41m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/nginx LoadBalancer 10.96.113.223 152.70.173.212 80:32177/TCP 33m &lt; db-infra + workload &gt; What’s next In this article, we covered the various deployment options OCI supports and focused in on the Container model for stateless applications like Functions. We also discussed cluster configuration and the OCI Registry service. At this point, you should be ready for our next topic! In the next section of the series, we’ll discuss workload deployment. By Malte Menkhoff","categories": ["iac","opensource"],
        "tags": ["open-source","terraform","iac","devops","get-started"],
        "url": "/tutorials/oci-iac-framework/getting-started-with-oci-step-4-app-infrastructure",
        "teaser": ""
      },{
        "title": "Workload Deployment",
        "excerpt":" OCI best practices Oracles’s best practice recommendation for workloads on Oracle Cloud Infrastructure (OCI) is to use an operator host to install and maintain the service application and configuration for the workload. As you might imagine, since the number of applications and needed configurations varies, our goal at this step will be to dig deeper into the concept of the operator host and explore which configurations and automation tools can be best used to deploy your workload on OCI. Operator Host The aim of the operator host is to: Perform post-provisioning tasks with any automation tools that require a local installation. Provide administrators access without the need to upload API authentication keys (instance_principal). Access to Operator Host As described in the base section of the series, OCI provides the ability to use a Bastion service for a controlled and secure access to resources in your tenancy. The operator host concept leverages this service, providing administrators and application hosts a quick and secure mechanism to access a system which can run scripts or any automation tool for maintaining and operating the services. OCI Tool Support for Automation and Configuration Management Terraform is primarily used to define the infrastructure required to host the applications for the service. However, by itself, Terraform is not designed to perform certain application installation or configuration tasks. Oracle recognizes that everyone’s needs are different. Throughout this series we’ve attempted to present ways to use the most common tools on the market in combination with Terraform to provide and guide you with a set of best practices. As always, it’s up to you to decide which tool combination best suits your individual workload and which best fulfills your requirements. Terraform is focussed on Infrastructure as Code (IaC). While its strength lies in provisioning resources there’s a potentially need for tools to install, configure, and manage applications on top of the infrastructure. There are a number of external tools which can be used to automate the workload deployment. In this session, we’ll focus on how to use the most common tools like Ansible, Puppet, and Chef. We’ll also take a cloud-native approach, so we’ll show you in a demo how you can leverage Helm charts to deploy workloads via the Oracle Resource Manager onto an OKE cluster deployed previously. Ansible OCI supports the use of Ansible’s modules to automate cloud infrastructure provisioning along with configuration, complex operation process orchestration, and deployment/maintenance of your software assets. Resources: The OCI Ansible collection supports both Ansible Tower and AWX. Ansible Tower - For more information on how to set up the collection with Ansible Tower, refer to the Ansible blog post. AWX - To install the free version of Ansible Tower (AWX) on an OCI Compute instance, you can use ansible solution on GitHub and the following ansible example playbooks. A complete example of how you can use Ansible to deploy Kubernetes and Istio can be found in this article. For additional information on Ansible, check out the ansible collection. Puppet While many organizations are using Terraform to provision Oracle cloud resources, a solution for continuous integration should be considered when it comes to ongoing management of resources. That’s where Puppet can help. With Puppet you can: Integrate cloud resources into your existing infrastructure and manage everything with one tool. Use existing puppethiera data to configure parts of your OCI infrastructure. Have a tighter integration between OCI configuration in general and the configuration management on your systems. Extend Puppet with ociconfig The oci_config module extends the Puppet language to contain types needed to create and manage the lifecycle of objects within your Oracle Cloud Infrastructure. Although this is traditionally the domain of Terraform scripts, being able to manage these objects with Puppet has proven to be a big plus for many customers. For example: Your organization is already using Puppet and not Terraform. Introducing a new tool into your organization might be more then you want or need. In these cases, Puppet in combination with this module can be a great help. You want to use existing Hiera data to configure parts of your OCI infrastructure. In this case, using this module is great. It integrates with all of the existing hieradata, just like your other Puppet code. You need tighter integration between OCI configuration in general and the configuration management on your systems. Again, this module is for you. Since it makes use of standard Puppet you can use all of the rich Puppet features like exported resources to integrate all of your configuration settings both on the cloud level as well as on the machines. Puppet example code: Configuration for using a tenant: puppet apply /software/tenant_setup.pp Your output should be something like: Notice: Compiled catalog for oci in environment production in 0.09 seconds Notice: /Stage[main]/Main/Oci_tenant[enterprisemodules]/fingerprint: defined 'fingerprint' as 'xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx' Notice: /Stage[main]/Main/Ocitenant[enterprisemodules]/privatekey: created with specified value Notice: /Stage[main]/Main/Oci_tenant[enterprisemodules]/region: defined 'region' as 'eu-frankfurt-1' Notice: /Stage[main]/Main/Ocitenant[enterprisemodules]/tenancyocid: defined 'tenancy_ocid' as 'ocid1.tenancy.oc1..xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' Notice: /Stage[main]/Main/Ocitenant[enterprisemodules]/userocid: defined 'user_ocid' as 'ocid1.user.oc1..xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' Notice: Applied catalog in 0.02 seconds First inspection: puppet resource ociidentitycompartment Your output should look something like: bash-4.2# puppet resource ociidentitycompartment * ENTERPRISE MODULES Universal License INTERNAL USE ONLY * ociidentitycompartment { 'your_tenant (root)/ManagedCompartmentForPaaS': ensure =&gt; 'present', compartment =&gt; '/', compartment_id =&gt; 'ocid1.tenancy.oc1..xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx', description =&gt; 'idcs-f7246e2bbacf4a11a7e231507e34fdec|22626923|user@domain.com-Enterprise Modules B.V.-838062', id =&gt; 'ocid1.compartment.oc1..aaaaaaaai2wkrvdvyxfuekjbt3jnv7b4hrlkvwnklu6uryy2daqsq425tzaa', lifecycle_state =&gt; 'ACTIVE', provider =&gt; 'sdk', time_created =&gt; '2019-10-24T08:42:26+00:00', } ociidentitycompartment { 'yourtenant (root)/testcompartment_1': ensure =&gt; 'present', compartment =&gt; '/', compartment_id =&gt; 'ocid1.tenancy.oc1..xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx', description =&gt; 'changed', id =&gt; 'ocid1.compartment.oc1..aaaaaaaatfskqfckrl4sucabclbsss47uyttlmwwur6lsm7crl3lrz7glfta', lifecycle_state =&gt; 'ACTIVE', provider =&gt; 'sdk', time_created =&gt; '2020-01-23T15:42:35+00:00', } Resources: ociidentitycompartment Download the ociconfig module from Puppet forge. Read the Puppet Enterprise Guide to install the oci_config Puppet module. Follow the guide to deploy an Oracle 19 database via Puppet. Chef Chef is a powerful automation platform that transforms infrastructure into code. Whether you’re operating in the cloud, on-premises, or in a hybrid environment, Chef automates how infrastructure is configured, deployed, and managed across your network, no matter its size. This diagram shows how you develop, test, and deploy your Chef code: Chef Plugin for OCI (Image Courtesy) knife-oci plugin The knife-oci plugin allows users to interact with OCI through Chef Knife. The following are the available knife-oci plugin commands: Action Command Launch an OCI instance and bootstrap it as a Chef node knife oci server create List OCI compartments knife oci compartment list Delete an OCI instance knife oci server delete List OCI instances in a given compartment. Note: All instances in the compartment are returned, not only those that are Chef nodes knife oci server list List the images in a compartment knife oci image list List the VCNs in a compartment knife oci vcn list List the subnets in a VCN knife oci subnet list List the shapes that may be used for a particular image type knife oci shape list List the availability domains for your tenancy knife oci ad list Resources: The knife-oci plugin can be downloaded from the public repo. How to setup the knife-oci plugin can be found in this article. OCI documentation for the chef. Helm Helm helps you manage Kubernetes applications while Helm Charts help you define, install, and upgrade even the most complex Kubernetes application. Charts are easy to create, version, share, and publish so you can avoid tedious copying-and-pasting. Advantages of using Helm for the deployment of applications on top of Kubernetes: Managed Complexity Easy Updates Simple Sharing Rollbacks Oracle Resource Manager The Oracle Resource Manager (ORM) supports the Terraform provider for Helm and can be easily used in combination with the Terraform Kubernetes providers. Reference: Details about third-party provider versions of ORM can be found on this providers page. In order to get the needed information about the Kubernetes cluster, we’ll need to get the content of the Kubernetes cluster which has been deployed. This can be achieved for the Kubernetes and Helm provider through the following: # Gets kubeconfig data \"ocicontainerengineclusterkubeconfig\" \"okeclusterkube_config\" { clusterid = ocicontainerenginecluster.okecluster.id } # https://docs.cloud.oracle.com/en-us/iaas/Content/ContEng/Tasks/contengdownloadkubeconfigfile.htm#notes provider \"kubernetes\" { loadconfigfile = \"false\" # Workaround for tf k8s provider &lt; 1.11.1 to work with ORM clustercacertificate = base64decode(yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"clusters\"\"cluster\") host = yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"clusters\"\"cluster\" exec { apiversion = \"client.authentication.k8s.io/v1beta1\" # Workaround for tf k8s provider &lt; 1.11.1 to work with orm - yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkubeconfig.content)\"users\"\"user\"[\"apiVersion\"] args = yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)[\"users\"\"user\"\"args\", yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"users\"\"user\"\"args\", yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"users\"\"user\"\"args\", yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"users\"\"user\"\"args\", yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"users\"\"user\"\"args\", yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"users\"\"user\"\"args\", yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"users\"\"user\"\"args\"] command = yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"users\"\"user\"[\"command\"] } } # https://docs.cloud.oracle.com/en-us/iaas/Content/ContEng/Tasks/contengdownloadkubeconfigfile.htm#notes provider \"helm\" { kubernetes { loadconfigfile = \"false\" # Workaround for tf helm provider &lt; 1.1.1 to work with ORM clustercacertificate = base64decode(yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"clusters\"\"cluster\") host = yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"clusters\"\"cluster\" exec { apiversion = yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkubeconfig.content)\"users\"\"user\"[\"apiVersion\"] args = yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)[\"users\"\"user\"\"args\", yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"users\"\"user\"\"args\", yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"users\"\"user\"\"args\", yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"users\"\"user\"\"args\", yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"users\"\"user\"\"args\", yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"users\"\"user\"\"args\", yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"users\"\"user\"\"args\"] command = yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"users\"\"user\"[\"command\"] } } } Best practice: It’s recommended that you separate Kubernetes by service into logical entities called namespaces (see below). namespaces The following code snippet in Terraform creates a namespace: resource \"kubernetesnamespace\" \"&lt;service&gt;namespace\" { metadata { name = \"&lt;service&gt;\" } dependson = [ocicontainerenginenodepool.okenodepool] } Best practice: As we need to have a working Kubernetes up and ready with available worker nodes, we recommend that you wait until you can verify that worker nodes have been deployed in a previous step of Terraform. Deploy Kubernetes application The next step would be to deploy your Kubernetes application with a Helm release into the previously created namespace. This can be achieved by storing your Helm files in a sub-directory of your Terraform environment (e.g., helm_charts) and referencing your charts in your Terraform code as follows: resource \"helmrelease\" \"&lt;application1&gt;\" { dependson = [ocicontainerenginenodepool.okenodepool] name = \"&lt;application_1&gt;\" chart = \"helmcharts/&lt;application1&gt;\" namespace = kubernetesnamespace.&lt;service&gt;namespace.id wait = false timeout = 300 } Deploy multiple applications It’s also possible to deploy multiple applications for your service in repeating the code fragment as follows: resource \"helmrelease\" \"&lt;application2&gt;\" { dependson = [ocicontainerenginenodepool.okenodepool] name = \"&lt;application_2&gt;\" chart = \"helmcharts/&lt;application2&gt;\" namespace = kubernetesnamespace.&lt;service&gt;namespace.id wait = false timeout = 300 } Best practices: timeout - Set a timeout to give Helm a time frame within which the deployment has to be completed. dependson - As the Helm provider is not able to identify whether or not the Kubernetes deployment has been finished, we add a dependson section to make sure that the Kubernetes cluster is up and running before deploying. Reference: Information about namespaces can be found in this Kubernetes article on namespaces. Demonstration of a Helm deployment To demonstrate how to use the ORM to deploy workload on Kubernetes cluster, we’ll show an example by deploying a Helm release of an hivemq cluster and a kafka connector in one namespace. The target setup will look as follows: We’re using the following generic code which can be used independent of the IaaS or Kubernetes stack: # Gets kubeconfig data \"ocicontainerengineclusterkubeconfig\" \"okeclusterkube_config\" { clusterid = ocicontainerenginecluster.okecluster.id } # https://docs.cloud.oracle.com/en-us/iaas/Content/ContEng/Tasks/contengdownloadkubeconfigfile.htm#notes provider \"kubernetes\" { loadconfigfile = \"false\" # Workaround for tf k8s provider &lt; 1.11.1 to work with ORM clustercacertificate = base64decode(yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"clusters\"\"cluster\") host = yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"clusters\"\"cluster\" exec { apiversion = \"client.authentication.k8s.io/v1beta1\" # Workaround for tf k8s provider &lt; 1.11.1 to work with orm - yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkubeconfig.content)\"users\"\"user\"[\"apiVersion\"] args = yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)[\"users\"\"user\"\"args\", yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"users\"\"user\"\"args\", yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"users\"\"user\"\"args\", yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"users\"\"user\"\"args\", yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"users\"\"user\"\"args\", yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"users\"\"user\"\"args\", yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"users\"\"user\"\"args\"] command = yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"users\"\"user\"[\"command\"] } } # https://docs.cloud.oracle.com/en-us/iaas/Content/ContEng/Tasks/contengdownloadkubeconfigfile.htm#notes provider \"helm\" { kubernetes { loadconfigfile = \"false\" # Workaround for tf helm provider &lt; 1.1.1 to work with ORM clustercacertificate = base64decode(yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"clusters\"\"cluster\") host = yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"clusters\"\"cluster\" exec { apiversion = yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkubeconfig.content)\"users\"\"user\"[\"apiVersion\"] args = yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)[\"users\"\"user\"\"args\", yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"users\"\"user\"\"args\", yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"users\"\"user\"\"args\", yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"users\"\"user\"\"args\", yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"users\"\"user\"\"args\", yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"users\"\"user\"\"args\", yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"users\"\"user\"\"args\"] command = yamldecode(data.ocicontainerengineclusterkubeconfig.okeclusterkube_config.content)\"users\"\"user\"[\"command\"] } } } resource \"kubernetesnamespace\" \"hivekafkanamespace\" { metadata { name = \"hivekafka\" } dependson = [ocicontainerenginenodepool.okenodepool] } resource \"helm_release\" \"hivemq\" { dependson = [ocicontainerenginenodepool.okenodepool] name = \"hivemq\" chart = \"helm_charts/hivemq\" namespace = kubernetesnamespace.hivekafkanamespace.id wait = false timeout = 300 } # Deploy kafka-connect chart resource \"helm_release\" \"kafkaconnect\" { dependson = [ocicontainerenginenodepool.okenodepool] name = \"kafka-connect\" chart = \"helm_charts/cp-kafka-connect\" namespace = kubernetesnamespace.hivekafkanamespace.id wait = false timeout = 120 } #Output the public IP addresses of the helm-chart generated service load balancers resource \"timesleep\" \"wait120_seconds\" { dependson = [helmrelease.hivemq] create_duration = \"120s\" } data \"kubernetesservice\" \"osskafka_connect\" { dependson = [timesleep.wait120seconds] metadata { name = \"oss-kafka-connect-service\" namespace = \"hivekafka\" } } data \"kubernetesservice\" \"hivemqmqtt\" { dependson = [timesleep.wait120seconds] metadata { name = \"hivemq-mqtt\" namespace = \"hivekafka\" } } output \"osskafkaconnectloadbalanceripaddress\" { //value = [data.kubernetesservice.osskafkaconnect.status.0.loadbalancer.0.ingress.0.ip] value = [data.kubernetesservice.osskafkaconnect.loadbalancer_ingress[0].ip] } output \"hivemqmqttloadbalancerip_address\" { //value = [data.kubernetesservice.hivemqmqtt.status.0.load_balancer.0.ingress.0.ip] value = [data.kubernetesservice.hivemqmqtt.loadbalanceringress[0].ip] } Examining the code: When you’re comparing the code with the example it’s easy to see that the content itself hasn’t really changed. We’ve only added some output, in this case the IP addresses of the loadbalancers, which will be automatically deployed by the Helm chart. Lastly, we’ve added a 120-second timeout since the loadbalancers will be deployed directly by Kubernetes, something which Terraform isn’t able to see. View the deployment To get a list of current deployments, pods, and services, run the following kubectl command: kubectl -n hivekafka get deployments,pods,services Your output should look something like: NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/hivemq-cluster 3/3 3 3 61m deployment.apps/oss-kafka-connect-deployment 2/3 3 2 61m NAME READY STATUS RESTARTS AGE pod/hivemq-cluster-59c44cdb59-fpvb9 1/1 Running 0 61m pod/hivemq-cluster-59c44cdb59-nw6dd 1/1 Running 0 61m pod/hivemq-cluster-59c44cdb59-txrt5 1/1 Running 0 61m pod/oss-kafka-connect-deployment-5f87774458-gbr9s 0/1 CrashLoopBackOff 11 61m pod/oss-kafka-connect-deployment-5f87774458-qnmgr 1/1 Running 12 61m pod/oss-kafka-connect-deployment-5f87774458-zcgpt 1/1 Running 12 61m pod/wallet-extractor-job-9j8xl 0/1 Completed 0 61m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/hivemq-control-center LoadBalancer 10.96.171.15 &lt;pending&gt; 8080:30666/TCP 61m service/hivemq-discovery ClusterIP None &lt;none&gt; 1883/TCP 61m service/hivemq-mqtt LoadBalancer 10.96.49.214 129.159.77.93 1883:31094/TCP 61m service/oss-kafka-connect-service LoadBalancer 10.96.114.200 129.159.74.132 80:31501/TCP 61m What’s next In the sixth and final segment of our series, we’ll discuss how to organize your tenants in a section on governance. By Malte Menkhoff","categories": ["iac","opensource"],
        "tags": ["open-source","terraform","iac","devops","get-started"],
        "url": "/tutorials/oci-iac-framework/getting-started-with-oci-step-5-workload-deployment",
        "teaser": ""
      },{
        "title": "Governance",
        "excerpt":" Compartments Something important to think about as you begin is how you will organize your tenants. Oracle Cloud Infrastructure offers a key feature in building up a virtual PC with a tenancy, and has introduced compartments to have a proper organizational structure so you can organize your infrastructure and use policies per compartments for a proper role and permission concept. When you first start working with Oracle Cloud Infrastructure, you need to think carefully about how you want to use compartments to organize and isolate your cloud resources. Compartments are fundamental to that process. Most resources can be moved between compartments. However, it’s important to think through the compartment design for your organization up front, before implementing anything. Compartments are tenancy-wide, across regions. When you create a compartment, it’s available in every region that your tenancy is subscribed to. You can get a cross-region view of your resources in a specific compartment with the tenancy explorer. After creating a compartment, you need to write at least one policy for it, otherwise no one can access it (except administrators or users who have permissions set at the tenancy level). When creating a compartment inside another compartment (up to six-levels of sub-compartments are supported), the compartment inherits access permissions from compartments higher up its hierarchy. When you create an access policy, you need to specify which compartment to attach it to. This controls who can later modify or delete the policy. Depending on how you’ve designed your compartment hierarchy, you might attach it to the tenancy, a parent, or to the specific compartment itself. To place a new resource in a compartment, you simply specify that compartment when creating the resource (the compartment is one of the required pieces of information to create a resource). Keep in mind that most IAM resources reside in the tenancy (this includes users, groups, compartments, and any policies attached to the tenancy) and can’t be created in or managed from a specific compartment. The structure of compartment varies in most cases by the organizational structure of the company. Well-established and large companies have, in many cases, centralized services like a security or a network compartment. Smaller and newer companies could have a leaner and less complex setup and are organized by projects without central entities which are responsible for certain elements in the infrastructure. The flexibility and the features by OCI in using compartments to organize and isolate cloud resources gives you the ability to build up your organization, or a desired new setup of your tenancy to fulfill your requirement in the organization of your elements. We are supporting both centralized and federated application DevOps models. Most common models are dedicated DevOps teams aligned with a single workload. In the case of smaller workloads or COTS or 3rd party application, a single AppDevOps team is responsible for workload operation. Independent of this model every DevOps team manages several workload staging environments (DEV, UAT, PROD) deployed to individual landing zones/subscriptions. Each landing zone has a set of RBAC permissions managed with OCI IAM provided by the Platform SecOps team. When the base is handed over to the DevOps team, the team is end-to-end responsible for the workload. They can independently operate within the security guardrails provided by the platform team. If dependency on central teams or functions are discovered, it is highly recommended to review the process and eliminated as soon as possible to unblock DevOps teams. A project-based setup: A department-based setup: Both setups are just examples and will require a discovery workshop with the customer to build the compartment structure based on his requirements. The landing zone, as part of the base setup in step 2, is intended to provide an initial setup as blueprint for a classical 3-tier web-application where each layer is logically separated for each department with centralized management of IAM, network, and security. Cloud Costs It’s key to have a clear view on cloud cost. Foundations: OCI Pricing and Billing and Billing and Cost Management allow you to improve control and visibility over your cloud budgets, usage, and spend. This document gives you some guidance how to manage Cloud cost effectively. You will be alerted based on your own business rules and are able to individually break down all your cloud usage. See Jenet (Cloud cost controller) in our 4 minutes Introduction to Oracle Cloud Infrastructure Cost Management video to get an initial idea of effective cloud cost management. In our example Jenet is responsible for Cost Management. This consists of: Manage Cloud Budgets Stay on top of cloud spend Analyze usage for cost optimization To do so, Oracle provides Jenet Enterprise-grade Controls for Cost Management. OCI provides you a comprehensive set of tools out of the box to manage Cloud cost effectively. Manage Cloud cost effectively (out of the box) Billing and Payment Tools Overview Oracle Cloud Infrastructure provides various billing and payment tools that make it easy to manage your service costs. Predictability and Control Budgets are set on cost-tracking tags or on compartments (including the root compartment) to track all spending in that cost-tracking tag or for that compartment and its children. Budgets can be used to set thresholds for your Oracle Cloud Infrastructure spending. You can set alerts on your budget to let you know when you might exceed your budget, and you can view all of your budgets and spending from one single place in the Oracle Cloud Infrastructure console. See Budgets Overview for more information. Budgets help you track your Oracle Cloud Infrastructure (OCI) spending. They monitor costs at a compartment level or cost-tracking tag level. You can set alerts on a budget to receive an email notification based on an actual or forecasted spending threshold. Budget alerts also integrate with the Events service. You can use this integration and the Oracle Notifications service to send messages through PagerDuty, Slack, or SMS. You can also use the integration with Events service to trigger functions that create quotas resulting in budgets with hard limits. Create a budget and alert Create a function Create a rule As a result, you can prevent the creation of new Compute resources in your tenancy. Anyone who tries to create resources after crossing the budget is unable to do so and sees a message notifying them that the compartment quota was exceeded. Source: Enforced budgets on OCI using functions and quotas Visibility Cost Analysis Dashboard provides easy-to-use visualization to help you track and optimize your Oracle Cloud Infrastructure spending by Service (shown by default when the Cost Analysis page first opens) Service and Description Service and SKU (Part Number) Service and Tag (see Oracle Cloud Infrastructure Tagging for more details) Compartment (see Oracle Cloud Infrastructure Compartments for more details) Monthly Costs To use Cost Analysis, the following policy statement is required: Allow group &lt;group_name&gt; to read usage-report in tenancy A cost report is a comma-separated value (CSV) file that is similar to a usage report, but also includes cost columns. The report can be used to obtain a breakdown of your invoice line items at resource-level granularity. As a result, you can optimize your Oracle Cloud Infrastructure spending, and make more informed cloud spending decisions. A usage report is a comma-separated value (CSV) file that can be used to get a detailed breakdown of resources in Oracle Cloud Infrastructure for audit or invoice reconciliation. To use cost and usage reports, the following policy statement is required: define tenancy usage-report as ocid1.tenancy.oc1..aaaaaaaaned4fkpkisbwjlr56u7cj63lf3wffbilvqknstgtvzub7vhqkggq endorse group &lt;group&gt; to read objects in tenancy usage-report For more information, see Cost and Usage Reports Overview Deutsche Standardkontenrahmen Ein Kontenrahmen ist ein Verzeichnis, das alle Kostenarten systematisch numerischen Konten für die Buchführung in einem Wirtschaftszweig zuordnet. Er dient als Richtlinie und Empfehlung für die Aufstellung eines konkreten Kontenplans in einem Unternehmen. Damit sollen einheitliche Buchungen von gleichen Geschäftsvorfällen erreicht und zwischenbetriebliche Vergleiche ermöglicht werden. (Quelle: Wikipedia) SKR 03 (für publizitätspflichtige Firmen – Prozessgliederungsprinzip) SKR 04 (für publizitätspflichtige Firmen – Abschlussgliederungsprinzip, Kontenrahmen nach dem Bilanzrichtliniengesetz (BiRiliG) unter Berücksichtigung der Neuerungen des Bilanzrechtsmodernisierungsgesetz(BilMoG)) Wir stellen hier für Sie eine Abbildung der Standardkontenrahmen SKR 03, SKR 04 als “Defined Tags” zur Verfügung. Diese Tags können Sie mit dem Cost Analysis Dashboard auswerten. Um die dafür notwendigen Namespaces zu verwalten benötigen Sie folgende Berechtigungen Allow group GroupA to use tag-namespaces in tenancy Um die dafür notwendigen Namespaces auszuwerten benötigen Sie folgende Berechtigungen Allow group GroupA to read tag-namespaces in tenancy Standardkontenrahmen SKR 03 z.B. DATEV-Kontenrahmen nach dem Bilanzrichtlinie-Umsetzungsgesetz Standardkontenrahmen - Prozessgliederungsprinzip (SKR 03) Die hier beispielhaft implementierten Konten stammen aus der Quelle Software, Anschaffung und Abschreibung. Mapping Standardkontenrahmen zu Namespaces Namespace Key Value Resources SKR03 0 Anlage- und Kapitalkonten   SKR03 0027 EDV-Software Entgeltlich erworbene Konzessionen, gewerbliche Schutzrechte und ähnliche Rechte und Werte sowie Lizenzen an solchen Rechten und Werten SKR03 0044 EDV-Software Selbst geschaffene immaterielle Vermögensgegenstände SKR03 1 Finanz- und Privatkonten   SKR03 2 Abgrenzungskonten   SKR03 3 Wareneingangs- und Bestandkonten   SKR03 4 Betriebliche Aufwendungen   SKR03 4806 Wartungskosten für Hard- und Software Sonstige betriebliche Aufwendungen SKR03 4822 Abschreibungen auf immaterielle Vermögensgegenstände Abschreibungen auf immaterielle Vermögensgegenstände des Anlagevermögens und Sachanlagen SKR03 4964 Aufwendungen für die zeitlich befristete Überlassung von Rechten (Lizenzen, Konzessionen) Sonstige betriebliche Aufwendungen SKR03 7 Bestände an Erzeugnissen   SKR03 8 Erlöskonten   SKR03 8995 Aktivierte Eigenleistungen zur Erstellung von selbst geschaffenen immateriellen Vermögensgegenständen Andere aktivierte Eigenleistungen SKR03 9 Vortrags- und statistische Konten   Implementierungsbeispiel: resource \"ociidentitytagnamespace\" \"skr03tag_namespace\" { Required compartmentid = var.compartmentid description = \"Standardkontenrahmen - Prozessgliederungsprinzip\" name = \"SKR03\" Optional defined_tags = {\"SKR03.0\" = \"Anlage- und Kapitalkonten\" } defined_tags = {\"SKR03.0027\" = \"EDV-Software (Entgeltlich erworbene Konzessionen, gewerbliche Schutzrechte und ähnliche Rechte ...)\" } defined_tags = {\"SKR03.0044\" = \"EDV-Software (Selbst geschaffene immaterielle Vermögensgegenstände) \" } defined_tags = {\"SKR03.1\" = \"Finanz- und Privatkonten\" } defined_tags = {\"SKR03.2\" = \"Abgrenzungskonten\" } defined_tags = {\"SKR03.3\" = \"Wareneingangs- und Bestandkonten\" } defined_tags = {\"SKR03.4\" = \"Betriebliche Aufwendungen\" } defined_tags = {\"SKR03.4806\" = \"Wartungskosten für Hard- und Software\" } defined_tags = {\"SKR03.4822\" = \"Abschreibungen auf immaterielle Vermögensgegenstände\" } defined_tags = {\"SKR03.4964\" = \"Aufwendungen für die zeitlich befristete Überlassung von Rechten (Lizenzen, Konzessionen)\" } defined_tags = {\"SKR03.7\" = \"Bestände an Erzeugnissen\" } defined_tags = {\"SKR03.8\" = \"Erlöskonten\" } defined_tags = {\"SKR03.8995\" = \"Aktivierte Eigenleistungen zur Erstellung von selbst geschaffenen immateriellen Vermögensgegenständen\" } defined_tags = {\"SKR03.9\" = \"Vortrags- und statistische Konten\" } is_retired = false } Standardkontenrahmen SKR 04 z.B. DATEV-Kontenrahmen nach dem Bilanzrichtlinie-Umsetzungsgesetz Standardkontenrahmen - Abschlussgliederungsprinzip (SKR 04) Die hier beispielhaft implementierten Konten stammen aus der Quelle Software, Anschaffung und Abschreibung. Mapping Standardkontenrahmen zu Namespaces Namespace Key Value Resources SKR04 0 Anlagevermögen (Bestand: Aktiv)   SKR04 0135 EDV-Software Entgeltlich erworbene Konzessionen, gewerbliche Schutzrechte und ähnliche Rechte und Werte sowie Lizenzen an solchen Rechten und Werten SKR04 0144 EDV-Software Selbst geschaffene immaterielle Vermögensgegenstände SKR04 1 Umlaufvermögen (Bestand: Aktiv)   SKR04 2 Eigenkapitalkonten (Bestand: Passiv)   SKR04 3 Fremdkapitalkonten (Bestand: Passiv)   SKR04 4 Betriebliche Erträge (Erfolg: Ertrag)   SKR04 4825 Aktivierte Eigenleistungen zur Erstellung von selbst geschaffenen immateriellen Vermögensgegenständen Andere aktivierte Eigenleistungen SKR04 5 Betriebliche Aufwendungen (Erfolg: Aufwand)   SKR04 6 Betriebliche Aufwendungen (Erfolg: Aufwand)   SKR04 6200 Abschreibungen auf immaterielle Vermögensgegenstände Abschreibungen auf immaterielle Vermögensgegenstände des Anlagevermögens und Sachanlagen SKR04 6495 Wartungskosten für Hard- und Software Sonstige betriebliche Aufwendungen SKR04 6835 Mieten für Einrichtungen (bewegliche Wirtschaftsgüter) Cloud Ressourcen wie z.B. Compartment, Group, Policy, User, Network, Storage, Compute können hier verbucht werden. SKR04 6837 Aufwendungen für die zeitlich befristete Überlassung von Rechten (Lizenzen, Konzessionen) Sonstige betriebliche Aufwendungen SKR04 7 Weitere Erträge und Aufwendungen (Erfolg: Aufwand, Ertrag)   SKR04 9 Vortrags- und statistische Konten (Bestand: Rechnungsabgrenzung usw.)   Implementierungsbeispiel: resource \"ociidentitytagnamespace\" \"skr04tag_namespace\" { Required compartmentid = var.compartmentid description = \"Standardkontenrahmen - Abschlussgliederungsprinzip\" name = \"SKR04\" Optional defined_tags = {\"SKR04.0\" = \"Anlagevermögen (Bestand: Aktiv)\" } defined_tags = {\"SKR04.1\" = \"Umlaufvermögen (Bestand: Aktiv)\" } defined_tags = {\"SKR04.0135\" = \"EDV-Software (Entgeltlich erworbene Konzessionen, gewerbliche Schutzrechte und ähnliche Rechte ...)\" } defined_tags = {\"SKR04.0144\" = \"EDV-Software (Selbst geschaffene immaterielle Vermögensgegenstände) \" } defined_tags = {\"SKR04.2\" = \"Eigenkapitalkonten (Bestand: Passiv)\" } defined_tags = {\"SKR04.3\" = \"Fremdkapitalkonten (Bestand: Passiv)\" } defined_tags = {\"SKR04.4\" = \"Betriebliche Erträge (Erfolg: Ertrag)\" } defined_tags = {\"SKR04.4825\" = \"Aktivierte Eigenleistungen zur Erstellung von selbst geschaffenen immateriellen Vermögensgegenständen\" } defined_tags = {\"SKR04.5\" = \"Betriebliche Aufwendungen (Erfolg: Aufwand)\" } defined_tags = {\"SKR04.6\" = \"Betriebliche Aufwendungen (Erfolg: Aufwand)\" } defined_tags = {\"SKR04.6200\" = \"Abschreibungen auf immaterielle Vermögensgegenstände\" } defined_tags = {\"SKR04.6495\" = \"Wartungskosten für Hard- und Software\" } defined_tags = {\"SKR04.6837\" = \"Aufwendungen für die zeitlich befristete Überlassung von Rechten (Lizenzen, Konzessionen)\" } defined_tags = {\"SKR04.7\" = \"Weitere Erträge und Aufwendungen (Erfolg: Aufwand, Ertrag)\" } defined_tags = {\"SKR04.9\" = \"Vortrags- und statistische Konten (Bestand: Rechnungsabgrenzung usw.)\" } is_retired = false } Unified Billing This topic describes how you can unify billing across multiple tenancies by sharing your subscription. You should consider sharing your subscription if you want to have multiple tenancies to isolate your cloud workloads, but you want to have a single Universal Credits commitment. For example, you have a subscription with a $150,000 commitment, but you want to have three tenancies, because the credits are going to be used by three distinct groups that require strictly isolated environments. Two types of tenancies are involved when sharing a subscription in the Console: The parent tenancy (the one that is associated with the primary funded subscription). Child tenancies (those that are consuming from a subscription that is not their own). Notable benefits of sharing a subscription includes: Sharing a single commitment helps to avoid cost overages and allows you to consolidate your billing. Enabling multi-tenancy cost management. You can analyze, report, and monitor across all linked tenancies. The parent tenancy has the ability to analyze and report across each of your tenancies through Cost Analysis and Cost and usage reports, and you can receive alerts through Budgets. Isolation of data. Customers with strict data isolation requirements can use a multi-tenancy strategy to continue restricting resources across their tenancies. The remainder of this topic provides an overview of how to share your subscription between tenancies, and provides best practices on how to isolate workloads, in order to help you determine if you should use a single-tenancy or multi-tenancy strategy. You can unify billing across multiple tenancies by sharing your subscription between tenancies. To use subscription sharing, the following policy statements are required: Allow group linkUsers to use organizations-family in tenancy Allow group linkAdmins to manage organizations-family in tenancy For more information, see Unified Billing Overview. Invoices You can view and download invoices for your Oracle Cloud Infrastructure usage. Oracle Order-to-Cash has launched a dedicated page Customer Billing Support to support our customers in understanding the Oracle Cloud invoicing experience. When visiting Customer Billing Support, customers can access content targeting specific needs and easily submit billing inquiries. The web page content is as follows: Billing Support: Email or call Oracle’s global Collections offices. Videos: Brief animations detailing various aspects of the invoice process. Billing Basics: This journey through Oracle Cloud billing basics covers the events that trigger the invoicing process and when to expect a bill. Subscription Invoicing: A guide to billing for Oracle metered and non-metered subscriptions. Overage and Bursting: This video explains how to avoid unexpected charges for Oracle Cloud services. Dispute Process: In this guide through the Oracle dispute process, customers learn who to contact and how to resolve billing questions. FAQ: Consult our frequently asked questions regarding Cloud invoicing. Glossary: Basic terminology used for Cloud features and services. For questions or any additional information, please contact cloudinvoicingus@oracle.com or see Viewing Your Subscription Invoice. Payment Methods The Payment Method section of the Oracle Cloud Infrastructure Console allows you to easily manage how you pay for your Oracle Cloud Infrastructure usage. For more information, see Changing Your Payment Method. Manage Cloud cost effectively (more advanced) Optimization If you’re using any cloud, you might regularly ask yourself questions like, “Why is the bill so high this month?” or “What would it actually cost to move this application to the cloud?” If so, this blog is for you. Today, I aim to make you familiar with the practices you need to control and predict your cost without compromising your performance. Whether you’re part of the finance department in charge of controlling the budget, a business decision-maker evaluating a new project, or a DevOps engineer thinking of new functionality for your application, cloud cost management is mission-critical and can make or break your business. Accessing limitless possibilities is leading to cloud exuberance, and it’s time to tame the beast. Tag everything from day 1 Sharing is not caring Time is money Choose performance responsibly Focus your attention on the whales Consolidate your databases Listen to your advisor Involve your stakeholders and automate Adopt cloud native technologies and containers Compare prices and total cost of ownership You find more details to do this in 10 effective ways to save cost in the cloud. Extensibility Oracle Cloud Infrastructure Usage and Cost Reports to Autonomous Database Tool usage2adw usage2adw is a tool which uses the Python SDK to extract the usage and cost reports from your tenant and load it to Oracle Autonomous Database. (DbaaS can be used as well) Authentication to OCI by User or instance principals. It uses APEX for Visualization and generates Daily e-mail report. Main Features Usage Current State Usage Over Time Cost Analysis Cost Over Time Rate Card for Used Products from GitHub Modern Cloud Economics Unlocking business value of cloud for enterprise workloads. A C-Suite’s guide to build and execute the Enterprise Cloud Strategy that delivers cloud’s full business value potential. Commercial principles enable enterprises to continuously leverage the optimal commercial frameworks of cloud service provider, based on the changing usage profiles and deployment requirements, thereby de-risking unexpected cost overruns as well as maximizing the combined financial productivity of on-premise licenses, annual license support, and cloud subscription. The principles are the following: Delink data and network linear usage from cost Avoid service deployment lock-in Re-purpose on-premise spend to acquire future cloud capabilities OCI enablers for Commercial principles OCI offers a range of commercial enablers to optimize rate, de-risk cost overruns and maximize financial productivity across the investments in Oracle on-premise licenses and cloud subscriptions. The key enablers are: Best price performance guarantee Avoid service deployment lock-in Re-purpose on-premise spend to acquire future cloud capabilities By Malte Menkhoff","categories": ["iac","opensource"],
        "tags": ["open-source","terraform","iac","devops","get-started"],
        "url": "/tutorials/oci-iac-framework/getting-started-with-oci-step-6-governance",
        "teaser": ""
      },{
        "title": "Hosting a private Helm Repository on OCI (Oracle Cloud Infrastructure) with ChartMuseum and OCI Object Storage",
        "excerpt":"In another article, we deployed a helm chart (Redis) into Oracle Container Engine (OKE) which we pulled from the stable repository. We’ve already described how to host the container images privately on Oracle Registry Service (OCIR). What if we want to do the same for our helm charts? Fortunately, there is a solution for that too. In this article we’ll introduce you to chartmuseum, an open source Helm Chart Repository server with support for various cloud storage backends, including OCI Object Storage. Additional information By default, the stable repo is hosted at here. We can also manually add the incubator repository if we want to: helm repo add incubator https://kubernetes-charts-incubator.storage.googleapis.com/ This should respond with something similar to: \"incubator\" has been added to your repositories The container images in those repositories are also publicly available (for example, the default image for redis is bitnami/redis in bitnami’s container redis repository on Dockerhub). Prerequisites An Oracle Cloud Infrastructure Free Tier account. Start for Free. A MacOS, Linux, or Windows computer with ssh support installed. OCI Cloud Shell - It provides a great platform for quickly working with Terraform as well as a host of other OCI interfaces and tools. Getting started In this tutorial, we’ll be using the terraform-oci-oke project to provision the OKE cluster. For more details, see this post on provisioning OKE using Terraform. Below is an architecture of what we’ll build: Install nginx-controller Install DNS - Follow this post to install ExternalDNS. nginx - Install nginx-controller. We’ll use this to access chartmuseum later. In the command below, replace chartmuseum.acme.com by your preferred hostname: helm install --name nginxcontroller stable/nginx-ingress \\ --set controller.name=controller \\ --set defaultBackend.enabled=true \\ --set defaultBackend.name=defaultbackend \\ --set rbac.create=true \\ --set controller.service.annotations.\"external-dns\\.alpha\\.kubernetes\\.io/hostname\"=chartmuseum.acme.com Verify that an OCI Load Balancer has been created and a DNS A record has been inserted in the DNS Zone. Create OCI Bucket In the OCI Console, navigate to Object Storage and create a bucket called chartmuseum: Deploy chartmuseum We want a similar experience to the stable or incubator repos: anonymous GET but protected POST requests Basic Auth for authentication purposes Create secrets We’ll need to create 2 secrets to accomplish this. secret 1 - Let’s start by creating a secret to hold the Basic Auth username and password: kubectl create secret generic chartmuseum-auth --from-literal=user=curator --from-literal=pass=password secret 2 - You’ll also need to create a secret that will allow chartmuseum to communicate with the OCI APIs. Temporarily, copy your API keys to the bastion. You can also do this locally if you have kubectl and local access to the kubeconfig. Create a file config and enter your user and tenancy OCIDs, api key fingerprint, and region value. The key_file value has to be /home/chartmuseum/.oci/oci.key. [DEFAULT] user=&lt;USER_OCID&gt; fingerprint=&lt;APIKEYFINGERPRINT&gt; key_file=/home/chartmuseum/.oci/oci.key tenancy=&lt;TENANCY_OCID&gt; region=&lt;REGION&gt; Create the secret using appropriate absolute or relative path to the config file and private API key. kubectl create secret generic chartmuseum-secret --from-file=config=\"/path/to/config\" --from-file=key_file=\"/path/to/apikey.pem\" Install chartmuseum With the secrets created, we’re ready to start deploying chartmuseum. Download the chartmuseum values file: curl -o values.yaml https://raw.githubusercontent.com/helm/charts/master/stable/chartmuseum/values.yaml Edit the values.yaml file and replace the parameters as follows: env: open: STORAGE: oracle STORAGEORACLECOMPARTMENTID:&lt;COMPARTMENT_OCID&gt; STORAGEORACLEBUCKET: chartmuseum STORAGEORACLEPREFIX: chartmuseum DISABLE_API: false AUTHANONYMOUSGET: true AUTH_REALM: chartmuseumexistingSecret: chartmuseum-auth existingSecretMappings: BASICAUTHUSER: user BASICAUTHPASS: passingress: enabled: true labels: dns: \"ocidns\"annotations: kubernetes.io/ingress.class: nginxhosts: - name: chartmuseum.acme.com path: / tls: falseoracle: secret: enabled: true name: chartmuseum-secret config: config keyfile: keyfile Install chartmuseum: helm install --name=chartmuseum -f values.yaml stable/chartmuseum Wait for the pod to run: kubectl get pods -w This should give output similar to: NAME READY STATUS RESTARTS AGE chartmuseum-chartmuseum-748c8dbbd8-7nctc 1/1 Running 0 5m20s Verify installation Verify that the ingress has been created: kubectl get ing This should give output similar to: NAME HOSTS ADDRESS PORTS AGE chartmuseum-chartmuseum chartmuseum.acme.com 80 7m9s And that it also maps to the chartmuseum service: k describe ing This should give output similar to: chartmuseum-chartmuseum Name: chartmuseum-chartmuseum Namespace: default Address: Default backend: default-http-backend:80 () Rules: Host Path Backends ---- ---- -------- chartmuseum.acme.com / chartmuseum-chartmuseum:8080 () Annotations: kubernetes.io/ingress.class: nginx Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CREATE 6m20s nginx-ingress-controller Ingress default/chartmuseum-chartmuseum Normal UPDATE 5m31s nginx-ingress-controller Ingress default/chartmuseum-chartmuseum Verify whether you can reach chartmuseum publicly with your browser. Pushing a chart to chartmuseum Install the helm push plugin: helm plugin install https://github.com/chartmuseum/helm-push Add the repo: helm repo add --username curator --password password cm http://chartmuseum.acme.com/ Create a basic test chart, mychart helm create mycharthelm package mychart This should respond with something similar to: Successfully packaged chart and saved it to: /home/opc/chart/mychart-0.1.0.tgz Push the chart: helm push mychart cm This should respond with something similar to: Pushing mychart-0.1.0.tgz to cm... Done. If we search for mychart at this point, we’ll only find the local copy: helm search mychart This will respond with something similar to: NAME CHART VERSION APP VERSION DESCRIPTION local/mychart 0.1.0 1.0 A Helm chart for Kubernetes We’ll need to do a few things to get mypart to appear. First, do a repo update: helm repo update cm This should respond with something similar to: Hang tight while we grab the latest from your chart repositories... ...Skip local chart repository ...Successfully got an update from the \"cm\" chart repository ...Successfully got an update from the \"stable\" chart repository Update Complete. ⎈ Happy Helming!⎈ And then, conduct a new search: helm search mychart This should respond with something similar to: NAME CHART VERSION APP VERSION DESCRIPTION cm/mychart 0.1.0 1.0 A Helm chart for Kubernetes local/mychart 0.1.0 1.0 A Helm chart for Kubernetes Our test chart (mychart) appears! Testing authentication At this point, we could test authentication by removing the repo and adding it again without the username and password. Instead, we’ll try something a little more elegant and use a combination of Postman and chartmuseum’s APIs to test. These are the behaviors we’re looking for: GET: No Auth Required POST: Auth Required DELETE: Auth Required From this, you can see we’re able to get a list of charts using GET without authentication. However, a DELETE or POST without authentication fails: Yet another twist: when we enter the credentials, the chart deletion succeeds: If we now check OCI Object Storage, we can see our chart there: Let’s Encrypt Now that chartmuseum is up and running, we’ll also want to secure its usage by encrypting traffic. To do this, we’ll use Let’s Encrypt and cert-manager. cert-manager is an “add-on to automate the management and issuance of TLS certificates from various issuing sources.” At this point, you can follow along with the cert-manager installation guide or read on. Create the CustomResourceDefinitions namespace for cert-manager and disable resource validation on the namespace: kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.8/deploy/manifests/00-crds.yamlkubectl create namespace cert-managerkubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true Add the Jetpack Helm repo and update: helm repo add jetstack https://charts.jetstack.io helm repo update jetstack Install cert-manager using its helm chart: helm install --name cert-manager --namespace cert-manager \\ Verify the installation: kubectl get pods --namespace cert-manager This should respond with something similar to: NAME READY STATUS RESTARTS AGE cert-manager-776cd4f499-98vsh 1/1 Running 0 3h14m cert-manager-cainjector-744b987848-pkk5s 1/1 Running 0 3h14m cert-manager-webhook-645c7c4f5f-4mbjd 1/1 Running 0 3h14m Test that the webhook works by creating a test-resources.yaml file: apiVersion: v1 kind: Namespace metadata: name: cert-manager-test --- apiVersion: certmanager.k8s.io/v1alpha1 kind: Issuer metadata: name: test-selfsigned namespace: cert-manager-test spec: selfSigned: {} --- apiVersion: certmanager.k8s.io/v1alpha1 kind: Certificate metadata: name: selfsigned-cert namespace: cert-manager-test spec: commonName: example.com secretName: selfsigned-cert-tls issuerRef: name: test-selfsigned Create the test resources: kubectl create -f test-resources.yaml Check the status of the newly-created certificate: kubectl describe certificate -n cert-manager-testName: selfsigned-cert It should respond with something similar to: Namespace: cert-manager-test Labels: Annotations: API Version: certmanager.k8s.io/v1alpha1 Kind: Certificate Metadata: Creation Timestamp: 2019-06-25T00:45:25Z Generation: 1 Resource Version: 116448 Self Link: /apis/certmanager.k8s.io/v1alpha1/namespaces/cert-manager-test/certificates/selfsigned-cert UID: 8576413b-96e2-11e9-b5fc-0a580aed39b8 Spec: Common Name: example.com Issuer Ref: Name: test-selfsigned Secret Name: selfsigned-cert-tls Status: Conditions: Last Transition Time: 2019-06-25T00:45:26Z Message: Certificate is up to date and has not expired Reason: Ready Status: True Type: Ready Not After: 2019-09-23T00:45:26Z Events: Note: You can delete the test resources after testing: kubectl delete -f test-resources.yaml Configuring Let’s Encrypt issuers For reference, you can follow the quick-start guide for using cert-manager with Nginx Ingress. Let’s start with creating a staging issuer by creating a staging-issuer.yaml and using your email address: apiVersion: certmanager.k8s.io/v1alpha1 kind: Issuer metadata: name: cm-staging spec: acme: # The ACME server URL server: https://acme-staging-v02.api.letsencrypt.org/directory # Email address used for ACME registration email: youremailaddress # Name of a secret used to store the ACME account private key privateKeySecretRef: name: cm-staging # Enable the HTTP-01 challenge provider http01: {} Create the staging issuer: kubectl create -f staging-issuer.yaml This will respond with something similar to: issuer.certmanager.k8s.io/cm-staging created Repeat the above for a production environment by creating a production-issuer.yaml, once again using your email address: apiVersion: certmanager.k8s.io/v1alpha1 kind: Issuer metadata: name: cm-prod spec: acme: # The ACME server URL server: https://acme-v02.api.letsencrypt.org/directory # Email address used for ACME registration email: youremailaddress # Name of a secret used to store the ACME account private key privateKeySecretRef: name: cm-prod # Enable the HTTP-01 challenge provider http01: {} Create the issuer: kubectl create -f production-issuer.yaml Check the status of the staging issuer: kubectl describe issuer cm-stagingName: cm-staging This should respond with something similar to: Namespace: default Labels: Annotations: API Version: certmanager.k8s.io/v1alpha1 Kind: Issuer Metadata: Creation Timestamp: 2019-06-25T03:28:16Z Generation: 1 Resource Version: 143856 Self Link: /apis/certmanager.k8s.io/v1alpha1/namespaces/default/issuers/cm-staging UID: 452908da-96f9-11e9-b5fc-0a580aed39b8 Spec: Acme: Email: youremailaddress Http 01: Private Key Secret Ref: Name: cm-staging Server: &lt;https://acme-staging-v02.api.letsencrypt.org/directory&gt; Status: Acme: Uri: &lt;https://acme-staging-v02.api.letsencrypt.org/acme/acct/9718789&gt; Conditions: Last Transition Time: 2019-06-25T03:28:17Z Message: The ACME account was registered with the ACME server Reason: ACMEAccountRegistered Status: True Type: Ready Events: Enable TLS on chartmuseum Edit your values.yaml file for chartmuseum and look for the Ingress section. Pay special attention to the added configuration commands. annotations: kubernetes.io/ingress.class: nginx kubernetes.io/tls-acme: \"true\" certmanager.k8s.io/issuer: \"cm-staging\" certmanager.k8s.io/acme-challenge-type: http01 ## Chartmuseum Ingress hostnames ## Must be provided if Ingress is enabled ## hosts: - name: chartmuseum.acme.com path: / tls: true tlsSecret: cm-tls Upgrade your helm chart: helm upgrade chartmuseum stable/chartmuseum -f values.yaml This will respond with something similar to: Release \"chartmuseum\" has been upgraded. Happy Helming! cert-manager will read the annotations and create a certificate: kubectl get certificate This will respond with something similar to: NAME READY SECRET AGE cm-tls True cm-tls 45m Take a quick peek at the certificate: kubectl describe certificate cm-tlsName: cm-tls This will respond with something similar to: Namespace: default Labels: app=chartmuseum chart=chartmuseum-2.3.1 dns=ocidns heritage=Tiller release=chartmuseum Annotations: API Version: certmanager.k8s.io/v1alpha1 Kind: Certificate Metadata: Creation Timestamp: 2019-06-25T03:33:47Z Generation: 1 Owner References: API Version: extensions/v1beta1 Block Owner Deletion: true Controller: true Kind: Ingress Name: chartmuseum-chartmuseum UID: d5a9cee1-9673-11e9-a790-0a580aed1020 Resource Version: 152567 Self Link: /apis/certmanager.k8s.io/v1alpha1/namespaces/default/certificates/cm-tls UID: 0ac806f6-96fa-11e9-8836-0a580aed4b3e Spec: Acme: Config: Domains: chartmuseum.acme.com Http 01: Ingress Class: nginx Dns Names: chartmuseum.acme.com Issuer Ref: Kind: Issuer Name: cm-staging Secret Name: cm-tls Status: Conditions: Last Transition Time: 2019-06-25T04:19:27Z Message: Certificate is up to date and has not expired Reason: Ready Status: True Type: Ready Not After: 2019-09-23T03:19:27Z Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Cleanup 44m cert-manager Deleting old Order resource \"cm-tls-3817114402\" Normal OrderCreated 43m (x2 over 44m) cert-manager Created Order resource \"cm-tls-3433596774\" Normal OrderComplete 43m cert-manager Order \"cm-tls-3433596774\" completed successfully Normal Cleanup 2m21s cert-manager Deleting old Order resource \"cm-tls-3433596774\" Normal Generated 2m14s (x3 over 47m) cert-manager Generated new private key Normal GenerateSelfSigned 2m14s (x3 over 47m) cert-manager Generated temporary self signed certificate Normal OrderCreated 2m14s (x3 over 47m) cert-manager Created Order resource \"cm-tls-3817114402\" Normal OrderComplete 2m13s (x2 over 47m) cert-manager Order \"cm-tls-3817114402\" completed successfully Normal CertIssued 2m13s (x3 over 47m) cert-manager Certificate issued successfully View additional certificate details Once complete, cert-manager will have created a secret with the details of the certificate based on the secret used in the ingress resource. You can use the describe command as well to see some details: kubectl describe secret cm-tls This should respond with something similar to: Name: cm-tls Namespace: default Labels: certmanager.k8s.io/certificate-name=cm-tls Annotations: certmanager.k8s.io/alt-names: chartmuseum.acme.com certmanager.k8s.io/common-name: chartmuseum.acme.com certmanager.k8s.io/ip-sans: certmanager.k8s.io/issuer-kind: Issuer certmanager.k8s.io/issuer-name: cm-staging Type: kubernetes.io/tls Data ==== ca.crt: 0 bytes tls.crt: 3578 bytes tls.key: 1679 bytes Accessing chartmuseum If you access chartmuseum now, you’ll see a warning: If you ignore the warning and go ahead anyway, you’ll be able to access chartmuseum except that now you’ll be accessing it over HTTPS. Your browser will also warn you that it’s added an exception to this site. Let’s take a look at how to correct this: Edit the values.yaml for your chartmuseum again and this time change the issuer annotation to cm-prod: certmanager.k8s.io/issuer: \"cm-prod\" Run a helm upgrade again: helm upgrade chartmuseum stable/chartmuseum -f values.yaml This should respond with something similar to: Release \"chartmuseum\" has been upgraded. Happy Helming! Delete the secret: kubectl delete secret cm-tls This will cause cert-manager to get a new certificate. You can verify this: kubectl describe certificate cm-tls This will respond with something similar to: . . .Normal Generated 33s (x4 over 61m) cert-manager Generated new private key Normal GenerateSelfSigned 33s (x4 over 61m) cert-manager Generated temporary self signed certificate Normal OrderCreated 33s (x4 over 58m) cert-manager Created Order resource \"cm-tls-3433596774\" Normal CertIssued 31s (x5 over 61m) cert-manager Certificate issued successfully Normal OrderComplete 31s (x3 over 57m) cert-manager Order \"cm-tls-3433596774\" completed successfully If you now access chartmuseum, you will be able to access it with HTTPS and will not be prompted with the security warning/exception. What’s next We’ve covered a lot of ground in this article. Hopefully, you’ve found this useful! At this point, you should be ready to explore all of the features that chartmuseum has to offer. Chartmuseum enhances your CI/CD capabilities by: enabling you to host your helm charts privately and securely integrating with your CI/CD deployment tool chain and pipelines supporting multiple teams and organizations with multitenant capabilities using a variety of storage capabilities including local file system, Oracle Object Storage, OpenStack Object storage, and others. Running chartmuseum outside of your Kubernetes cluster: If you’d like to run it outside your Kubernetes cluster (like on a VM), you can follow this guide instead. Additional information: To explore more information about development with Oracle products: Oracle Developers Portal Oracle Cloud Infrastructure References Chartmuseum docs Chartmuseum helm package docs cert-manager installation guide cert-manager with Nginx Ingress By ","categories": null,
        "tags": ["back-end","oci"],
        "url": "/tutorials/helm-repository-chartmuseum-object",
        "teaser": ""
      },{
        "title": "Get Started with the Feature Store HopsWorks (LogicalClocks) on Oracle Cloud",
        "excerpt":" 1 Introduction In this walk-through you’ll configure your environment to run HopsWorks in Oracle Cloud Infrastructure. Prerequisites VM 2.1 with Oracle Linux 7.9 (OEL7) has been deployed in Oracle Cloud Infrastructure (OCI) Oracle Linux 7.9 uses pip3.6 by default Python 3.6 or higher is installed You have access to root either directly or using sudo. In OCI you are connected as user opc with sudo privilege by default Begin &raquo; 2 Jupyterlab Installation The Jupyterlab install is pretty simple. It consists of setting up Python, then installing Python components and libraries. Let’s start with setting up the Python environment. Python Setup By default, OEL7 runs Python 3. The first step is to install virtualenv and pip. Install virtualenv Virtualenv enables us to create isolated sandpits to develop Python applications without running into module or library conflicts. It’s simple to install. $ sudo pip3.6 install virtualenv Next, we can create a virtual environment and enable it. Create a myvirtualenv Environment $ virtualenv -p /usr/bin/python3 myvirtualenv # Activate the env $ source myvirtualenv/bin/activate Check the List of Python Libraries in Your Environment Running the following command will show what Python models we have installed at this point. $ pip3 list Package Version ---------- ------- pip 21.1.3 setuptools 57.1.0 wheel 0.36.2 WARNING: You are using pip version 21.1.3; however, version 21.2.1 is available. You should consider upgrading via the '/home/opc/myvirtualenv/bin/python -m pip install --upgrade pip' command. Upgrade Your pip Environment $ /home/opc/myvirtualenv/bin/python -m pip install --upgrade pip Jupyterlab Setup $ pip3 install jupyterlab Install Python Libraries for Machine Learning or an ETL Process $ pip install pandas $ pip install pandarallel $ pip install dask $ pip install seaborn $ pip install matplotlib $ pip install plotly $ pip install -lxml==4.6.3 $ pip install selenium $ pip install beautifulsoup4 $ pip install scikit-learn Install Other Python Libraries for Kafka Access and WEB Server Access $ pip install kafka-python (v2.0.0) $ pip install Flask $ pip install gunicorn Install Extensions for Jupyterlab Environment $ pip install jupytercontribnbextensions $ jupyter contrib nbextension install --user $ jupyter nbextension enable execute_time/ExecuteTime &laquo; Back Continue &raquo; 3 Configure Jupyterlab Like the OEL7 Linux Service Create a script to automatically instantiate and reboot Jupyterlab with opc user. $ vi /home/opc/launchjupyterlab.sh Script for launchjupyterlab.sh Using virtualenv, you can launch Jupyterlab on a specific port (for example: 8001) and listen on a public IP. #!/bin/bash # Activate myvirtualenv Environment source myvirtualenv/bin/activate cd /home/opc if [ \"$1\" = \"start\" ]; then nohup jupyter-lab --ip=0.0.0.0 --port=8001 &gt; ./nohup.log 2&gt;&amp;1 &amp; echo $! &gt; /home/opc/jupyter.pid else kill $(cat /home/opc/jupyter.pid) fi We need to make the script executable so it can be run from the jupyterlab service. $ chmod 777 /home/opc/launchjupyterlab.sh Connect to Root User $ sudo -i Create Script to Start, Stop “jupyterlab” $ vi /etc/systemd/system/jupyterlab.service Launch “opc” User with “launchjupyterlab.sh” [Unit] Description=Service to start jupyterlab for opc Documentation= [Service] User=opc Group=opc Type=forking WorkingDirectory=/home/opc ExecStart=/home/opc/launchjupyterlab.sh start ExecStop=/home/opc/launchjupyterlab.sh stop [Install] WantedBy=multi-user.target Test Jupyterlab Service $ systemctl start jupyterlab $ systemctl status jupyterlab $ systemctl enable jupyterlab &laquo; Back Continue &raquo; 4 Reboot for a Final Check Now reboot your machine to check if the jupyterlab script is enabled by default on port 8001. You need to open port 8001 to your virtual machine VM 2.1 in order to access using your public IP. $ firewall-cmd --permanent --zone=public --list-ports $ firewall-cmd --get-active-zones $ firewall-cmd --permanent --zone=public --add-port=8001/tcp $ firewall-cmd --reload If you’re running directly on a virtual machine and have a browser installed, it should take you directly into the jupyter environment. Connect to “http://xxx.xxx.xxx.xxx:8001/”. You should see the next Python web environment “Jupyterlab.” &laquo; Back By Olivier Francois Xavier Perard","categories": ["cloudapps"],
        "tags": ["open-source","machine-learning"],
        "url": "/tutorials/hops-works-logicalclocks",
        "teaser": ""
      },{
        "title": "How to Deploy Spark Standalone in Oracle Cloud (OCI)",
        "excerpt":" 1 Introduction The following walk-through guides you through setting up your environment to run Spark and Hadoop on Oracle Cloud Infrastructure (OCI). Begin &raquo; 2 Prerequisites You’ve deployed a VM 2.1 or + with Oracle Linux 7.9 (OEL7) in OCI. You’re running Oracle Linux 7.9 which is running JVM by default. You have access to root either directly or via sudo. By default in OCI, you are connected like “opc” user with sudo privilege. [opc@xxx ~]$ java -version java version \"1.8.0_281\" Java(TM) SE Runtime Environment (build 1.8.0_281-b09) Java HotSpot(TM) 64-Bit Server VM (build 25.281-b09, mixed mode) &laquo; Back Continue &raquo; 3 Java Installation The install is quite simple. It consists of setting up Java, then installing Spark and Hadoop components and libraries. Let’s start with setting up the Spark and Hadoop environment. Download the last version of JDK 1.8 because Hadoop 2.X is using this Java version. rpm -ivh /home/opc/jdk-8u271-linux-x64.rpm Check Java Version. java -version &laquo; Back Continue &raquo; 4 Spark and Hadoop Setup The next step is to install Spark and Hadoop environment. First, choose the version of Spark and Hadoop you want to install. Then, download the version you want to install: Download Spark 2.4.5 for Hadoop 2.7 cd /home/opc wget http://apache.uvigo.es/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz Download Spark 2.4.7 for Hadoop 2.7 wget http://apache.uvigo.es/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz Download Spark 3.1.1 for Hadoop 3.2 wget http://apache.uvigo.es/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz Install the Spark and Hadoop Version Install the Spark and Hadoop Version chosen in the directory “/opt”. sudo -i cd /opt tar -zxvf /home/opc/spark-2.4.5-bin-hadoop2.7.tgz #or tar -zxvf /home/opc/spark-2.4.7-bin-hadoop2.7.tgz #or tar -zxvf /home/opc/spark-3.1.1-bin-hadoop3.2.tgz &laquo; Back Continue &raquo; 5 Install PySpark in Python3 environment /opt/Python-3.7.6/bin/pip3 install 'pyspark=2.4.7' /opt/Python-3.7.6/bin/pip3 install findspark Next we shall create a virtual environment and enable it. Modify your environment to use this Spark and Hadoop Version Add to “.bashrc” for the user “opc” the following lines: # Add by %OP% export PYTHONHOME=/opt/anaconda3 export PATH=$PYTHONHOME/bin:$PYTHONHOME/condabin:$PATH # SPARK ENV #export JAVAHOME=$(/usr/libexec/javahome) export SPARK_HOME=/opt/spark-2.4.5-bin-hadoop2.7 export PATH=$SPARK_HOME/bin:$PATH export PYSPARK_PYTHON=python3 export PYSPARKDRIVERPYTHON=jupyter export PYSPARKDRIVERPYTHON_OPTS='notebook' &laquo; Back Continue &raquo; 6 Test your Spark and Hadoop Environment If you’re running directly on a virtual machine and have a browser installed, it should take you directly into the jupyter environment. Connect to http://xxx.xxx.xxx.xxx:8001/. Then upload the next notebooks: Notebook test findspark. Notebook test Pyspark. Notebook test Pyspark with Mysql. &laquo; Back By Olivier Francois Xavier Perard","categories": null,
        "tags": ["oci","open-source","spark","java","data-science"],
        "url": "/tutorials/how-to-deploy-spark-oci",
        "teaser": ""
      },{
        "title": "Topics",
        "excerpt":" Filter: All Tags Collections Personas Languages Frameworks Topics always-free 8 analytics 6 ansible 1 apache 1 apex 1 api 2 architect 1 automation 5 back-end 16 community 1 data-management 3 data-science 3 data-visualization 1 devops 41 docker 2 field-notes 1 flask 1 front-end 5 full-stack 1 gaming 3 get-started 32 go 3 graalvm 2 iac 24 iot 2 java 2 javascript 4 jupyter 2 kubernetes 11 machine-learning 2 mysql 10 nodejs 10 oci 27 oke 7 open-source 49 orm 1 php 1 postman 1 python 7 rpi 1 ruby 1 serverless 2 spark 1 spring 1 streaming 1 terraform 25 typescript 2 ubuntu 2 verrazzano 4 Deploying Verrazzano on Oracle Container Engine for Kubernetes Oracle Cloud Infrastructure IaC Framework Proxima Safe Rust on OCI Steampipe on Oracle Cloud Infrastructure Terraform 101 Tutorials Way to Go on OCI Personas architect 1 back-end 16 community 1 data-management 3 data-science 3 devops 41 front-end 5 full-stack 1 Languages go 3 java 2 javascript 4 mysql 10 nodejs 10 php 1 python 7 ruby 1 terraform 25 typescript 2 Frameworks ansible 1 flask 1 graalvm 2 kubernetes 11 spark 1 spring 1 Tags always-free 8 analytics 6 apache 1 apex 1 api 2 automation 5 data-visualization 1 docker 2 field-notes 1 gaming 3 get-started 32 iac 24 iot 2 jupyter 2 machine-learning 2 oci 27 oke 7 open-source 49 orm 1 postman 1 rpi 1 serverless 2 streaming 1 ubuntu 2 verrazzano 4 By ","categories": null,
        "tags": null,
        "url": "/topics/",
        "teaser": ""
      },{
        "title": "Collections",
        "excerpt":" Deploying Verrazzano on Oracle Container Engine for Kubernetes Deploying a multi-cluster Verrazzano instance on Oracle Container Engine for Kubernetes (OCI). Oracle Cloud Infrastructure IaC Framework Jumpstart your OCI experience using the OCLOUD framework to build your environment in 7 steps Proxima Safe Joining the Dots in OCI to Build a Stream Analysis Lab. Rust on OCI Use Rust to implement applications on Oracle Cloud Infrastructure Steampipe on Oracle Cloud Infrastructure Resources for using Steampipe on Oracle Cloud Infrastructure. Terraform 101 Jumpstart your OCI management experience using IaC (specifically Terraform) with this tutorial series. Tutorials Build your skillset with tutorials on OCI and Oracle tools. Way to Go on OCI Use Go to implement an application on Oracle Cloud Infrastructure Latest tutorials How to get Going on Oracle Cloud Compute Dec 12th, 2022 How to build and run Go applications on Oracle Cloud Infrastructure Compute Instances. How to produce logging from the Go application and capture the logging into OCI Logging. open-source devops get-started back-end go Way to Go on OCI Series Dec 12th, 2022 Use Go to implement an application on Oracle Cloud Infrastructure open-source devops get-started automation iac Building an API in Rust and hosting on Oracle Cloud Infrastructure Dec 12th, 2022 How to build an API using Rust and host it on Oracle Cloud Infrastructure. open-source devops get-started back-end rust Using Sybl to connect to your Oracle Database Dec 12th, 2022 How to build safely with modern technology. open-source devops get-started back-end rust Getting started with Rust Dec 12th, 2022 The beginning of a series on building applications using the Rust programming language. open-source devops get-started back-end rust Building Safe, Fast, and Manageable Applications with Rust and Oracle Cloud Infrastructure Dec 12th, 2022 How to build safely with modern technology. open-source devops get-started back-end rust Rust on OCI Series Dec 12th, 2022 Use Rust to implement applications on Oracle Cloud Infrastructure open-source devops get-started rust Using the AWS Migration Service with HeatWave on AWS with WordPress as an Example Jun 16th, 2022 A quick example of how you can move data over for use in MySQL HeatWave on AWS. mysql database heatwave Connecting To and Managing HeatWave on AWS Jun 16th, 2022 Getting signed up, signed on, and some basic management tasks for your HeatWave on AWS cluster. mysql database heatwave aws Getting Started with MySQL HeatWave on AWS Jun 16th, 2022 Getting your OCI tenancy ready to connect to MySQL HeatWave on AWS. We will create a compute instance, DB System, and endpoint. We also began to provision MySQL HeatWave on AWS. mysql database heatwave aws HeatWave on AWS Metrics and Performance Tools Jun 16th, 2022 A quick overview of the performance and metrics console windows in MySQL HeatWave on AWS. mysql database heatwave Creating a Simple Chatbot using NodeJS on OCI May 26th, 2022 A very basic chatbot using nodejs on an oracle cloud compute instance. open-source oci nodejs Working in Go applications with Oracle Database and Oracle Cloud Autonomous Database May 26th, 2022 Working with on premises Oracle Database and OCI Autonomous Database in Go applications, either on OCI Compute Instances or anywhere else. open-source devops get-started automation iac Oracle Cloud Infrastructure Functions in Go and Using the OCI Go SDK for accessing OCI services from Go May 26th, 2022 Updated: Dec 12th, 2022 Building OCI Functions in Go and using the Go SDK for OCI to leverage OCI services. open-source devops get-started automation serverless docker back-end go iac Go Software Engineering and Automation with Oracle Cloud Infrastructure DevOps May 26th, 2022 How to build, deploy, test and expose Go applications on Oracle Cloud Infrastructure with DevOps, Resource Manager, Compute and API Gateway. open-source devops get-started automation back-end go iac Hooking Go applications into OCI Streaming, using OCI Key Vault and Go Deployment on OKE May 26th, 2022 Hooking Go applications into OCI Streaming, using OCI Key Vault and Go Deployment on OKE open-source devops get-started automation iac Get started with Ruby on Rails on Oracle Cloud Apr 27th, 2022 ruby rails MySQL RubyOnRails Deploying a Custom Nodejs Web Application Integrated with Identity Cloud Service for Unique Single Sign On UX Mar 10th, 2022 How to configure custom Node.js web application to have a unique Single Sign On user experience. open-source oci always-free nodejs javascript Tracing A Node.js with OCI Application Performance Monitoring and Zipkin Feb 25th, 2022 Using Zipkin as an open-source tracking tool with OCI Application Performance Monitoring. open-source oci nodejs Tracing A Node.js with OCI Application Performance Monitoring and Zipkin Feb 25th, 2022 Using Zipkin as an open-source tracking tool with OCI Application Performance Monitoring. open-source oci nodejs Sending Emails from OCI with Email Delivery Service in Node.js Feb 14th, 2022 Use Oracle Cloud Infrastructure to manage a high-volume email solution for sending out emails to many recipients for critical communications. open-source oci always-free nodejs javascript Making calls to the Oracle Cloud API using Postman Feb 3rd, 2022 Learn how to make calls directly to the OCI API using Postman. api oci postman Creating an E-commerce Site with Oracle Coherence CE and Micronaut Jan 14th, 2022 Coherence and Micronaut are powerful bedfellows when used together. This tutorial steps you through using them to build a hypothetical online sock shop. That’s right, socks. analytics oci League of Legends Optimizer using Oracle Cloud Infrastructure—Building an Adversarial League of Legends AI Model Jan 7th, 2022 In this series’ third article, Ignacio uses Python to train an ML model for 1v1 matchups. analytics gaming League of Legends Optimizer using Oracle Cloud Infrastructure—Data Extraction and Processing Jan 7th, 2022 Using machine learning to optimize your LoL experience isn’t as hard as you think. Ignacio shows you how. analytics gaming Deploying and monitoring a Redis cluster to Oracle Container Engine (OKE) Dec 15th, 2021 Redis, Prometheus, OKE, oh my! back-end oci Hosting a private Helm Repository on OCI (Oracle Cloud Infrastructure) with ChartMuseum and OCI Object Storage Dec 15th, 2021 This guide walks you through how to host Helm Chart privately with ChartMuseum, an open source Helm Chart Repository server with support for various cloud storage backends, including OCI Object Storage. back-end oci Virtual Desktop Infrastructure(VDI) for Unreal Engine 5 on Oracle Cloud Infrastructure with nVidia GPU Dec 12th, 2021 Get Virtual Desktop Infrastructure set up on Oracle Cloud Infrastructure with nVidia GPU. Deploying MongoDB in Oracle Cloud (OCI) Linux VM Dec 10th, 2021 MongoDB and Oracle Cloud Infrastructure play well together — Olivier shows you how to configure your envionment to connect them. devops python Using Oracle's Machine Translation Services for NLP Analysis Dec 10th, 2021 Ignacio walks you through Oracle MTS, a translation service, using Python and even provides a benchmarking function to test its speed. python oci Governance Dec 9th, 2021 Introduction to Governance as part of the OCLOUD framework open-source terraform iac devops get-started Oracle Cloud Infrastructure IaC Framework Series Dec 9th, 2021 Jumpstart your OCI experience using the OCLOUD framework to build your environment in 7 steps open-source terraform iac devops get-started Workload Deployment Dec 9th, 2021 How to deploy and configure your code on the OCLOUD framework landing zone open-source terraform iac devops get-started Application Infrastructure Dec 6th, 2021 How to deploy application infrastructure on top of the OCLOUD framework open-source terraform iac devops get-started Deploying Cassandra in Oracle Linux Dec 3rd, 2021 Cassandra, an open-source NoSQL database, plays well with Oracle Cloud Infrastructure. Let Olivier show you how to configure it. open-source oci back-end Deploying A Multi-Cluster Verrazzano On Oracle Container Engine for Kubernetes (OKE) Part 2 Dec 3rd, 2021 How to deploy Verrazzano an OKE cluster. open-source oke kubernetes verrazzano terraform devops Deploying A Multi-Cluster Verrazzano On Oracle Container Engine for Kubernetes (OKE) Part 1 Dec 3rd, 2021 How to deploy Verrazzano an OKE cluster. open-source oke kubernetes verrazzano terraform devops Deploying Verrazzano on Oracle Container Engine for Kubernetes (OKE) Dec 3rd, 2021 How to deploy Verrazzano an OKE cluster. open-source oke kubernetes verrazzano terraform devops Deploying Verrazzano on Oracle Container Engine for Kubernetes Series Dec 3rd, 2021 Deploying a multi-cluster Verrazzano instance on Oracle Container Engine for Kubernetes (OCI). open-source verrazzano oke devops kubernetes Database Infrastructure Dec 1st, 2021 How to deploy database infrastructure on top of the OCLOUD framework open-source terraform iac devops get-started Free Tier: Install WordPress on an Ubuntu Instance Nov 24th, 2021 This tutorial guides you through configuring WordPress on your Ubuntu OCI instance. ubuntu back-end Installing and using Calico on Oracle Container Engine (OKE) Nov 24th, 2021 Ali walks you through configuring OKE with Calico, an open-source networking tool for Kubernetes. oke devops Introduction to Steampipe on Oracle Cloud Infrastructure Nov 24th, 2021 Steampipe is a great tool that you should have in your IaC toolbox. Learn about how to use it with OCI here! open-source steampipe iac devops get-started field-notes Modernizing the Healthcare platform with a GraalVM Proof of Value Nov 24th, 2021 Ali makes the case for GraalVM in the healthcare industry by diving deep into a couple strategic solutions. graalvm devops Call a Function using API Gateway Nov 20th, 2021 Using Oracle functions to process data via an Oracle API Gateway, and creating a python function to extract HTTP information. python oci How to import data from Microsoft SQL Server to MySQL Database Service Nov 18th, 2021 This tutorial walks you through the process for exporting data from Microsoft SQL Server and then importing it into MySQL Database Service in OCI. mySQL analytics back-end Use Matomo Website Analytics on OCI with MDS Nov 18th, 2021 Matamo, an alternative to Google Analytics, is becoming increasingly popular. This walkthrough shows you how to use this powerful tool with MySQL Database Service and Oracle Cloud Infrastructure. mysql analytics back-end Get Started with the Feature Store HopsWorks (LogicalClocks) on Oracle Cloud Nov 18th, 2021 This walk-through will guide you through setting up your environment to run HopsWorks with OCI. open-source machine-learning Installation Guide for OCI Monitoring Nov 18th, 2021 This tutorial walks you through configuring a basic OCI monitoring solution with components based on Ansible in Oracle Linux 8. ansible data-visualization data-management back-end Deploying Verrazzano on Oracle Container Engine for Kubernetes (OKE) Nov 12th, 2021 How to deploy Verrazzano an OKE cluster. open-source oke kubernetes terraform devops Creating flexible OCI Load Balancers with OKE Nov 2nd, 2021 New load balancer shapes mean more options to optimize your configuration. Ali walks you through how to create these shapes using OKE. kubernetes More Terraform Resources Nov 1st, 2021 A curated list of helpful resources for expanding your knowledge of Terraform. open-source terraform iac devops get-started Destroying resources with Terraform Nov 1st, 2021 How to keep things neat and tidy with Terraform. open-source terraform iac devops get-started Polyglot Application Observability Oct 31st, 2021 Use GraalVM to leverage the PinPoint service, which allows tracing transactions and data flows between multiple software components and identifies problematic areas along with potential bottlenecks. graalvm Deploy a modern data lake on OCI Oct 29th, 2021 Using Terraform to create a data lake on Oracle Cloud Infrastructure. kubernetes devops terraform oci Service Delivery Framework Oct 29th, 2021 How to deploy a landing zone in OCI open-source terraform iac devops get-started Automating OCI with Terraform Oct 29th, 2021 You want to know how you can automate tasks on OCI - here you can find a short deep dive into automation with Terraform open-source terraform iac devops get-started Getting Started with Oracle Cloud Infrastructure (OCI) Oct 29th, 2021 Want to start with OCI and don’t know how? Here’s a quick look at what OCI is all about. open-source terraform iac devops get-started Deploying Joomla! on OCI and MDS Oct 28th, 2021 MDS (MySQL Database Service) and Moodle are a perfect match for OCI. In this tutorial, you’ll learn how to deploy this popular learning management system with OCI. docker nodejs mysql Extending Terraform OKE with a helm chart Oct 28th, 2021 Extend a sample repo with your own extensions to make reusable provisioning scripts. kubernetes devops terraform Deploy Moodle on OCI with MDS Oct 27th, 2021 MDS (MySQL Database Service) and Moodle are a perfect match for OCI. In this tutorial, you’ll learn how to deploy this popular learning management system with OCI. oci mysql Using OCI Cloud Shell & Bastion with MySQL Database Service Oct 26th, 2021 Recently, Oracle added a Bastion Service to OCI. This tutorial walks you through using this service with OCI Cloud Shell. oci mysql always-free League of Legends Optimizer—Data Extraction and Processing Oct 26th, 2021 Can OCI be used to make League of Legends even better? Ignacio Martínez walks you through configuring an ML and data-based approach to to this best selling game. api gaming oci always-free python Get Started with Apache and PHP on Ubuntu and OCI Oct 21st, 2021 Using Free Tier OCI to install PHP and Apache on an Ubuntu Linux instance, then connecting the instance to the internet for further development. always-free get-started open-source apache php ubuntu nodejs front-end Deploy Apache Superset with MySQL Database Service on Oracle Cloud Infrastructure Oct 20th, 2021 Deploy Open Source BI, reporting tool on OCI using MySQL Databse Service. data-management front-end mysql oci analytics Terraform Variables Oct 14th, 2021 Learn about how variables are used in Terraform. open-source terraform iac devops get-started Understanding The Basics Of Terraform Oct 14th, 2021 Let’s go through some of the foundational concepts you should understand when working with Terraform. open-source terraform iac devops get-started How to Deploy a Python Flask Application in a Kubernetes cluster Oct 11th, 2021 Setting up a Kubernetes cluster using Oracle Cloud, and deploying a Python application with Flask framework using Cloud Shell. flask kubernetes oci open-source python Experiencing Terraform Oct 8th, 2021 Experience the power of Terraform through a short project. open-source terraform iac devops get-started Install JupyterLab in OCI Oct 7th, 2021 This tutorial will guide you through setting up your environment to run JupyterLab on Oracle Cloud Infrastructure. oci get-started jupyter python data-science machine-learning open-source Get Started with your own LAMP stack application on Oracle Cloud Oct 1st, 2021 Build Move &amp; Modernize Apps and Hybrid Solutions, DevOps and Automation on OCI. data-management front-end mysql oci orm Making changes using Terraform Sep 30th, 2021 Time to make changes to our tutorial environment using Terraform. But how?! open-source terraform iac devops get-started Creating Resources with Terraform Sep 30th, 2021 How to create resources using Terraform. open-source terraform iac devops get-started Kubernetes - Deploy a Node Express Application Sep 30th, 2021 How to deploy a Node Express application to a Kubernetes cluster on OCI. OCI open-source nodejs front-end kubernetes How to Deploy Spark Standalone in Oracle Cloud (OCI) Sep 30th, 2021 A short walkthrough of setting up Spark and Hadoop in your OCI environment. oci open-source spark java data-science Why Infrastructure as Code? Sep 28th, 2021 Heard about Infrastructure as Code (IaC) but not sure what it’s about or why you should care? This tutorial’s for you! open-source terraform iac devops get-started Terraform 101 Series Sep 28th, 2021 Jumpstart your OCI management experience using IaC (specifically Terraform) with this tutorial series. open-source terraform iac devops get-started Deploying the Argo CD on Oracle Container Engine for Kubernetes (OKE) Sep 22nd, 2021 How to deploy the Argo CD on an OKE cluster. open-source oke kubernetes terraform devops Install Node Express on an Oracle Linux Instance Sep 22nd, 2021 Use an Oracle Cloud Infrastructure Free Tier account to set up an Oracle Linux compute instance, install a Node Express application and access your new app from the internet. open-source oci always-free nodejs javascript typescript front-end Manually configuring Data Science service on Oracle Cloud Infrastructure Sep 22nd, 2021 Configure your tenancy for Data Science and test creating a notebook session. oci data-science nodejs javascript typescript Deploy Cassandra on Oracle Cloud (OCI) Linux VM Sep 21st, 2021 Set up your environment to run Cassandra in Oracle Cloud Infrastructure. oci python jupyter back-end Install Spring Boot on an compute instance on Oracle Cloud Sep 20th, 2021 Updated: Sep 22nd, 2021 Use an Oracle Cloud Infrastructure Free Tier account to set up an Oracle Linux compute instance, install a Spring Boot application, set up a virtual network, and access your new app from the internet. oci java always-free back-end spring Proxima Safe Series Aug 20th, 2021 Joining the Dots in OCI to Build a Stream Analysis Lab. Mr. Stark's Briefcase and ProximaSafe Aug 20th, 2021 Updated: Sep 1st, 2021 oci iot streaming serverless rpi full-stack Build Your Own Language Translator on Oracle Cloud Aug 15th, 2021 Using Oracle Cloud to learn a to speak a new language oci iot apex always-free architect community By ","categories": null,
        "tags": null,
        "url": "/collections/",
        "teaser": ""
      },{
        "title": "Way to Go on OCI",
        "excerpt":" By Lucas Jellema","categories": null,
        "tags": ["open-source","devops","get-started","automation","iac"],
        "url": "/tutorials/way-to-go-on-oci/",
        "teaser": ""
      },{
        "title": "Steampipe on Oracle Cloud Infrastructure",
        "excerpt":" Steampipe Articles Steampipe is a terrific open-source tool that falls within the infrastructure-as-code (IaC) space, allowing you to interact with OCI using SQL statements. How cool is that?! These articles are here to help make life easier, using Steampipe with OCI. Steampipe on Oracle: Introduction to Steampipe on Oracle Cloud Infrastructure By Tim Clegg","categories": null,
        "tags": ["open-source","terraform","iac","devops","get-started"],
        "url": "/tutorials/steampipe/",
        "teaser": ""
      },{
        "title": "Oracle Cloud Infrastructure IaC Framework",
        "excerpt":" Landing Zone Difficulty: Expert Want to get started with Oracle Cloud Infrastructure (OCI), but not sure how to do it in a pragmatic way? This series will give you a structured approach to getting started with OCI in seven steps! Here’s what we’ll cover: Introduction to OCI Step 1: Overview of the OCI terraform provider Step 2: How to build the basic infrastructure Step 3: How to build the database infrastructure Step 4: How to build the application infrastructure Step 5: How to deploy your workload on top of the infrastructure Step 6: How to introduce governance in your tenancy Step 7: Create and provide an easy to consume service catalog of your environment By Malte Menkhoff","categories": null,
        "tags": ["open-source","terraform","iac","devops","get-started"],
        "url": "/tutorials/oci-iac-framework/",
        "teaser": ""
      },{
        "title": "Rust on OCI",
        "excerpt":" By ","categories": null,
        "tags": ["open-source","devops","get-started","rust"],
        "url": "/tutorials/rust-on-oci/",
        "teaser": ""
      },{
        "title": "Use Cases",
        "excerpt":" AI/ML and Data Science (1) AR/VR and Makers (1) Build and Run Cloud Native Apps (34) Cloud-Native Software Development on OCI (13) DevOps and Automation on OCI (1) Enterprise Cloud Native Development (1) Top Frameworks for Top Languages (10) Video Games, Servers, and Development (2) Java, GraalVM, and Helidon (1) Build, Move, and Modernize Applications (8) Run and Integrate Open Source Solutions on OCI (22) Personal Cloud Services (1) By ","categories": null,
        "tags": null,
        "url": "/use-cases/",
        "teaser": ""
      },{
        "title": "Tutorials",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/tutorials/",
        "teaser": ""
      },{
        "title": "Deploying Verrazzano on Oracle Container Engine for Kubernetes",
        "excerpt":" Deploying Verrazzano Verrazzano packs a nice set of capabilities that helps you with the operational side of Kubernetes. From monitoring to logging and security, a Kubernetes or application administrator can get more done, faster. So let’s try Verrazzano on Oracle Container Engine for Kubernetes (OKE)! Here’s what we’ll cover in the series: Setting up: We’ll begin with a brief introduction to Verrazzano, Oracle’s end-to-end container platform for “multi-cloud and hybrid environments” Part One: We’ll create a cluster using the Terraform OKE module Part Two: We’ll conclude by configuring the clusters so they behave like a large, global cluster By ","categories": null,
        "tags": ["open-source","verrazzano","oke","devops","kubernetes"],
        "url": "/tutorials/multi-cluster-verrazzano-oke/",
        "teaser": ""
      },{
        "title": "Proxima Safe",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/tutorials/proxima-safe/",
        "teaser": ""
      },{
        "title": "Terraform 101",
        "excerpt":" Terraform 101 Difficulty: Beginner. Heard about Infrastructure as Code (IaC) but not sure if it’s for you? Tired of pointing-and-clicking to manage your OCI infrastructure or just looking to reacquaint yourself? This set of tutorials is for you! Here’s what we’ll cover: Why Infrastructure as Code? Experiencing Terraform Understanding the basics of Terraform Terraform variables Creating Resources with Terraform Making changes using Terraform Destroying resources with Terraform More Terraform Resources By Tim Clegg","categories": null,
        "tags": ["open-source","terraform","iac","devops","get-started"],
        "url": "/tutorials/tf-101/",
        "teaser": ""
      },{
        "title": "Install JupyterLab in OCI",
        "excerpt":" 1 Introduction This tutorial will guide you through setting up your environment to run JupyterLab on Oracle Cloud Infrastructure. JupyterLab is the web-based user interface for Project Jupyter. For additional information, see: Getting started with JupyterLab JupyterLab Documentation Signing Up for Oracle Cloud Infrastructure Getting started with OCI Cloud Shell Begin &raquo; 2 Prerequisites To successfully complete this tutorial, you’ll need the following: An Oracle Cloud Infrastructure Free Tier account. Start for Free. A MacOS, Linux, or Windows computer with ssh support installed. OCI Cloud Shell (It provides a great platform for quickly working with Terraform as well as a host of other OCI interfaces and tools.) Virtual Machine 2.1 with Oracle Linux 7.9 (OEL7) deployed in Oracle Cloud Infrastructure (OCI). Oracle Linux 7.9 using pip3.6 by default. Python 3.6 or higher installed Access to root, either directly or via sudo. By default in OCI, you are connected as the opc user with sudo privilege. &laquo; Back Continue &raquo; 3 Getting started Getting up and running with JupyterLab is pretty simple. We’ll cover all the important steps in this tutorial: Setting up Python and installing Python components and libraries Installing JupyterLab Configuring JupyterLab Lets start with setting up the Python Environment. &laquo; Back Continue &raquo; 4 Python Setup By default, OEL7 runs Python 3. The first step is to install pip and virtualenv. Install virtualenv Virtualenv enables us to create isolated sandboxes for developing Python applications without running into module or library conflicts. It’s very simple to install: sudo pip3.6 install virtualenv Next, we create a virtual environment called myvirtualenv virtualenv -p /usr/bin/python3 myvirtualenv To activate the new environment: source myvirtualenv/bin/activate Now, to get a list of Python Libraries in our environment. The following command will show which Python models we have installed at this point: console (myvirtualenv)$ pip3 list Package Version ---------- ------- pip 21.1.3 setuptools 57.1.0 wheel 0.36.2 WARNING: You are using pip version 21.1.3; however, version 21.2.1 is available. You should consider upgrading via the '/home/opc/myvirtualenv/bin/python -m pip install --upgrade pip' command. To upgrade your PIP Environment for this virtual environment: /home/opc/myvirtualenv/bin/python -m pip install --upgrade pip &laquo; Back Continue &raquo; 5 JupyterLab Setup Use pip to install JupyterLab: pip3 install jupyterlab Install Python Libraries for Machine Learning or ETL Process: pip install pandas pip install pandarallel pip install dask pip install seaborn pip install matplotlib pip install plotly pip install -lxml==4.6.3 pip install selenium pip install beautifulsoup4 pip install scikit-learn Install other Python libraries for Kafka Access and WEB Server Access: pip install kafka-python (v2.0.0) pip install Flask pip install gunicorn Install extensions for JupyterLab environment: pip install jupytercontribnbextensions jupyter contrib nbextension install --user jupyter nbextension enable execute_time/ExecuteTime &laquo; Back Continue &raquo; 6 Configure JupyterLab like an OEL7 Linux Service Create a script to instantiate automatically and reboot jupyterlab with the opc user. vi /home/opc/launchjupyterlab.sh Add the following content to launchjupyterlab.sh. You must use the virtualenv you created, and you can launch JupyterLab on a specific port (e.g., 8001) and listen on your VM’s public IP. #!/bin/bash # Activate myvirtualenv Environment source myvirtualenv/bin/activate cd /home/opc if [ \"$1\" = \"start\" ]; then nohup jupyter-lab --ip=0.0.0.0 --port=8001 &gt; ./nohup.log 2&gt;&amp;1 &amp; echo $! &gt; /home/opc/jupyter.pid else kill $(cat /home/opc/jupyter.pid) fi Now, we’ll need to make the script executable so it can be run from the jupyterlab service: chmod 777 /home/opc/launchjupyterlab.sh Connect to the root user: sudo -i Create a script to start and stop the jupyterlab service: vi /etc/systemd/system/jupyterlab.service Add the following to jupyterlab.service: [Unit] Description=Service to start jupyterlab for opc Documentation= [Service] User=opc Group=opc Type=forking WorkingDirectory=/home/opc ExecStart=/home/opc/launchjupyterlab.sh start ExecStop=/home/opc/launchjupyterlab.sh stop [Install] WantedBy=multi-user.target Test the JupyterLab Service: systemctl start jupyterlab systemctl status jupyterlab systemctl enable jupyterlab &laquo; Back Continue &raquo; 7 Reboot your VM Reboot your machine to check if the jupyterlab script is enabled by default on the port we defined (8001). Open port 8001 to your virtual machine VM 2.1 so you can access JupyterLab using your Public IP: firewall-cmd --permanent --zone=public --list-ports firewall-cmd --get-active-zones firewall-cmd --permanent --zone=public --add-port=8001/tcp firewall-cmd --reload Connect to http://xxx.xxx.xxx.xxx:8001/ (replacing xxx with your public IP) in your browser. NOTE: If you’re running directly on a virtual machine and have a browser installed, it should take you directly into the Jupyter environment. You should now see the Python Web environment “JupyterLab”. &laquo; Back Continue &raquo; 8 What's next To explore more information about development with Oracle products: Oracle Developers Portal Oracle Cloud Infrastructure &laquo; Back By ","categories": ["ai-ml"],
        "tags": ["oci","get-started","jupyter","python","data-science","machine-learning","open-source"],
        "url": "/tutorials/install-jupyter-lab-on-oci",
        "teaser": ""
      },{
        "title": "Install Spring Boot on an compute instance on Oracle Cloud",
        "excerpt":" 1 Introduction In this tutorial, you’ll use an Oracle Cloud Infrastructure Free Tier account to set up an Oracle Linux compute instance, install a Spring Boot application, and then configure it to be accessible from the internet. This tutorial also covers all the steps necessary to set up a virtual network for your host and connect that host to the internet. Key tasks include how to: Set up a compartment for your development work. Install your Oracle Linux instance and connect it to your Virtual Cloud Network (VCN). Set up an Oracle Cloud Infrastructure virtual cloud network and related network services required for your host to connect to the internet. Set up ssh encryption keys to access your Oracle Linux Server. Configure ingress rules for your VCN. Configure Spring Boot on your instance. Connect to your instance from the internet. Below is a simplified diagram of the setup for your Linux instance. For additional information, see: Signing Up for Oracle Cloud Infrastructure Launch your first Linux VM Begin &raquo; 2 Before you begin To successfully complete this tutorial, you must have the following: Requirements An Oracle Cloud Infrastructure Free Tier account. Start for Free. A MacOS, Linux, or Windows computer with ssh support installed. &laquo; Back Continue &raquo; 3 Set up a compartment for development Configure a compartment for your development. Create a compartment Create a compartment for the resources that you create in this tutorial. Log in to the Oracle Cloud Infrastructure Console. Open the navigation menu and select Identity &amp; Security. Under Identity, select Compartments. Select Create Compartment. Fill in the following information: Name: &lt;your-compartment-name&gt; Description: Compartment for &lt;your-description&gt;. Parent Compartment: &lt;your-tenancy&gt;(root) Select Create Compartment. Reference: Create a compartment &laquo; Back Continue &raquo; 4 Install your Oracle Linux Instance Use the Create a VM Instance wizard to create a new compute instance. The wizard does several things when installing the instance: Creates and installs a compute instance running Oracle Linux. Creates a VCN with the required subnet and components needed to connect your Oracle Linux instance to the internet. Creates an ssh key pair you use to connect to your instance. Review installation steps To get started installing your instance with the Create a VM Instance wizard, follow these steps: From the main landing page, select Create a VM instance wizard. The Create Compute Instance page is displayed. It has a section for Placement, Image and shape, Networking, Add SSH keys, and Boot volume. Choose the Name and Compartment for the instance. Initial Options Name: &lt;name-for-the-instance&gt; Create in compartment: &lt;your-compartment&gt; Either enter a value for the Name or leave the system-supplied default. Review the Placement settings. Accept the default values provided by the wizard. The following is sample data. The actual values change over time and differ from data center to data center. Placement Availability domain: AD-1 Capacity type: On-demand capacity. Fault domain: Oracle chooses the best placement. For Free Tier, use the Always Free Eligible option for Availability domain. Review the Image and shape settings. Accept the default values provided by the wizard. The following is sample data. The actual values change over time and differ from data center to data center. Image: Image: Oracle Linux 7.9 Image build: 2020.11.10-1 Shape: Shape: VM.Standard.E2.1.Micro OCPU count: 1 Memory (GB): 1 Network bandwidth (Gbps): 0.48 For Free Tier, use Always Free Eligible for the Shape options. Review the Networking settings. Aceept the default values provided by the wizard. The following is sample data. The actual values change over time and differ from data center to data center. Virtual cloud network: vcn-&lt;date&gt;-&lt;time&gt; Subnet: vcn-&lt;date&gt;-&lt;time&gt; Assign a public IPv4 address: Yes Review the Add SSH keys settings. Accept the default values provided by the wizard. Select the Generate a key pair for me option. Select both the Save Private Key and Save Public Key options to save the private and public SSH keys for this compute instance. If you want to use your own SSH keys, select one of the options to provide your public key. Put your private and public key files in a safe location. You cannot retrieve your SSH keys after the compute instance has been created. Review the Boot volume settings. Accept the default values provided by the wizard. Leave all check boxes unchecked. Select Create to create the instance. Provisioning the system might take several minutes. You have successfully created an Oracle Linux instance. &laquo; Back Continue &raquo; 5 Enable internet access The Create a VM Instance wizard automatically creates a VCN for your instance. You must manually add an ingress rule to your subnet to allow internet connections on port 8080. Create an ingress rule for your VCN Follow these steps to select your VCN’s public subnet and add the ingress rule. Open the navigation menu and select Networking. Select Virtual Cloud Networks. Select the VCN you created with your compute instance. With your new VCN displayed, select &lt;your-subnet-name&gt; subnet link. The public subnet information is displayed with the Security Lists at the bottom of the page. A link to the Default Security List for your VCN is displayed. Select the Default Security List link. The default Ingress Rules for your VCN are displayed. Select Add Ingress Rules. An Add Ingress Rules dialog is displayed. Fill in the ingress rule with the following information. Fill in the ingress rule as follows: Stateless: Checked Source Type: CIDR Source CIDR: 0.0.0.0/0 IP Protocol: TCP Source port range: (leave-blank) Destination Port Range: 8080 Description: Allow HTTP connections Select Add Ingress Rule. Now HTTP connections are allowed. Your VCN is configured for Spring Boot. You have successfully created an ingress rule that makes your instance available from the internet. &laquo; Back Continue &raquo; 6 Install and configure Spring Boot Before the Spring Boot application is ready to use, you first configure the instance you created previously and then install 3 software packages: Git, OpenJDK 8, and Maven 3.6. Before you begin the Spring Boot set up Configure the port for your instance Open the navigation menu and select Compute. Under Compute, select Instances. Select the link to the instance you created earlier. From the Instance Details page look under the Instance Access section. Save the public IP address the system created for you. You use this IP address to connect to your instance. Open a Terminal or Command Prompt window. Navigate to the directory where you stored the ssh encryption keys you created. Connect to your instance with the SSH command ssh -i &lt;your-private-key-file&gt; opc@&lt;x.x.x.x&gt; Since you identified your public key when you created the instance, this command logs you into your instance. You can now issue sudo commands to install and start your server. Enable an HTTP connection on port 8080. sudo firewall-cmd --permanent --add-port=8080/tcp sudo firewall-cmd --reload The firewall is configured for Spring Boot. Install Git Install Git v2 from the IUS Community Project. On the site, navigate to the current version of the Git core package and then download to a local ~/temp directory. Downloading the Git RPM looks similar to the following: cd mkdir temp cd ~/temp wget https://repo.ius.io/7/x8664/packages/g/git224-core-2.24.2-1.el7.ius.x8664.rpm Once the Git RPM download has completed, install the RPM. Install the RPM with yum. sudo yum install git224-core-2.24.2-1.el7.ius.x86_64.rpm Test for sucessful install. git --version If the installation was successful, git --version will echo something similar to the following: git version 2.24.2 Git is installed. Install OpenJDK 8 Install OpenJDK 8 using yum. sudo yum install java-1.8.0-openjdk-devel java -version Set JAVA_HOME in .bashrc. Update .bashrc: vi ~/.bashrc In the file, append the following text and save the file: # set JAVA_HOME export JAVAHOME=/etc/alternatives/javasdk Activate the preceding command in the current window. source ~/.bashrc Java is installed. Install Maven 3.6 Install Maven from an Apache mirror. Go to the main Maven site’s download page. Get the URL for the latest version and download with wget. Download the Maven zip file. For example: wget http://apache.mirrors.pair.com/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz Extract the program files. sudo tar xvfz apache-maven-3.6.3-bin.tar.gz Install the program files by moving the files to the /opt directory. sudo mv apache-maven-3.6.3 /opt/ Add the Maven path /opt/apache-maven-3.6.3/bin to your PATH environment variable and activate Maven by sourcing your .bashrc. vi ~/.bashrc Add export PATH=$PATH:/opt/apache-maven-3.6.3/bin and save. source ~/.bashrc Maven is ready to use. Build your Spring Boot application Follow these steps to set up your Spring Boot application: From your home directory, check out the Spring Boot Docker guide with Git: git clone http://github.com/spring-guides/gs-spring-boot-docker.git Navigate to the gs-spring-boot-docker/initial directory. Edit the Application.java file located in src/main/java/hello/. Update Application.java with the following: package hello; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; @SpringBootApplication @RestController public class Application { @RequestMapping public String home(){ return \"&lt;h1&gt;Spring Boot Hello World!&lt;/h1&gt;\"; } public static void main(String[] args) { SpringApplication.run(Application.class, args); } } Save the file. Use Maven to build the application. mvn package If the build is successful, Maven will echo: [INFO] BUILD SUCCESS Run the application. java -jar target/gs-spring-boot-docker-0.1.0.jar Test your application from the command line or a browser. From a new terminal, connect to your instance with your SSH keys and test with curl: curl -X GET http://localhost:8080 From your browser, connect to the public IP address assigned to your instance: http://&lt;x.x.x.x&gt;:8080. On either your instance or in your browser, you see Spring Boot Hello World! Congratulations! You have successfully created a Spring Boot application on your instance. &laquo; Back Continue &raquo; 7 What's Next You have successfully installed and deployed a Spring Boot application on Oracle Cloud Infrastructure using a Linux instance. To explore more information about development with Oracle products: Oracle Developers Portal Oracle Cloud Infrastructure &laquo; Back By ","categories": ["java","modernize"],
        "tags": ["oci","java","always-free","back-end","spring"],
        "url": "/tutorials/install-spring-boot-on-an-oracle-instance",
        "teaser": ""
      },{
        "title": "Java, GraalVM, and Helidon",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/use-cases/java",
        "teaser": ""
      },{
        "title": "Build Your Own Language Translator on Oracle Cloud",
        "excerpt":" こんにちは！ Have you always wanted to learn a new language and be introduced to a new and different culture? I have, and for the last six months, I have been using Duolingo to get an introduction to the Japanese language. Among the seven secrets recommended by TED translators in this blog post, the general consensus seems to be integrating the language in your daily lives. Background Since I started learning the language, I have been exposed to many new words. However, they remain in my memory, only as long as I keep using them in my daily conversations, or at very least, reading them on a regular basis. My wife has a habit of scribbling on pieces of paper, filling them with words that she had learned. Thinking that we could keep them tidy in one spot, we purchased a small whiteboard where she could write them in erasable ink. Not surprisingly, it filled up quickly and became harder each day to decide what to keep or erase, as our vocabulary increased. Flashcards are a commonly used tool to aid memorizing new concepts and terms and are a promising replacement. These days, there are many applications available on mobile devices and computers for creating flashcards. However, we wanted the words to be flashed constantly and the device needed to be placed in a common area that we frequented. That would be the kitchen! It didn’t make sense to devote a mobile device for a single purpose and they were probably too expensive in cost and power consumption. Furthermore, we would have had to invest in a special mount to hold the device in place, unobtrusively. A fridge magnet would have been perfect! As a gadget freak, I had acquired a wide range of electronic components over the years. More recently, I have been intrigued by a low-powered, small-form factor Microcontroller Unit (MCU), manufactured by Espressif. The chip I am referring to is the ESP32. Out of the box, the device comes with WiFi and Bluetooth support and only required minimal amount of power to run. To build one though, I would need to source the right parts and possibly custom 3D-print the enclosure like I did a few years ago when I created my own e-badge for the annual ODTUG Kscope conference. Fortunately, the clever folks at M5Stack came up with a 4.7 inch e-ink device that more than met my requirements. The M5Paper is powered by a variant of the ESP32 chip and comes with all the ingredients I needed for this project. As mentioned earlier, the ESP32 comes with WiFi support and a sizable amount of memory (for an embedded device), built-in magnet, a self-contained power supply, and a MicroSD card slot. As a bonus, I also had access to a built-in RTC (real-time clock), temperature and humidity sensor, buttons and a touch screen for interactivity and three expansion slots to further extend my application. This was the perfect device for my APEX Language Learner! Objective Before starting out to build the solution, I had in mind the following goals: Create an easy to use web application to manage a repository of Japanese characters, Romaji to aid pronunciation for an English speaker, and of course, the English translation of the word or sentence. Create a web service that returns a random word. Display the random word on the M5Paper. Here’s what it looks like upon completion: All that in a weekend of fun! Toolkit To achieve my goals, here are the ingredients I used: 01x M5Stack M5Paper ESP32 Development Kit 01x MicroSD card 01x Oracle Cloud Free Tier account 01x Oracle Always Free Autonomous Database (ADB) Oracle Application Express (APEX, pre-installed with the ADB) Oracle REST Data Services (ORDS, pre-installed with the ADB) Oracle Cloud Free Tier Resources The Oracle Cloud Free Tier account offers a suite of resources offered by Oracle at no charge and no time limits. Yes, you read that correctly! Inside the bag of free resources, you will find not one, but two Always Free Autonomous Databases! There are many great reads on how to get started setting up your Oracle Cloud account and creating your first ADB, including this piece by Todd Sharp. Luc Demanche and I had also written a comprehensive overview on what Always Free resources are available and some ideas on fully exploiting them. The book Getting Started with Oracle Cloud Free Tier is available for purchase at your favorite book store. Rapid Application Development with APEX As noted in the contents of the toolkit, the ADB already comes preinstalled with the powerful rapid application development frame work affectionately known as APEX. To get up and running quickly with APEX on the ADB, please check out this guide by Todd Bottger. To learn more about this low-code platform, please check out the resources available on this webpage. There you will find links to documentations, tutorials and books for getting started. As part of the package, the ADB also comes installed with ORDS, an enabler of web services that interface the web with the database using standard protocols. ORDS also comes with a feature called SQL Developer Web. This is a browser-based application for working with and managing the Oracle Database and if you are interested to learn more, please check out this blog post by (that) Jeff Smith. Content Management The first objective is to create a simple CRUD web application for us to enter Japanese characters that we wanted to memorize, the romanized form for easier pronunciation and the English translation. As the goal is to quickly build up the content and write the necessary software for displaying the words, I really did not want to fuss with choosing a front-end and back-end programming framework, implementing security features etc., just to get this rolling. APEX provides me a declarative approach to create a modern CRUD application that I can use on any device, with security and performance built right in the heart of the platform. CRUD Simplified To enter the words into the repository, I will only need a page listing all the Japanese words in the database and a form to enter or edit the words and their translations. There are different ways to start creating the application. One approach would require me to create the required database objects using SQL scripts and then walk through the steps of creating the application and web components. Or, I could simply start with a spreadsheet and let APEX do all the heavy lifting. In the spreadsheet, all I need to do is fill the three columns with starter data and the column names in the first row, and then save it as a CSV file. Next, from the APEX App Builder, click on the Create button to create a new application. Select the From a File option. We will use the spreadsheet as the starting point. Drag and drop the CSV file containing the seed data. Enter a suitable name for your data and then click the Configure button. In this modal dialog, you can make any customizations to the database table that you are about to create, for example, the data type, the column name etc. Click Save Changes to save any customizations that you make. When returned to the Load Data page, click the button Load Data to proceed with the import. The number of rows imported should tally with contents of the CSV file. If so, click Create Application for the final step. In the next screen, you will have the opportunity to fine tune some settings before the application is created. You will notice that APEX has already added a few starting pages like the Faceted Search (if you are keen to know more, I wrote a more in-depth discussion on this topic here) and Interactive Report page. You may also introduce addition prefabricated features and change the Authentication Scheme. When you are done, click the Create Application button and the APEX engine will generate the application as defined. Once the application has been created, you will be returned to the App Builder with the new application open. Simply click the Run Application button to launch the application. APEX has security built-in and you will be required to login in order to use the application. Both authentication and authorization rules can be defined declaratively and usually does not require much coding. For example, APEX supports OAuth2 and you may choose to use this by creating and specifying a new Authentication Scheme called Social Sign-In. In my case, I chose to use Google accounts with a simple authorization rule that checks my email address before allowing access. If you like to learn more about setting up OAuth2 please see these articles on working with Google and Azure AD. In under an hour, you should have a functional web application that lets you: List the words contained in the new table. Create, modify or delete entries. The default application user interface theme is also responsive and will be usable on mobile devices as well. Content Distribution Unlike a mobile device such as an Apple iPad or Android phone, the M5Paper does not come with a web browser for rendering HTML from a web application to display dynamic content. It has to be programmed at a low level to display the information that you require. Since the ESP32 has networking capabilities, we are able to design a solution that pulls the needed data from a web service. Oracle REST Data Services (ORDS) is a platform that would facilitate the creation of such web services. In the following steps below, you will learn how easy it is to create and publish a web service to be consumed by the embedded device. Start by accessing the RESTful Services module in APEX: On the left pane, select the Modules node and then when the page loads, click the Create Module button on the top-right: Enter the following details about the new module: Module Name: word Base Path: /words Leave the remaining fields with their default values and then click the Create Module button to continue. Once created, scroll down and then on the left, click the Create Template button: Provide the following details about the template: URI Template: random Leave the remaining fields with their default values and then click the Create Template button to continue. Scroll down again and then click the Create Handler button to create a GET handler for the template. Ensure the following options are selected: Method: GET Source Type: Collection Query In the Source field, enter the following SQL statement to perform a simple random selection of a single row: select * from ( select jpntext, romaji, engtext from japanese_word order by dbms_random.normal() ) fetch first 1 rows only IMPORTANT Omit the semi-colon at the end of the statement. Once the web service has been created, simply click the “Copy” button (underlined in red) to obtain the web service URL: If you have access to a Bash shell and both curl and json_pp are available, run the following command to test the web service: curl https://apeks.app/ords/lab/words/random | jsonpp -jsonopt utf8,pretty You should get an output similar to this: % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 347 0 347 0 0 896 0 --:--:-- --:--:-- --:--:-- 896 { \"count\" : 1, \"hasMore\" : false, \"items\" : [ { \"eng_text\" : \"Friday\", \"jpn_text\" : \"金曜日\", \"romaji\" : \"Kin'yōbi\" } ], \"limit\" : 25, \"links\" : [ { \"href\" : \"https://apeks.app/ords/lab/words/random\", \"rel\" : \"self\" }, { \"href\" : \"https://apeks.app/ords/lab/metadata-catalog/words/item\", \"rel\" : \"describedby\" }, { \"href\" : \"https://apeks.app/ords/lab/words/random\", \"rel\" : \"first\" } ], \"offset\" : 0 } Alternatively, test the web service by opening the URL in a web browser. Content Consumption and Display As introduced earlier in the article, I decided to use the M5Paper for the display device for the following reasons and features: The ESP32 chip used in the device is low-powered and has built-in WiFi support. It comes with a relatively large sized e-paper display that does not require power to continuously display the same content. The embedded RTC allows the device to power down into a “deep sleep” and then waking up on a predefined schedule. It is powered by a large capacity lithium polymer battery. The enclosure is sturdy and has magnets that I can use to attach to my fridge door. Has a MicroSD slot that allows easy upload of resource files. Multiple IO interfaces, buttons and an embedded temperature and humidity sensor for future extensions. Inexpensive (for what it offers) at a price of $69 USD. With the M5Paper, I have a choice of using either Arduino or MicroPython. UIFlow As with many ESP32-based products, the M5Paper supports both Arduino and MicroPython. M5Stack also provides their UIFlow firmware, which is based on MicroPython. It contains APIs that make working with compatible sensor products, and is easily programmable with their graphical Integrated Development Environment (IDE). Developers can choose to program the device using either the Blockly interface, or straight up MicroPython. At the time of writing, the alpha version of the IDE supports and is certainly a great option to consider. Like APEX, it is designed for rapid application development! While I like the “RADiness” of Blockly, I eventually decided to go with Arduino for two reasons. Firstly, the Arduino libraries appear to be more matured and better documented. The other reason was the ease of adding fonts to the application. As this is a multilingual application, I needed to use a font library that supported unicode characters and it was much easier and straightforward to do when programming with Arduino. Setup Begin by downloading the latest the Arduino IDE from their website and installing it on your computer. Besides writing code, the next two most commonly performed actions will involve these two buttons: The button with the tick mark on the left is to compile and verify the code, while the button on the right with the right pointing arrow, both compiles and uploads the code to the connected device. However, before you can upload the code, the following configuration is required. First, we need to add the board descriptions and required libraries to the Arduino IDE. Under the File menu, select the Preferences item, and then click the button next to the text field labelled Additional Boards Manager URLS. Enter the URL https://m5stack.oss-cn-shenzhen.aliyuncs.com/resource/arduino/packagem5stackindex.json and then click OK. Return to the main window and then go the Boards Manager utility. You will find this under the Tools menu and when you place your mouse cursor over the currently selected Board, you will see an additional item called Board Manager. Launch the Board Manager and then search the term “m5stack”. Install the M5Stack board by M5Stack official. To work with JSON, we will also need a third-party library called ArduinoJson. We can install this using the Library Manager that can be accessed from under the Tools menu (the item is actually called Manage Libraries…). Finally, select the M5Paper board. The preset parameter values should work, but you must select the communications port that is assigned to the device when connected via USB. Note For Windows, it may be necessary to install additional device drivers to successfully connect the device to your PC. The code snippets in the remaining sections of this article are intended to highlight key points in the solution and should be read in the context of a larger piece of code. The complete source code I used for this project is published here. Initialization We begin by including the M5EPD library in the code. This library’s code base is open sourced and is published in this GitHub repository. The company has also published the API documentation here for reference. The code below then performs some basic initialization steps: #include &lt;M5EPD.h&gt; M5EPD_Canvas canvas(&amp;M5.EPD); // create the canvas void setup() { M5.begin(); // Initialize the device M5.RTC.begin(); // Initialize the RTC M5.TP.SetRotation(0); // Set the orientation of the touch panel M5.EPD.SetRotation(0); // Set the orientation of the display M5.EPD.Clear(true); // Clears the screen content ... } Internet Access The ESP32 is Internet-ready, but to get connected, we will need to setup and initialize the WiFi connection. M5Stack provides the required library, so simply include that in your application and the following code for establishing the connection. #include &lt;WiFi.h&gt; const char SSID[] = \"mywifissid\"; const char WLAN_PASSWORD[] = \"supersecret\"; void setup() {} WiFi.begin(SSID, WLAN_PASSWORD); while (WiFi.status() != WL_CONNECTED) { delay(500); Serial.print(\".\"); // print dots to the serial monitor to know that we are still waiting for a connection. } displayFooter(\"WiFi connected.\"); ... } Retrieve the Data Once connected, we will then require the ability to make the REST call using a suitable client that supports the HTTP protocol. Fortunately, the M5Stack boards also provide this dependency. Include the HTTPClient library (note the upper-cased “HTTP”) and the third-party ArduinoJson library that we had installed earlier. #include &lt;HTTPClient.h&gt; include &lt;ArduinoJson.h&gt; HTTPClient http; String payload; setup() { http.begin(\"https://apeks.app/ords/lab/words/random\"); int httpCode = http.GET(); // httpCode will be negative on error if(httpCode &gt; 0) { if(httpCode == HTTPCODEOK) { payload = http.getString(); // get the HTTP response payloadAvailable = true; Serial.println(payload); // for debugging displayFooter(\"Data received via Oracle REST Data Services.\"); } } else { displayFooter(\"Failed to load data.\"); Serial.printf(\"[HTTP] GET... failed, error: %s\\n\", http.errorToString(httpCode).c_str()); } http.end(); // If the REST call was successful, parse the JSON content if(payloadAvailable) { DynamicJsonDocument doc(2048); deserializeJson(doc, payload); // Only one row should have been returned by the REST call. String jpnText = doc\"items\"[\"jpn_text\"]; String romaji = doc\"items\"[\"romaji\"]; String engText = doc\"items\"[\"eng_text\"]; displayFooter(\"Data entered via Oracle Application Express parsed and loaded.\"); displayContent(jpnText, romaji, engText); } ... } Display Words and Translations With the backend interface done, let’s now focus on the fun part, displaying the data! To display the Japanese characters, I needed a font that supports the appropriate code page. M5Stack has an example code that demonstrates the use of truetype fonts in M5Paper. The code demonstrates how to load and use the Gensen font. For convenience, I loaded the required .ttf (truetype font) file, which you can find in the example code, into the root partition of a freshly formatted MicroSD card with 256 MB storage capacity and loaded it into the single available slot. The code below then loads the font and creates the canvas: void setup() { canvas.loadFont(\"/GenSenRounded-R.ttf\", SD); // Load the font from the MicroSD card. canvas.createCanvas(960, 540); // Create the canvas with the maximum dimension of the e-paper in landscape orientation. ... } The code to render the display are divided into two functions, displayContent and displayFooter. There are three lines of text for rendering the Japanese characters, Romaji and English translations respectively. Text in all three lines are center-aligned, while the single line in the footer are left-aligned. void displayContent(String jpnText, String romaji, String engText) { canvas.fillCanvas(0); uint16_t x = (960 - canvas.textWidth(jpnText)) / 2; uint16_t y = 100; canvas.createRender(JPNFONTSIZE, 256); canvas.setTextColor(15); canvas.setTextSize(JPNFONTSIZE); canvas.setTextDatum(TC_DATUM); canvas.drawString(jpnText, x, y); canvas.destoryRender(0); ... } void displayFooter(String text) { uint16_t x = 20; uint16t y = SCREENHEIGHT - FOOTERBARHEIGHT / 2; canvas.fillRect(0, SCREENHEIGHT - FOOTERBARHEIGHT, SCREENWIDTH, FOOTERBARHEIGHT, 15); canvas.createRender(24, 256); canvas.setTextSize(24); canvas.setTextColor(0); canvas.setTextDatum(CL_DATUM); canvas.drawString(text, x, y); canvas.destoryRender(0); canvas.pushCanvas(0, 0, UPDATEMODEDU); } There sixteen shades of grey available for use on the e-paper display, beginning with 0 for the lightest shade “white”, and 15 as the maximum for black. Thus the main content is rendered with text in black on a white background, and the footer is in an inverted color scheme. Saving Power It is evident that I am a huge fan of e-paper display technology. Its major advantage is that it can hold the displayed image even after it is powered down. For the electronic flashcard, I did not require real-time display of data or any interactivity with the buttons. I could therefore benefit from putting the device into a deep sleep after it has obtained and rendered the information on screen. This allows the device to run for a pretty long time before needing a recharge. The code snippets below demonstrate how to power down the device and have it wake automatically after a predefined number of seconds. const uint16t WAKEINTERVAL_SECONDS = 900; // 15 minutes const uint16t SHUTDOWNDELAY_SECONDS = 1; ... void loop() { delay(SHUTDOWNDELAYSECONDS * 1000); // wait to allow refresh to complete before shutdown M5.shutdown(WAKEINTERVALSECONDS - SHUTDOWNDELAYSECONDS); } The M5.shutdown function is overloaded and provides developers with four different ways to shutdown and wake the device. Check out the API documentation for additional information. Compiling and Debugging The final step is to compile the code and then uploading it to the M5Paper. To do this, ensure that the correct board and port are selected, and the device is plugged into an available USB port on your computer. If you need to debug the code, then be sure to open the Serial Monitor found under the Tools menu. Any calls like Serial.println would print the outputs to the Serial Monitor. When you are ready, click the Upload button or Ctrl-U and watch the outputs at the bottom of the IDE for any potential issues. If all goes well, the device will startup and perform the tasks you have programmed it to do. Summary I am a fan of both APEX and M5Stack products for very similar reasons. They are easy to understand, develop and deploy a usable product in a short amount of time, and yet, allows for low-level extensibility should the need arises. “It’s not about our product, our company, our brand. It’s not about how the user feels about us. It’s about how the user feels about himself[/herself], in the context of whatever it is our product, service, cause helps him[/her] do and be.” Kathy Sierra Badass: Making Users Awesome I hope this simple but useful project will inspire you to build your own APEX applications with an IoT extension that interacts with the real world. 読んでくれてありがとう！ By Adrian Png","categories": ["personal"],
        "tags": ["oci","iot","apex","always-free","architect","community"],
        "url": "/tutorials/learning-languages-with-oracle-cloud",
        "teaser": ""
      },{
        "title": "League of Legends Optimizer using Oracle Cloud Infrastructure—Data Extraction and Processing",
        "excerpt":"Recap and Introduction Welcome to the second article on the League of Legends Optimizer series! In this article, we’ll continue where we left off in the first article. As a reminder, let’s review what we covered last time last time: We defined and modelled our problem, understanding the different steps in the drafting phase of the game We explored the various endpoints offered by Riot Games in their official API We periodically extracted as many games as possible for each player, storing the Match ID’s in a specific collection in our database for further processing. We pulled data from the most skilled players around the world and built a data set of these players, which left us with a structure like this in our non-relational autonomous database: { \"tier\": \"CHALLENGER\", \"veteran\": false, \"wins\": 229, \"request_region\": \"br1\", \"rank\": \"I\", \"inactive\": false, \"summonerId\": \"dKMYAJhqPpBuI9hSfIon_a4zSbtCwTFep-DA6Lq9YwqlIQ\", \"hotStreak\": true, \"queue\": \"RANKEDSOLO5x5\", \"losses\": 198, \"freshBlood\": true, \"puuid\": \"aRjqIYDtBMBU2j-8EfHY6dJ0RZ9TqXgWLeNvpcjRWlCBaP8HGBAWFRAiehRM4Jo-lgJXXrjTCOcIKg\", \"summonerName\": \"Qats\", \"leaguePoints\": 922 } Code Optimization Before building a dataset with match information, there are some things to consider: The player and match datasets are ever-growing and will continue to expand in size and complexity if we keep executing our code The functions gettopplayer() and get_puuid() can be more thoroughly optimized For this second point, I propose a code revamp and bugfix before resuming the ML pipeline work. It’s important to keep optimizing code even when we think it won’t matter, because in the end we might need to do it anyway. It’s much better to have the ideas fresh rather than waiting for the right moment to do these optimizations and forgetting how to optimize the code at all. Optimizing API call efficiency Our constantly-growing dataset made me realize there must be a way to retrieve skilled players without constantly requesting their PUUIDs. In fact, we should only retrieve the PUUID for a player if this player isn’t already present in the database. By definition, the PUUID will never change even when summoners change their display/in-game names. The first order of optimization is to check whether players are already in our DB collection before requesting their PUUID. This will save us precious API calls which will allow our program to focus on processing, and less on API restrictions and rate limits. # Insert the users. \tfor x in totalusersto_insert: \t\tx['request_region'] = region \t\tx['queue'] = queue \t\ttry: \t\t\tqbe = {'summonerId':x['summonerId']} \t\t\tif len(collection_summoner.find().filter(qbe).getDocuments()) == 0: \t\t\t\t# In case they don't exist in the DB, we get their PUUIDs, in case they change their name. \t\t\t\toverallregion, tagline = determineoverall_region(region) \t\t\t\tx['puuid'] = getpuuid(overallregion, x['summonerName'], tagline, connection) \t\t\t\tcollection_summoner.insertOne(x) \t\t\t\tprint('Inserted new summoner: {} in region {}, queue {}'.format(x['summonerName'], region, queue)) \t\t\telse: \t\t\t\tprint('Summoner {} already inserted'.format(x['summonerName'])) \t\t\t\tcontinue \t\texcept cx_Oracle.IntegrityError: \t\t\tprint('Summoner {} already inserted'.format(x['summonerName'])) \t\t\tcontinue As shown, we’ll check for new players in our collection before inserting them. In case they aren’t present, we’ll request their PUUIDs and their respective information, and proceed to insert them. Additionally, I also implemented a SODA database method that will remove faulty summoners (see deletejsondb()). In case a summoner can’t be found given their specific username for a region, we’ll remove it from our DB since all subsequent requests (asking the Riot Games API for their games) will always result in errors: # Remove a document from a collection based on the key and value pairs provided. def deletejsondb(connection, collectionname, oncolumn, on_value): \tsoda = connection.getSodaDatabase() \txcollection = soda.createCollection(collectionname) \tqbe = {oncolumn: onvalue} \tx_collection.find().filter(qbe).remove() When calling get_puuid(), we’ll check for the HTTPs response status code before doing anything: # Inside get_puuid() if response.status_code == 200: \t\t#print('Printing response for user {} - region {}: -----\\n{}'.format(summoner_name, region, response.json())) \t\tpass \telif response.status_code == 404: \t\tprint('PUUID not found for summoner {}'.format(summoner_name)) \t\tdeletejsondb(connection, 'summoner', 'summonerName', summoner_name) \telse: \t\tprint('Request error (@getpuuid). HTTP code {}'.format(response.statuscode)) \t\treturn These two modifications may seem insignificant, but they improved code efficiency by up to 80%, depending on the number of already-existing players in the summoner collection. Querying the API for new summoners was much faster, since duplicate values would be automatically ignored, and I’d only have to wait for the new ones’ PUUIDs. Reducing Redundant Processing Finally, as our last optimization, we’ll add new keys in our document, called processed1v1 and processed5v5, which will indicate whether a match has already been processed for the 1v1 and 5v5 models or not (more on this in the next sections). In case this match has been processed, we’ll keep it in our dataset, but won’t extract the contents of the match from now on. This will reduce overloading the CPU and processing times of our Python code, since this data mining process has been programmed to take into consideration all values present in the database (as any data mining process should). Considering this previous collection structure: { \"matchid\": \"BR12133968346\" } We’ll expand this into (example with a non-processed match): { \"matchid\": \"BR12133968346\", \"processed_1v1\": 0, \"processed_5v5\": 0 } Subsequently, in our data_mine() function, we’ll only process those matches which haven’t been processed. After processing, we’ll change their processed bit in order not to make redundant processing and API calls: allmatchids = collectionmatch.find().filter({'processed1v1': {\"$ne\":1}}).getDocuments() After processing our 1v1 models, in the upcoming articles we’ll filter the same way for the other processed bit: allmatchids = collectionmatch.find().filter({'processed5v5': {\"$ne\":1}}).getDocuments() Extracting Game Data Since we have a great collection of games, and our code has already survived a code optimization iteration, we’ll get straight into the data provided by Riot’s API about matches. The preliminary structure of data that we can process is too large to paste here, but there’s more than enough information. The following JSON structure outlines the most important variables that we might need. { \"metadata\": { \"dataVersion\": \"2\", \"matchId\": \"BR1_2133968346\", \"participants\": [ \"MAwy3tdjisAuet11VKKSM-WhzljY1onACC9rLpriEoWoAKGB8rNampDsoUy-1KBfYDCEAJcMqO33MQ\", \"zs2vbZ7MtlgtuSY1HaLPytK5mr07nGUXkXXQhPxIJ2uff07VNaRKJk_f-uxkDEIJGRf9Weg9y4bm3w\", \"GI68lPlGrXbLIZ_o536EFmf1FyWY9mjqEmeEeg6NHiKcUboLZpsKaXL5TvCa1aBBWQgE4c6Y2EzMbQ\", \"RznuEthBrn_aO6q4RWYwX-bp96BBcVVSccBsMUxRGMJsSrSiVR5cwpgqqpAE0krz5MzqGFbNVdtxXA\", \"x73jcQmLJozEJETZWF17Xz-8tqS7zUyT4vY2ctcYkz9vGmbM5sgNkPJmJE8U9W-9DSRce43cUn1yvg\", \"KbDKfIXKhdXrzDKyFdmSpLFfIHIiOYHIVqr0TFpHY4xIaoUUIH8nd0IgZV5C1E28c2vRLYLdU6uwRQ\", \"bEXzoqxnVFYL2-NwyHvv-u3xojRcP-cf0INO8J_l_MVljXW-dA5xcXgBWnWmpFvG1YIMAFKDJcXg\", \"iV-M2lcWZ7cqLKKC4rrDy6TLx_W2QUIGRuqSyYzjW0zj8Ku6pwncIZctSqjFlUdpUErT7dB3muiuxQ\", \"PpcjIHRykKFBBiCVsx0f1YMnbQAL6It8TtBkORMFBuScePoB3s8pYX-kPZpgkmD0HVW65lFm7N3kGg\", \"p7GibwRhHT1jmmw7vXSVVSck2FDjBnxm_MyVlSL8Tko58qPHH9gy6wEwthuPbf98Zrn1J9qDhX4nPw\" ] }, \"info\": { \"gameDuration\": 1739425, \"gameId\": 2133968346, \"gameVersion\": \"10.25.348.1797\", \"participants\": [\"a lot of information on individual scores, summoners, etc. From here we will just extract the championName for each player\"], \"platformId\": \"BR1\", \"teams\": [ { \"bans\": [ { \"championId\": 25, \"pickTurn\": 1 }, { \"championId\": 121, \"pickTurn\": 2 }, { \t\"this goes on for all pick turns\": 1 } ], \"objectives\": { \"baron\": { \"first\": false, \"kills\": 1 }, \"champion\": { \"first\": false, \"kills\": 31 }, \"dragon\": { \"first\": false, \"kills\": 0 }, \"inhibitor\": { \"first\": false, \"kills\": 0 }, \"riftHerald\": { \"first\": true, \"kills\": 1 }, \"tower\": { \"first\": true, \"kills\": 2 } }, \"teamId\": 100, \"win\": false }, {\"the same for the other team\": 1} ] } } Note that the championId variable is represented by a number. In case you want to make some additional processing and extract this information, I have developed a Python script which uses the official Riot Games’ Data Dragon, which allows us to determine which champion ID corresponds to which champion name. From this amount of data, which is a lot, and according to the problem statement we discussed in Article One, we need at least the following information: Patch number, since team compositions, and some aspects of the game change every patch due to changes in champions’ stats Team 1 composition Team 2 composition Win Team 1 / Win Team 2 binary variable Whether each team did First Blood, First Dragon, Baron, Tower, Herald and Inhibitor. These are variables we would like to consider for more complex models, and we might do that. Modelling the Matchup Predictor To create our first predictor and to test out the different possibilities of our models, we’ll build a structure that allows us to predict individual lane matchup results. Considering the JSON structure present in last section, we’ll now create one of its many derivatives, with the following structure: { \"pmatchid\": \"BR12133967391jungle\", \"data\": [ { \"goldEarned\": 9414, \"totalMinionsKilled\": 62, \"win\": false, \"kills\": 1, \"assists\": 7, \"deaths\": 8, \"champion\": \"Graves\", \"visionScore\": 13, \"puuid\": \"hRMN6-jyVZAm7nhzYioSiQxy4WfWqhaV7ozDjQZ9OMoE6HSv870_UAtVv6ybRilZGpIdzrz9VmqN-g\", \"totalDamageDealtToChampions\": 12217, \"summonerName\": \"divespeiro\" }, { \"goldEarned\": 10119, \"totalMinionsKilled\": 31, \"win\": true, \"kills\": 3, \"assists\": 10, \"deaths\": 3, \"champion\": \"Diana\", \"visionScore\": 22, \"puuid\": \"UdbGG79WCOn0DkjLv1-dpBZaVxIf3Nt-p9_IdEz5RwCwmj-wOHXSPeserm4uJz5v-RUZnzgleUqgQg\", \"totalDamageDealtToChampions\": 9178, \"summonerName\": \"soza\" } ], \"gameVersion\": \"10.25.348.1797\" } We’ll store this information inside a new collection called matchups. It will only include two players (one from each enemy team) and the API allows us to check in which lane these players played in. For each game (excluding games with AFKs - Away From Keyboard, meaning players that never connected into the game) we’ll have five different matchups (toplane, midlane, jungle lane, physical damage lane, and support lane). If we create this kind of structure, we’ll be able to check the best competitors for any lane and champion, which will help us make individual predictions and increase the chances of victory in the end. Note that the data available for one specific match will only be able to accurately predict results for a specific patch, since a champion may be optimal for patch 10.25 but really bad on the next one. That’s why we also store the matchup game version in our collection structure. Statistics Using an auxiliary file called findcounts.py, we can find the number of elements we have in each one of our collections. In my case, having executed the data extraction code for several iterations, and processing matchups, I find myself with the following data: Collection match has 1193746 documents Collection matchups has 1084470 documents Collection summoner has 31072 documents Collection match has 1013284 documents left to process From 31000 players, we have extracted about 1.2 million matches, and from these matches, we have processed only about 200.000 of them (since the matchups collection has an average of five documents per match). It looks like we won’t need to download any more new games before processing the remaining matches. We’ll keep executing the code to increase the dataset for creating the model in the next article. Next Steps After creating this data structure, we prepare for the next article where we’ll do a deep dive into the training of the model for 1v1 matchups. We’ll use TensorFlow as the main framework for ML operations and explore PyTorch, making an introduction to two of the most notable ML libraries available for Python. Hoping to see you in the next article where we’ll start training our 1v1 prediction model. How can I get started on OCI? Remember that you can always sign up for free with OCI! Your Oracle Cloud account provides a number of Always Free services and a Free Trial with US$300 of free credit to use on all eligible OCI services for up to 30 days. These Always Free services are available for an unlimited period of time. The Free Trial services may be used until your US$300 of free credits are consumed or the 30 days has expired, whichever comes first. You can sign up here for free. License Copyright (c) 2021 Oracle and/or its affiliates. Licensed under the Universal Permissive License (UPL), Version 1.0. See LICENSE for more details. Written by jasperan, edited by Victor Agreda Jr. By Ignacio Martínez","categories": ["games"],
        "tags": ["analytics","gaming"],
        "url": "/tutorials/lol-article-2",
        "teaser": ""
      },{
        "title": "League of Legends Optimizer using Oracle Cloud Infrastructure—Building an Adversarial League of Legends AI Model",
        "excerpt":"Recap and Introduction Welcome to the third article of the League of Legends Optimizer series! In this article, we’re diving deep into building a classifier model to predict the winner of two champion matchups withs Oracle Cloud Infrastructure (OCI). In previous articles, we’ve done the following: Defined and modelled our problem, understanding the different steps in the drafting phase of the game Explored the various endpoints offered by Riot Games in their official API Pulled data from the most skilled players around the world and built a data set of these players, which left us with a structure like this in our non-relational autonomous database Created data structures, such as the matchup structure, to represent the data we pulled from the API in an adversarial way: in this data structure (see this dataset), we faced each lane in a game against the enemy’s, and determined whether this player won or lost the game. Following this data structure, we’re going to make a reliable model that can predict the best champion to pick against another player using by using machine learning. The Data Structure From the Kaggle dataset, we see an example of the data structure we’re going to use to build our model: { \"pmatchid\": \"BR12133948485bottom\", \"data\": [ { \"goldEarned\": 10767, \"totalMinionsKilled\": 161, \"win\": false, \"kills\": 6, \"assists\": 8, \"deaths\": 6, \"champion\": \"Kayle\", \"visionScore\": 14, \"puuid\": \"s1j7_icmqQCl1vROjASKJLSGZmktnvcrt8Qm7g39T16YdxE-xTlX2nnrG400bMae7O3JWyf2Y4XX4Q\", \"totalDamageDealtToChampions\": 13008, \"summonerName\": \"EveBy\" }, { \"goldEarned\": 14787, \"totalMinionsKilled\": 172, \"win\": true, \"kills\": 14, \"assists\": 2, \"deaths\": 6, \"champion\": \"Kaisa\", \"visionScore\": 12, \"puuid\": \"zjBoj6G9dWbPgkKSvZpDIcDA2NG65M1FUOxlYCXUyff9I1GR_xIuOFLWXlzMjWV67gOnGFC7g6wCuw\", \"totalDamageDealtToChampions\": 23071, \"summonerName\": \"Goiasinho\" } ], \"gameVersion\": \"10.25.348.1797\" } The intricacies of how we built the data structure and derived the result from it are explained in the previous article. It is important to remember that structuring and manipulating data in the data science process takes an average of 80 to 90% of the time, according to expert sources (image courtesy of “2020 State of Data Science: Moving From Hype Toward Maturity.”), and we shouldn’t be discouraged when spending most of our time processing and manipulating data structures. The ML algorithm is the easy part if you’ve correctly identified the correct data structure and adapted it to the structure ML algorithms expect. For our first model, we’re going to simplify the present data structure even more and get something like this: { \"matchid\": \"EUN12910807891_utility\", \"champ1\": \"Velkoz\", \"champ2\": \"Yuumi\", \"win\": 1 } Where win is a boolean variable that represents whether champ1 won or not. So, in this example, Velkoz won the game. If we analyze this data structure, we see that it’s very simplistic and only contains three useful variables (having excluded the identifier variable, which is only there to ensure we don’t have duplicate values in our JSON database). One of these variables is actually the result of the game and the feature that we’d like to predict. This can have two different implications: The model is simple and works because the problem is simple. I suggest always trying this out regardless of the prediction problem and checking if the model is actually able to make accurate predictions. The model doesn’t work because we oversimplified the problem and there are many more variables to consider to make improvements on model accuracy. This is most likely the case in many real-world problems, in which I also include our League of Legends Optimizer. So, probably, this initial ML model will not have great predictions. Nonetheless, we’ll try anyway with what we currently have. The Code We begin with simple data exploration of our initial dataset. import pandas as pd pd.setoption('floatformat', ''.format) import os import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression df = pd.readjson('datalocation.json') # we've stored the data file locally in this case df.head(5) match_id champ1 champ2 win EUN12910807891utility Velkoz Yuumi 1 EUN12910807891jungle Shaco Nidalee 1 EUN12909987530top Riven Sett 0 EUN12909987530middle Lissandra Kassadin 0 EUN12909987530bottom Ashe Ezreal 0 In this simple model, there is almost no need for data exploration since we fully understand what each variable means, as they are by-products of our initial data structure. We split our data into train-test sets: # We want to predict the 'win' variable. trainfeatures = traindataset.copy() testfeatures = testdataset.copy() trainlabels = trainfeatures.pop('win') # returns column 'win' testlabels = testfeatures.pop('win') # returns column 'win' We encode the data following the Data Science process: le = LabelEncoder() le = le.fit(champ_list) # fit the label encoder with the whole champion list. trainfeatures = trainfeatures.apply(lambda x: le.transform(x)) testfeatures = testfeatures.apply(lambda x: le.transform(x)) Note that the champ_list referenced in this code block is the list of all unique champions in LoL. We need to fit our label encoder with all possible values. Otherwise, new values will not be properly encoded or may be encoded as a duplicate number. # Normalization scaler = StandardScaler() trainfeatures = scaler.fittransform(train_features) testfeatures = scaler.transform(testfeatures) After scaling the data we can fit our model: logreg = LogisticRegression() logreg.fit(trainfeatures, trainlabels) print('Accuracy of Logistic regression classifier on training set: ' .format(logreg.score(trainfeatures, trainlabels))) print('Accuracy of Logistic regression classifier on test set: ' .format(logreg.score(testfeatures, testlabels))) The accuracies obtained for the logistic regression classifier are 0.51. This is like tossing a coin. We could be making better assumptions by having a bit of knowledge about the game and champion performances. So our hypothesis saying that this simplistic model would not work correctly is correct. We need to improve our model or add variables to it. However, we can still make predictions using our model. The code to make a prediction needs to consider new data, encode it and scale it, and then make a prediction: new_data = { 'champ1': ['Xayah', 'Karma', 'Xerath', 'Gragas', 'Chogath'], 'champ2': ['Tristana', 'Lulu', 'Syndra', 'Sejuani', 'Gnar'] } newdf = pd.DataFrame(newdata) In this case, as our model is very limited in regards to input variables, we make the following assumption: we’ll calculate a team winning by taking the mode of all lanes’ predictions. Let’s transform our data: newdf = newdf.apply(lambda x: le.transform(x)) new_df.tail(5) champ1 champ2 143 126 52 69 144 119 36 104 19 35 As we can see, our champion input variables have been properly one-hot encoded. As these are the two only variables we have for our model, applying a standard scaler will not make a difference, since all variables have a standard deviation of 1 between each other (all champions are translated into distinct integer numbers). If we make the prediction: result = logreg.predict(new_df) def find_winner(lst): return max(set(lst), key=lst.count) winnerprediction = findwinner(result.tolist()) [0 0 0 0 0] The results of the prediction indicate the predicted winning team in each case. In this case, it’s predicting champ2 to win in all five cases. This can be a coincidence or not, but it happens to be the correct prediction. In case of having discrepancies, we’d use the find_winner() function to find the mode of the prediction. Adding this to the ML model actually improves the implicit accuracy of our code, but not of the model itself: we’re simply combining a 51% accuracy ML model with additional statistics to make a better prediction. We may ask ourselves how we can measure the accuracy of our combined ML-statistics model The problem is we can’t, since we have no programming framework able to assist us with this. We’d have to code our own object-oriented programming framework that extends the functionalities of the current Pandas framework, for example. And the time required to do so greatly exceeds the expected results. It’s better we focus our resources, as data scientists, to using the frameworks available to us with our structured data, and finding a better model by improving the quality of our input data. No need to reinvent the wheel. Finally, to see the results in a human-readable way, we need to apply the inverse_transform() function to our still-encoded data: inverseprediction = newdf.apply(lambda x: le.inverse_transform(x)) # we apply inverse transform if winner_prediction == 1: print('Predicted winner is team 1: \\n{}'.format(str(inverse_prediction['champ1']))) else: print('Predicted winner is team 2: \\n{}'.format(str(inverse_prediction['champ2']))) Now, we can see one prediction per case, totaling 5 cases, and one final team prediction using our find_winner(): Predicted winner is team 2: Tristana Lulu Syndra Sejuani Gnar Improving the model As we’ve seen in practice, the accuracy of our model is not as good as it could be. We can improve it by adding more variables to our model. We’re going to create a model that considers all variables in our matchup data structure, and reduce the complexity of our ML code by using AutoML open-source tools for data exploration and model training. from pandas_profiling import ProfileReport df = pd.read_csv('matchups.csv') report = ProfileReport(df) report This simple code generates a dynamic report that shows the data types, missing values, and other information about the data. We explore the Pearson’s r correlation coefficient between the variables: We proceed to train our model with all variables, taking into consideration that most of the variables in our model are highly correlated. This is especially true for the amount of gold earned with respect to the number of kills and minions killed (which makes sense, as these are two of the actions that give out the most gold in-game). We also see that the vision score highly correlates with the amount of assists a player makes in a game. from autogluon.tabular import TabularPredictor, TabularDataset # train-test split df = TabularDataset('matchups.csv') train = df.sample(frac=0.8, random_state=200) test = df.drop(train.index) # a simple look into our data df.head(2) PMATCHID GOLDEARNED TOTALMINIONSKILLED WIN KILLS ASSISTS DEATHS CHAMPION VISIONSCORE PUUID TOTALDAMAGEDEALTTOCHAMPIONS SUMMONERNAME GAMEVERSION BR12309470512jungle 7670 37 False 4 2 7 Graves 23 b1ZVlTG630NWh8Hgc7H--SErq3E3OkV50XSBuzuzkIuA… 11215 tired blessed 11.14.385.9967 EUN12809958230top 11108 202 False 1 9 8 Gwen 28 19ii6j4OOWmkUaw_yAXhMOhcgUvZaK8M1yVT0I3HwBYQka… 17617 ozzyDD 11.8.370.4668 We determine our predicting feature and fit the model: label = 'WIN' savepath = './trainedmodels' # specifies folder to store trained models predictor = TabularPredictor(label=label, path=save_path).fit(train) We can now make predictions on our test data: y_test = test[label] # predict 'WIN' testdatanolabel = test.drop(columns=[label]) testdatanolabel.head() predictor = TabularPredictor.load(save_path) ypred = predictor.predict(testdata_nolabel) print(\"Predictions: \\n\", y_pred) perf = predictor.evaluatepredictions(ytrue=ytest, ypred=ypred, auxiliarymetrics=True) We can see some sample predictions by our model on the test data. row_id win 2 False 4 False 8 False 13 False 21 True And with all trained models, we create a leaderboard with descending accuracy: predictor.leaderboard(test, silent=True) model score_test score_val predtimetest predtimeval fit_time predtimetest_marginal predtimeval_marginal fittimemarginal stack_level can_infer fit_order NeuralNetMXNet 0.836975 0.836461 33.964055 2.709942 6597.902246 33.964055 2.709942 6597.902246 1 True 12 NeuralNetFastAI 0.835870 0.839318 5.002273 0.199540 823.687658 5.002273 0.199540 823.687658 1 True 10 LightGBMXT 0.835717 0.833317 12.957499 0.471436 82.666493 12.957499 0.471436 82.666493 1 True 3 LightGBMLarge 0.835348 0.831603 22.367250 1.271694 136.043589 22.367250 1.271694 136.043589 1 True 13 WeightedEnsemble_L2 0.833629 0.966949 53.280581 4.445315 7548.607457 0.009523 0.044953 8.442531 2 True 14 LightGBM 0.832460 0.829127 5.237847 0.278245 57.027379 5.237847 0.278245 57.027379 1 True 4 RandomForestEntr 0.824462 0.822840 21.206490 0.434653 396.571945 21.206490 0.434653 396.571945 1 True 6 RandomForestGini 0.823243 0.821697 29.693281 0.436249 256.947097 29.693281 0.436249 256.947097 1 True 5 XGBoost 0.823159 0.823602 2.646490 0.696468 27.096065 2.646490 0.696468 27.096065 1 True 11 ExtraTreesGini 0.817348 0.817602 22.973135 0.235120 40.649245 22.973135 0.235120 40.649245 1 True 8 ExtraTreesEntr 0.817314 0.817316 10.118013 0.233918 47.698864 10.118013 0.233918 47.698864 1 True 9 CatBoost 0.789888 0.956377 1.347230 1.019444 35.908529 1.347230 1.019444 35.908529 1 True 7 KNeighborsUnif 0.637996 0.641109 3.568800 0.234611 5.264223 3.568800 0.234611 5.264223 1 True 1 KNeighborsDist 0.637668 0.640442 3.253163 0.237193 5.326229 3.253163 0.237193 5.326229 1 True 2 As we can see, including more variables in the model greatly improved the accuracy and reduced MAE and MSE of our model. We can also see that the model is able to predict the outcome of the game in the test data given the features in our data structure. This proves that a simple model is not always the best solution. We can achieve better results by using more advanced models, in this case about 83% accuracy, which is pretty good for a real-world problem. Also, note that we don’t really care how the models are trained as long as they make good predictions. Of course, it’s important to know the basics of ML to see how data is structured, but I’d like you, as a reader, to finish reading this article and remember that the hardest part about data science and data engineering is not coding the ML model, but understanding the data and the problem, and structuring the data accordingly to satisfy our needs. The Problem and Next Steps In short, in the first model, we didn’t consider enough variables. The results of this ML model were no better than using simple statistics, and heavily relied on additional statistics support to make a bit more sense. After expanding the model further, we saw that the model could make predictions much more accurately. However, we needed to ask ourselves if this model is useful. Are we, as players of League of Legends, able to have this amount of data in the middle of a game? The answer is no. We’re just given simple statistics like the gold from the team and KDA ratio. Only programatically through the API do we have the possibility to access all this data. So, while the model is pretty good, it doesn’t have a practical side that we can use and take advantage of. This is what we’ll explore in the fourth article in this series: integrating such a model (or a similar one) with data that we can actually use in real-time to make accurate predictions; data aligned with what players have at hand. Stay tuned for article 4! How can I get started on OCI? Remember that you can always sign up for free with OCI! Your Oracle Cloud account provides a number of Always Free services and a Free Trial with US$300 of free credit to use on all eligible OCI services for up to 30 days. These Always Free services are available for an unlimited period of time. The Free Trial services may be used until your US$300 of free credits are consumed or the 30 days has expired, whichever comes first. You can sign up here for free. License Written by Ignacio Guillermo Martínez @jasperan Copyright (c) 2021 Oracle and/or its affiliates. By Ignacio Martínez","categories": ["games"],
        "tags": ["analytics","gaming"],
        "url": "/tutorials/lol-article-3",
        "teaser": ""
      },{
        "title": "League of Legends Optimizer—Data Extraction and Processing",
        "excerpt":"Introduction Welcome to the League of Legends Optimizer article series! In this series, we’ll create usable, real-world tools to showcase the power of artificial intelligence (AI) applied to videogames. There are a lot of developers out there who are gamers, so what better way to use AI than to create a machine learning (ML) model for one of the most popular videogames out there? We’ll use this opportunity to showcase the capabilities that Oracle products can give us in relation to the entire data science process (extracting, processing, storing and using this data seamlessly). In this first article, we’ll dive deep into the Data Extraction step using the official Riot Games API. The purpose of these series is to explain how I was able to construct an ML model predicting the outcome of a game based on historical data, and how this data has been useful for coaches in the banning and drafting phase of League of Legends (LoL). For those of you who don’t know LoL deeply, it’s a popular multiplayer competitive game. In a LoL game, two teams of five players compete against each other to destroy the other team’s nexus. First Steps Using Riot Games’ Developer Portal, I signed up for a personal application. I explored the API and found that there are many endpoints to extract data, not only for LoL, but also from Riot Games’ other games (Teamfight Tactics and Valorant). We dive together into these other games in the future. In LoL, 5 players choose among a wide variety of champions, which have their own synergies between each other. Champion powers are either buffed (improved) or nerfed (reduced) if their statistics are too overpowered or imbalanced compared to other champions in what League calls patches. Team composition is a very important feature of the game, since exploiting compositions where champions have synergies gives teams a greater chance of winning. Since champion’s statistics are changed in patches, these team compositions change constantly, and pro teams spend a lot of time devising strategies on how to create powerful team compositions that lead them to victory. Note that in LoL, players are rewarded gold when killing enemies, minions or jungle monsters, and with these items players may buy upgrades such as weapons that deal more damage. The intricacies of the videogame are very complex and it takes years to reach the highest level of play. Therefore, if you are not familiar with some of the concepts that I talk about in this series, I make sure to explain these advanced concepts in more detail when they come up. I am not a professional player, but I have been playing LoL since Season 1 at high Diamond level, which gives me more than enough knowledge to start fidgeting with some AI code related to League. Firstly, we should talk about the architecture for this project: Oracle Cloud Infrastructure, the official cloud provider for this project a non-relational JSON database to store my data while I make my processing GitHub for version control and storing the code; it’s available in case you want to take a closer look at all code components Understanding the problem To understand the problem, we must remember the concept of a team composition. If five players are extremely good at a champion, or a champion is severely overpowered in a specific patch, then there is a way to prevent these players from picking them. In the draft phase of a game, players can ban a specific champion if they anticipate that champion would be a major threat. Banning the champion prevents the enemy team from playing it. Here’s a detailed explanation of how this process (usually) works: Pick Intent (15s): Players select their intended picks to show their team. Banning Phase (27s): All players simultaneously ban a champion from selection, opponents’ bans are not revealed. Ban Reveal (5s): Bans are revealed to all teams. It is possible for both teams to ban the same champion. Champion Selection (27s in 6 turns): One team is allowed to make the first pick, then teams alternate and pick two at a time until each player has selected. No champion can be selected more than once in a game. Finalization (30s): There is an opportunity to trade champions between teammates, both members must own both champions involved. One player can initiate the trade, and the other accepts to trade. Let’s start with the assumption that each team gets 5 bans each. So 10 bans total. We create an ML model to predict the best team compositions based on inputs. Therefore, if a player picks (for example) Lee Sin, our model be able to respond to this input and suggest counter-picks (meaning, champions whose win percentage / win rate is very high against LeeSin), also considering possible synergies with teammates. How about our training data? For this set, we wouldn’t want to include decisions made by less-skilled players, so we would want to only include data from professional players in our initial dataset. To do this, let’s only consider players in the Challenger, Grand Master, and Master leagues for regions all over the world. Ideally, this would be a collection of players above Master’s ELO and use their matches as reference. So, with this idea in mind, let’s explore the API and see if it can help us in this. Exploring the API / Extracting Data The standard development API key from Riot Games allows us to explore all 57 endpoints available. In this set of endpoints, I found an endpoint that can fetch Challenger players in a region, as well as Grandmaster and Master players, so I used this endpoint. def gettopplayers(region, queue, connection): \tassert region in request_regions \tassert queue in ['RANKEDSOLO5x5', 'RANKEDFLEXSR', 'RANKEDFLEXTT'] \ttotalusersto_insert = list() \t# master, grandmaster and challenger endpoints \trequest_urls = [ \t\t'https://{}.api.riotgames.com/lol/league/v4/challengerleagues/by-queue/{}'.format( \t\t\tregion, \t\t\tqueue \t\t), \t\t'https://{}.api.riotgames.com/lol/league/v4/grandmasterleagues/by-queue/{}'.format( \t\t\tregion, \t\t\tqueue \t\t), \t\t'https://{}.api.riotgames.com/lol/league/v4/masterleagues/by-queue/{}'.format( \t\t\tregion, \t\t\tqueue \t\t) \t] \t \theaders = { \t\t\"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0\", \t\t\"Accept-Language\": \"en-US,en;q=0.5\", \t\t\"Accept-Charset\": \"application/x-www-form-urlencoded; charset=UTF-8\", \t\t\"Origin\": \"https://developer.riotgames.com\", \t\t\"X-Riot-Token\": api_key \t} In this screenshot, I create a set of request URLs to get challenger, grandmaster, and master players. I want to get all players in all available servers, as well as all queue types. So I focus on getting data from Solo Queue (where a player can only play together or with a duo partner, and the remainder of the team is composed of random teammates) and Flex Queue (where teams of up to 5 people can be built). In order to expand my initial dataset I firstly considered getting all players above Master’s ELO, but as my dataset grew, I had enough active players, so after a while I decided to only consider new challenger players as an addition to my player collection. Note that if you want to reproduce this code, you need your own API key and insert it as a header parameter. for x in request_urls: \tresponse = requests.get(x, headers=headers) \tif response.status_code == 200: \t\ttry: \t\t\tprint('Region: {} | Tier: {} | Queue: {} | Total Players: {}'.format(region, response.json()['tier'], \t\t\t\tresponse.json()['queue'], len(response.json()['entries']))) \t\texcept KeyError as e: \t\t\tpass \t\tfor y in response.json()['entries']: \t\t\ttry: \t\t\t\ty['tier'] = response.json()['tier'] \t\t\t\ty['request_region'] = region \t\t\t\ty['queue'] = queue \t\t\t\ty['puuid'] = getsummonerinformation(y['summonerName'], region) # insert their puuid as well. \t\t\t\ttotalusersto_insert.append(y) \t\t\texcept KeyError as e: \t\t\t\tpass \telse: \t\tprint('Request error (@gettopplayers). HTTP code {}: {}'.format(response.status_code, response.json())) \t\tcontinue \tprint('Total users obtained in region {} and queue {}: {}'.format(region, queue, len(totalusersto_insert))) In this code, I make the requests using the requests Python library, and process the outputs by parsing the response into JSON and traversing through the JSON object. # Insert into the database. soda = connection.getSodaDatabase() # this open an existing collection, if the name is already in use collection_summoner = soda.createCollection('summoner') Finally, the last step is to insert this information in the database. I used an Autonomous JSON Database for this, since I didn’t want to bother with processing JSON objects too much. If you have experience with both relational and non-relational databases, you probably know it’s very complex to work with JSON objects and store them in a relational database schema. It’s much better to use a non-relational approach for this. I also didn’t want to bother with database performance, so that’s why I chose Oracle’s Autonomous JSON DB. (It’s very convenient in this case because it was really easy to setup and you can configure some visualizations in the future for your data which are publicly available through a URL. I definitely recommend checking out the advantages of the Autonomous JSON Database in more detail here. In order to use Oracle’s Autonomous JSON DB, we use the SODA protocol (Simple Oracle Document Access). To find out more and to read about the library I used for this step above, visit this user guide. The connection parameters for the database have been established in a config.yaml file with the following structure: riotapikey: apikey_123123123 db: \tusername: myuser \tpassword: mypw In order to use this database’s username and password in the code, information for the Data Source Name needs to be passed. Connecting to the Autonomous JSON database requires three things: the username, password, and DSN, which contains the URL and port of the database to connect to. To find out more about how to get your Python code to run and be able to connect into your own Autonomous JSON database, please refer to the official documentation for the cx_Oracle library. # Insert the users. for x in totalusersto_insert: \tqbe = {'summonerId':x['summonerId']} \tx['request_region'] = region \tx['queue'] = queue \t# We get the PUUID for the user in case they change their name. \toverallregion, tagline = determineoverall_region(region) \tx['puuid'] = getpuuid(overallregion, x['summonerName'], tagline) \ttry: \t\tcollection_summoner.insertOneAndGet(x) \t\ttime.sleep(1) # rate limiting purposes \texcept cx_Oracle.IntegrityError: \t\tprint('Summoner {} already inserted'.format(x['summonerName'])) \t\tcontinue \tprint('Inserted new summoner: {} in region {}, queue {}'.format(x['summonerName'], region, queue)) Here I inserted the player’s data into a collection called summoner. This collection store all the players’ identifying information. Note that in order to unequivocally identify a player or summoner as we call them from now on, a summoner name is not enough. Why? Because players can change their summoner name in the Riot Games store, so storing their username would not guarantee the access to their data permanently. Therefore, to identify users, we make an additional API call to get the PUUID (Personal UUID) for all players so that in case these users change username, we still be able to use their PUUIDs to get additional information about them. { \"tier\": \"CHALLENGER\", \"veteran\": false, \"wins\": 229, \"request_region\": \"br1\", \"rank\": \"I\", \"inactive\": false, \"summonerId\": \"dKMYAJhqPpBuI9hSfIon_a4zSbtCwTFep-DA6Lq9YwqlIQ\", \"hotStreak\": true, \"queue\": \"RANKEDSOLO5x5\", \"losses\": 198, \"freshBlood\": true, \"puuid\": \"aRjqIYDtBMBU2j-8EfHY6dJ0RZ9TqXgWLeNvpcjRWlCBaP8HGBAWFRAiehRM4Jo-lgJXXrjTCOcIKg\", \"summonerName\": \"Qats\", \"leaguePoints\": 922 } This is an example of data obtained for a European challenger player and their associated data. We can store this information efficiently in the summoner collection with a schema like this: We primarily care about the summoner’s PUUID and their request region in order to know the geographical location of the player and make requests to the Riot Games API accordingly. And, what is the kind of information that we want from our extracted players? Well, in reality we wouldn’t want to get summoners’ information above a Masters’ ELO if we wouldn’t use this data somewhere else, or to produce something useful for our model. So, if we want to predict optimal team compositions, we need some League games to analyze. After looking into the API a bit more, I found an endpoint which extracts the latest number of matches played by any summoner at any time (the limit for every request is 100, but we can use pagination to chain several requests and retrieve many more games). We store these match IDs into an auxiliary collection in order to further process them. We could do this in only one step, but since this is a data mining process designed to run for long periods of time (even 24 hours a day), I decided that it would be best to simplify and use highly decoupled functions, each separate from the other. { \"matchid\": \"BR12133968346\" } So, we insert only these match IDs for further processing into an auxiliary collection called match. Where is this going? Now that we have our match IDs downloaded into our match collection, we need to analyze what information to extract from a match. Looking at the API endpoint, I found that there are huge amounts of information for every match. Ideally, we would like to have, at least, the following information from every match: A match identifier, so that we don’t insert the same match twice into our dataset Each team’s compositions Each team’s bans The outcome of the game (win or loss) Additionally, we can also calculate the outcome of each one of the matchups. For example, calculate whether the player in middle lane lost their own matchup against the enemy middle laner, and so on. This can also be useful to create another ML model, which consider individual matchups and make predictions to help out in the individual component of a match. In the end, we would like to feed a player’s champion into the model, and the model give us the best possible choices (either for the ally or enemy team). So, we would tell the model something like: “I want to know the best synergy to pick together with X champion and I also want to know their worst matchups.” We start exploring this in the third article in this series, where we dive deep into AI code on how to create these models, train them and improve them. I hope to see you in the next article of this series, where together we : Establish a stable architecture for our code Optimize our code, which is good, but once the dataset grows, the code starts being inefficient Process our data further and get it ready for our ML pipeline DataSets I have published my datasets into Kaggle for everyone to use, in case you don’t have as much as time to complement them and process them. The process of data collection is arduous and took me a very long time since the beginning of this project. I’ll keep updating the datasets with the progress I make in the following articles. You can find them in here: Master+ Matches Collection (1.000.000+) Master+ Players Collection (30.000+) Master+ Matchups Collection (1.000.000+) (a dataset that we’ll use in the forthcoming articles). Stay tuned for the next articles! How can I get started on OCI? Remember that you can always sign up for free with OCI! Your Oracle Cloud account provides a number of Always Free services and a Free Trial with US$300 of free credit to use on all eligible OCI services for up to 30 days. These Always Free services are available for an unlimited period of time. The Free Trial services may be used until your US$300 of free credits are consumed or the 30 days has expired, whichever comes first. You can sign up here for free. License Copyright (c) 2021 Oracle and/or its affiliates. Licensed under the Universal Permissive License (UPL), Version 1.0. See LICENSE for more details. Written by jasperan, edited by GreatGhostsss By Ignacio Martínez","categories": ["cloudapps"],
        "tags": ["api","gaming","oci","always-free","python"],
        "url": "/tutorials/lol-optimizer-using-oci-extraction-processing",
        "teaser": ""
      },{
        "title": "Making calls to the Oracle Cloud API using Postman",
        "excerpt":" Photo credit: Photo by Adis Bacinovic from Pexels > This post is the author’s “notes from the field,” a personal exploration of the topic, filled with all of the journey’s pitfalls and little “Aha!” moments. It’s not meant to be a tutorial, but to illustrate the patterns and troubleshooting mindset it sometimes takes to successfully run a thing. Follow along and enjoy. Often when I'm building an automated solution, I'll look at the [documentation for the OCI Terraform Provider]. Terraform is my typical go-to tool these days for automating OCI infrastructure, so it's a logical place to start. Thankfully, the OCI Terraform Provider does a good job of sticking pretty close to the OCI API, so much of the terminology will be the same between the two. Sometimes, though, the Terraform documentation doesn't have all of the details I need and I have to go a level deeper. This is the point where I typically dive into the [OCI API docs]. The OCI API OCI has a terrific API. Really. In case you haven't had the pleasure of perusing it, check out the [full API details]. It's definitely worth digging in to. The OCI API does have one or two idiosyncrasies, though, especially the signing process. It can be somewhat involved, which means that you can't just easily \"curl\" it with off-the-shelf curl. Fortunately, there are [plenty of examples], making it easier than ever to interact with the OCI API with your favorite language. I've also shared a quick-and-dirty way to interact with the OCI API using Ruby in [another article]. Even with these tools, it still isn't easy yet for me to fully explore the OCI API. Interacting with the OCI API This is where tools like [Paw] and [Postman] shine. They're both built to make it really easy to interact with APIs, and the type of apps I like to have on hand when I need to easily (and quickly) interact with the OCI API. Of the two, I opted to go with Postman since it doesn't require a purchase (at least for the bare-bones functionality). I started searching for some different options on how to get Postman to send the necessary format and stumbled across this [great article] which covered exactly what I was looking for. I followed the directions, which are summarized in the sections below. First try 1. Clone the [repo] 2. Import the OCI_Environment environment 3. Set the values for the tenancyId, authUserId, keyFingerprint, and privateKey variables (only setting the Current Value, not the Initial Value for each) 4. Import the OCIRESTINITIALIZATION collection 5. Run the ONETIMEINITIALIZATIONCALL call within the OCIREST_INITIALIZATION collection 6. Import the OCIRESTCOLLECTION 7. Modify the GETOCIANNOUNCEMENTS call for the proper endpoint and compartmentId At this point, I tried to run GETOCIANNOUNCEMENTS, but didn't have any success. I validated all of my credentials and they were correct... What?! Why?! I noticed that my API key had a password, but I hadn't entered it anywhere. This is when I face-palmed myself, realizing that I'd failed to set the passphrase variable (again, just the Current value). After doing that, it worked great! Ugh. Some of the simplest things can present the greatest challenges! Finally working To summarize, here are the steps I took that worked for me: 1. Clone the [repo] 2. Import the OCI_Environment environment 3. Set the values for the tenancyId, authUserId, keyFingerprint, privateKey, and passphrase variables (only setting the Current Value, not the Initial Value for each) 4. Import the OCIRESTINITIALIZATION collection 5. Run the ONETIMEINITIALIZATIONCALL call within the OCIREST_INITIALIZATION collection 6. Import the OCIRESTCOLLECTION 7. Modify the GETOCIANNOUNCEMENTS call for the proper endpoint and compartmentId Which is almost identical to the first pass, except this time I made sure to set the Current Value of the passphrase variable. With that out of the way, I was able to move forward. I was thrilled by the ability to easily make calls, but what I really needed was the ability to quickly switch between different OCI environments. Multiple Postman Environments One of the great things about Postman Environments is that they're designed to allow you to quickly switch as needed. My intention was to leverage a different Postman Environment for each OCI tenancy I needed to access through a handy custom script. I tried duplicating the OCI_Environment in the Postman Environment, making sure that the values got moved with it, but that didn't seem to work. Eventually, I discovered that I was copying the private key contents from one Environment to another, and that the formatting was being stripped/changed (within Postman). The solution was to copy the key contents and paste a fresh copy into the new Environment. Voila! Success! Refactoring the script Now, I should have submitted a PR, but I'm including it here so I can share it for now. To get this super-awesome script to work with variables, there are a few small modifications that could be made: | Original line | New line | |---------------|----------| | var host = pm.request.url.host.join(\".\") ; | var host = pm.variables.replaceIn(pm.request.url.host.join(\".\")); | | var escapedTarget = encodeURI(request.url.split(pm.request.url.host.join(\".\"))[1]); | var escapedTarget = encodeURI(pm.variables.replaceIn(request.url.split(pm.request.url.host.join(\".\")))[1]); | | body = pm.request.body.raw; | body = pm.variables.replaceIn(pm.request.body.raw); | The above few changes seem to permit me to use variables, which allows me to greatly extend the script. Refactored call Now that we've sorted out the script, let's refactor the example given in the code, changing the GETOCIANNOUNCEMENTS call. Start by adding the following variables: | Variable Lives In (Type) | Variable Name | Variable Value | | -- | :--: | :--: | | OCIRESTCOLLECTION (Collection) | oci_domain | oraclecloud.com | | OCIEnvironment (or whatever you're using) (Environment) | ociregion | | Next, go to the GETOCIANNOUNCEMENTS GET call and modify the request path to be: https://announcements../20180904/announcements?compartmentId= This tells it to use the OCI region specified in the Environment, as well as the tenancyId (also specified in the Environment) and the OCI domain name that's set in the Collection. Conclusion As usual, something that seemed so cut and dry ended up being quite a bit more involved. But, we've got a solution now, and a way to interact with the OCI API using Postman on our local computer. Yay! By Tim Clegg","categories": ["devops"],
        "tags": ["api","oci","postman"],
        "url": "/tutorials/making-calls-to-oci-api-with-postman",
        "teaser": ""
      },{
        "title": "Manually configuring Data Science service on Oracle Cloud Infrastructure",
        "excerpt":" Learn how to get started configuring your tenancy for Data Science and test creating a notebook session. This tutorial is directed at administrator users because they are granted the required access permissions. In this tutorial, you will: 1. Create a Data Scientists User Group 2. Create a Compartment for Your Work 3. Create a VCN and subnet 4. Create policies 5. Create a Dynamic Group and write policies for it 6. Create a notebook session Before You Begin To perform this tutorial successfully, you must have the following: * An OCI account with administrator privileges, see signing up for Oracle Cloud Infrastructure. * At least one user in your tenancy who wants to access the Data Science service. This user must be created in IAM. Creating a Data Scientists User Group You must create a user group for the data scientists to work in. 1. Open a supported browser and enter the Console URL, https://console..oraclecloud.com. \tThe can be us-ashburn-1, us-phoenix-1, and so on. Use one of the Data Sciencesupported Regions and Availability Domains. 2. Enter your cloud tenant and click Continue. 3. Sign in with your credentials. 4. Open the navigation menu and click Identity & Security. Under Identity, click Groups. \t \tA list of the groups in your tenancy displays. 5. Click Create Group. 6. Create a data-scientists group and enter a description: \t 7. Click Create. \tYou are advanced to the data-scientists group detail page that you created. 8. Click Add User to Group. \t 9. Select a user to add, and then click Add. \tThe selected user is added and appears in the group member list. 10. Repeat adding data scientist users until all of your users are added to the data-scientists group you just created: \tA list of the users in your tenancy displays. Creating a Compartment for Your Work Next, you create a compartment for your data science resources. 1. Open the navigation menu and click Identity & Security. Under Identity, click Compartments. \t 2. Click Create Compartment to create your compartment. 3. Name the new compartment data-science-work, and enter a description. \t 4. Click Create Compartment. \tThe data-science-work compartment is created, and added to the compartments list when it successfully creates. Creating a VCN and Subnet. You need to create a virtual cloud network (VCN) for use by the Data Science service. > Note: For a private subnet to have egress access to the internet, it must have a route to a NAT Gateway. For egress access to the public internet, we recommend that you use a private subnet with a route to a NAT Gateway. A NAT gateway gives instances in a private subnet access to the internet. 1. Open the navigation menu and click Networking. Under Core Infrastructure, click . Click Virtual Cloud Networks. \t 2. Select the compartment that you want to create the VCN in. 3. Click Start VCN Wizard. 4. Make sure that VCN with Internet Connectivity is selected, and then click Start VCN Wizard. \t 5. Enter datascience-vcn for the VCN Name. 6. Select the data-science-work compartment. This compartment contains the VCN you are creating. It takes time for this new compartment to be populated in the drop-down list, so refresh the page until it appears. 7. Click Next. 8. Use the Configure VCN and Subnets defaults as follows: \t 9. Make sure that Use DNS Hostnames in this VCN is selected. 10. Click Next. \tA review of the VCN configuration is displayed. 11. Click Create to create the VCN and the related resources (three public subnets and an internet gateway). \t \tUse this VCN and its private subnet when you create your notebook session. 12. Click View Virtual Cloud Network to review your VCN and subnets. Creating Policies Before you can launch a notebook session, you have to configure the Data Science policies. 1. Open the navigation menu and click Identity & Security. Under Identity, click Policies. 2. Click Create Policy. 3. Enter data-science-policy for the Name. 4. Enter Policy for data science users and service as the Description. 5. Select the data-science-work compartment. 6. Click Show manual editor. \t 7. Enter these three simple policy statements into the Policy Builder field: \tTo allow users in the data scientists group to perform all operations on projects, notebook sessions, models, and work requests that are found in the data-science-work compartment: \tconsole allow group data-scientists to manage data-science-family in compartment data-science-work \t \tTo allow those data scientists to use the VCN you just created and attach it to their notebook session: \tconsole allow group data-scientists to use virtual-network-family in compartment data-science-work \t \tTo allow the Data Science service to attach that VCN to your notebook session and route egress traffic from the notebook environment: \tconsole allow service datascience to use virtual-network-family in compartment data-science-work \t 8. Click Create to create your policy. Creating a Dynamic Group and Writing Policies for It To enable notebook sessions to access other OCI resources, such as Object Storage or model catalog, you have to create a dynamic group and write policies for the notebook sessions' resource principals. 1. Open the navigation menu and click Identity & Security. Under Identity, click Compartments. 2. Click the data-science-work compartment. \tThe compartment details page is displayed. \t 3. Click Copy to save the entire OCID to your clipboard. 4. Click Compartments to return to the list of compartments. 5. Click Dynamic Groups. 6. Click Create Dynamic Group. 7. Enter the following: * Name: data-science-dynamic-group * Description: Data Science dynamic group 8. Enter this matching rule. Replace with the compartment OCID you copied. \tconsole \tALL {resource.type = 'datasciencenotebooksession', resource.compartment.id = ''} \t \t \tThis matching rule means that all notebook sessions created in your compartment are added to data-science-dynamic-group. 9. Click Create. \tNext, write a policy to enable access for this dynamic group. 10. Click Policies. \t 11. Click Create Policy. 12. Enter the following: * Name: data-science-dynamic-group-policy * Description: Policy for the Data Science dynamic group 13. Select the data-science-work compartment. 14. Click Show manual editor, 15. Enter these policy statements into the Policy Builder field: \tTo allow the notebook sessions to perform CRUD operations on entries in the model catalog, projects, and notebook session resources: \tconsole \tallow dynamic-group data-science-dynamic-group to manage data-science-family in compartment data-science-work \t \tTo allow notebook sessions to perform CRUD operations on Data Flow applications and runs: \tconsole \tallow dynamic-group data-science-dynamic-group to manage dataflow-family in compartment data-science-work \t \tTo allow notebook sessions to list and read compartments and user names that are in the tenancy: \tconsole \tallow dynamic-group data-science-dynamic-group to read compartments in tenancy \tallow dynamic-group data-science-dynamic-group to read users in tenancy \t \tTo allow notebook sessions to read and write files to object storage buckets that are located in the data-science-work compartment: \tconsole \tallow dynamic-group data-science-dynamic-group to manage object-family in compartment data-science-work \t \t 16. Click Create to create the policy. \tYou can use this dynamic group with resource principals in notebook sessions. Creating a Notebook Session Lastly, you need to create a notebook session then test its access to the public internet. 1. Open the navigation menu and click Analytics & AI. Under Machine Learning, click Data Science. \t 2. Click Create Project. 3. Select the data-science-work compartment. 4. (Optional) Enter Initial Project for the Name. 5. (Optional) Enter my first project for the Description. \t 6. Click Create. The project details page appears. 7. Click Create Notebook Session. 8. Ensure that the data-science-work compartment is selected. 9. (Optional) Enter my-first-notebook-session for the Name. 10. Enter VM.Standard2.8 for the Instance Shape. 11. Enter 100 for the Block Storage Size to attach to your virtual machine. 12. Select the datascience-vcn VCN and Private Subnet-data-science-vcn subnet to route egress traffic from your notebook session. \t 13. Click Create to launch your first notebook session. \tYou are advanced to the notebook sessions page. Creating the notebook session takes a few minutes. When it's complete, the status turns to Active, and you can open the notebook session. 14. Click Open. 15. Enter your Oracle Cloud Infrastructure credentials to access the JupyterLab UI. \t 16. Click Terminal to perform a simple test to check that you can access the public internet from your notebook session. 17. Run this command: \t \tconsole \twget --spider https://www.oracle.com \t \tYou should see a response similar to: \t \tThe HTTP request sent, awaiting response... 200 OK indicates a successful test and you have public internet access in your notebook session. What's Next You are done with this simple tenancy setup. Now, you can follow the remaining instructions in the getting-started.ipynb notebook session to setup the following from your notebook environment: * OCI configuration file on the notebook environment. * Access the model catalog. * Access Object Storage. * Access Data Flow. Using Notebook Sessions to Build and Train Models shows you how to continue with Data Science. By Olivier Francois Xavier Perard","categories": ["am-ml"],
        "tags": ["oci","data-science","nodejs","javascript","typescript"],
        "url": "/tutorials/manually-configuring-a-data-science-tenancy",
        "teaser": ""
      },{
        "title": "Use Matomo Website Analytics on OCI with MDS",
        "excerpt":"Matomo is a Google Analytics alternative for tracking metrics on your websites. If you follow my blog, you know how easy it is to deploy popular Open Source web solutions like WordPress, Joomla!, Drupal, Moodle, and Magento on Oracle Cloud Infrastructure (OCI). All these solutions are using MySQL Database Service to store their data. I’ve recently added a new stack to deploy Matomo. Of course, this can be a standalone installation to collect all your analytics from self-hosted websites, but today I'll describe how to use it with an existing stack we've already deployed on OCI. For this example, I deployed WordPress using the following stack: oci-wordpress-mds. The first step is to deploy Matomo on OCI. In the second screen of the Stack’s wizard, we specify that we want to use an existing infrastructure. This way we won’t need to recreate our VCN, subnets, security lists, Internet Gateway, etc. --- we want share what we've already deployed for WordPress: As you can see for the majority of the input fields, we need to provide the OCID. These can be found on the OCI’s dashboard. For example the OCID for the MySQL Database can be found here: We do this for every resources we want to reuse. Then we create an apply job for the stack and when done we can get the public IP and other necessary information in the output section: We can then enter the public IP in a browser and finish the installation: It’s important to use the right connector/adapter: MYSQLI. Then we follow the wizard and enter the required information. When done, we can add our WordPress in Matomo to start tracking it. The first step is the retrieve its name (in our case, its public IP as I don’t use DNS): Once added in Matomo, we can retrieve the javascript code used to track our website: We copy that code and we go into the admin dashboard of our WordPress site to modify the theme and add the previous code in the header file: And this is all we needed to be able to get analytics of our website we deployed on OCI. By Frédéric Descamps","categories": ["modernize"],
        "tags": ["mysql","analytics","back-end"],
        "url": "/tutorials/matamo-with-oci-mds",
        "teaser": ""
      },{
        "title": "HeatWave on AWS Metrics and Performance Tools",
        "excerpt":"As you’re working with your data, you’ll want to check out the performance of MySQL HeatWave on AWS, and we provide a number of metrics for you to examine. HeatWave Cluster Workspaces Above, we see the Workspaces tab in the Console, having just run a query. While we see a memory snapshot, we can go deeper in the Performance tab. Performance Here’s the good stuff! Not only can you see performance per node, you can see the size of the dataset, the data dictionary, and if you click on Workload on the left side, you’ll see duration each step of the query took, and when queries have taken place. Clicking back on Cluster and scrolling down, we can see metrics related to the VM itself, including memory and connection usage, CPU, and so on. If you’re keen on squeezing every drop of performance that you can out of HeatWave on AWS, we got you. Want to know more? Join the discussion in our public Slack channel! By ","categories": ["cloudapps"],
        "tags": ["mysql","database","heatwave"],
        "url": "/tutorials/metrics-and-performance-hw-aws-devrel0622",
        "teaser": ""
      },{
        "title": "How to import data from Microsoft SQL Server to MySQL Database Service",
        "excerpt":"After checking out how we can import data from PostgreSQL and Amazon Redshift, let's see how we can export data from Microsoft SQL Server and import it into MySQL Database Service (MDS) in Oracle Cloud Infrastructure (OCI). This time we will use something extra (sure, for fun, but also because it’s practical): OCI Object Storage! The process will be to export the data directly to OCI Object Storage from the MS SQL Server. Afterwards, we'll import it to MySQL Database Service using the MySQL Shell importTable() utility, reading directly from the Object Storage Bucket. For this exercise, we will use the BikeStores sample database. Tables Definition Our first task is to get the table definitions of what we want to export to MDS: sql 1> :setvar SQLCMDMAXVARTYPEWIDTH 30 2> :setvar SQLCMDMAXFIXEDTYPEWIDTH 30 3> go 1> select TABLESCHEMA, tablename from information_schema.tables 2> go TABLESCHEMA tablename ------------------------------ ------------------------------ production categories production brands production products sales customers sales stores sales staffs sales orders sales order_items production stocks (9 rows affected) One big difference between SQL Server and MySQL is that in SQL Server there is a notion of database and tableschemas. In MySQL, \"databases\" and \"tableschemas\" are synonymous. As all table names are unique, we will just ignore the table_schema names in MySQL and only use the database’s name: BikeStores. It’s easy to get the table definition using SSMS (SQL Server Management Studio), but it’s only available on Windows. We have then two remaining options for Linux users like me: * use Azure Data Studio * use the sqlcmd command Azure Data Studio From Azure Data Studio, you can get the table definition using Script as Create: And then we get the selected table’s creation statement: As in the previous post (mentioned at the beginning of this article), some minor changes will be required for MySQL. Using sqlcmd For those not willing to use a GUI, it’s also possible to get the table’s definition using the command line. Unfortunately, in SQL Server, SHOW CREATE TABLE does not exist. We will use a store procedure to get the info we are looking for: sp_GetDDL. So let's download it: console $ wget https://www.stormrage.com/SQLStuff/spGetDDLLatest.txt I added the following two lines at the top of the downloaded file before loading it to SQL Server: sql SET QUOTED_IDENTIFIER ON GO To load it, this is the command: console $ sqlcmd -S localhost -U SA -P 'Passw0rd!' -i spGetDDLLatest.txt We can now connect interactively to SQL Server like this: console $ sqlcmd -S localhost -U SA -P 'Passw0rd!' -d BikeStores We call the new procedure using the schemaname and tablename of the tables we want to have in MySQL Database Service: sql 1> :setvar SQLCMDMAXVARTYPEWIDTH 1024 2> exec sp_GetDDL 'production.categories' 3> go The command will return something similar to this: sql IF OBJECT_ID('[production].[categories]') IS NOT NULL DROP TABLE [production].[categories] GO CREATE TABLE [production].[categories] ( [category_id] INT IDENTITY(1,1) NOT NULL, [category_name] VARCHAR(255) NOT NULL, CONSTRAINT [PKcategoriD54EE9B454313162] PRIMARY KEY CLUSTERED ([category_id] asc) ) For MySQL, we rewrite the CREATE statement like this: sql CREATE TABLE categories ( categoryid INT UNSIGNED AUTOINCREMENT NOT NULL PRIMARY KEY, category_name VARCHAR(255) NOT NULL ) And we do the same for all tables we want to import to MDS. You can use the Microsoft SQL Server Type Mapping to find the more suitable MySQL data type. Mounting Object Storage We will use s3fs-fuse to mount OCI Object Storage Bucket on our SQL Server --- as explained in this article --- and dump the tables in it. We use EPEL to install the required package: console $ sudo yum install -y s3fs-fuse We create a bucket on OCI’s Dashboard: We need to create an ACCESSKEYID and a SECRETACCESSKEY: We copy these keys on one single line separated with a colon in a file, for example ~/.passwd-ocifs. And we mount it like this: console $ chmod 600 ~/.passwd-ocifs $ mkdir /mnt/ocifs $ s3fs lefred-bucket /mnt/ocifs -o endpoint=us-ashburn-1 \\ > -o passwd_file=~/.passwd-ocifs \\ > -o url=https://ixxxxxxxxxx.compat.objectstorage.us-ashburn-1.oraclecloud.com/ \\ > -onomultipart -o usepathrequest_style Now we can write data directly to our Object Storage Bucket using /mnt/ocifs. Exporting Data Everything is ready to export the content of the tables into CSV files: console $ sqlcmd -S localhost -U SA -P 'Passw0rd!' -d BikeStores \\ > -Q \"set nocount on; select * from production.categories\" \\ > -o /mnt/ocifs/categories.csv -h-1 -s\",\" -w 700 -W $ ls -lh /mnt/ocifs/ total 512 -rw-r--r--. 1 root root 147 Aug 24 21:28 categories.csv We can directly see it in OCI’s Dashboard too: We do the exact same process for all the tables we want to import to MySQL Database Service. You can, of course, also use the GUI to export to CSV and import those CVS files using MySQL Shell directly without using Object Storage: Importing Data As usual, we will use MySQL Shell to import in MDS the data that has been generated from MS SQL Server. We start by creating the database and the tables if this is not yet done: Don’t forget that if you need an OCI config file on the compute instance, you can create it from the OCI Dashboard for your user (Identity -> User -> User Details): You'll need to download the keys if you generate them, and then copy the content of the config in ~/.oci/config and set the private key's location and filename: After that, you're ready to import each table using MySQL Shell: We can see that the data is now present in MySQL: Repeat the same operation for each table you want to load into MySQL Database Service. In case you haven't used sqlcmd and Object Storage and prefer the use of the GUI to generate the CSV files, you can import them like this: Conclusion Once again, the best solution to load data to MySQL Database Service is MySQL Shell. Enjoy MySQL and MySQL Database Service! By Frédéric Descamps","categories": ["modernize"],
        "tags": ["mySQL","analytics","back-end"],
        "url": "/tutorials/microsoft-sql-to-mds",
        "teaser": ""
      },{
        "title": "Modernizing the Healthcare platform with a GraalVM Proof of Value",
        "excerpt":" Imagine a Fortune 5 company, empowering millions of people worldwide with the information, guidance, and tools to make personal health choices, setting out to work on two parallel streams for modernizing their HealthCare platform’s Cloud-Native tech stack. * Stream 1: Existing Microservices comprise Java 8 + Spring + OpenJDK JIT as JRE, to be containerized in a Hybrid Cloud platform. * Stream 2: New Microservices with Kotlin + SpringBoot + OpenJDK 11 / GraalVM JIT, to be containerized in a Public Cloud platform. Tactical Solution — just-in-time with Stream 1: * Lift and Shift Stream 1 from existing Hotspot JIT to GraalVM JIT * Evaluate memory footprint, execution time improvement, and peak throughput with GraalVM * Benchmark Stream 1 with GraalVM JIT vs HotSpot JIT on performance heuristics * Endurance Testing with business-critical Healthcare API for benchmarking Strategic Solution — ahead-of-time with Stream 2: * Application re-writes are planned and primarily targeted for the Public Cloud Platform * Stream 2 tech stack is being considered with GraalVM Enterprise as an option depending on the outcome of Stream 1 benchmarking results * Serverless (initial thoughts) has got some excitement for GraalVM’s Native-Image due to memory footprint and cold-startup optimization Healthcare Context Interoperable healthcare IT enables clinicians to improve care coordinations and ensure that the information available to view in the healthcare services is part of a practitioners’ workflow. In terms of the technology stack, here are some considerations and goals: 1. Huge employee base, hundreds of APIs, and countless integrations and external systems 2. Optimize healthcare technology in order to provide a scalable tech infrastructure 3. Eliminate Local Resource Constraints for Building Cloud-Native Applications 4. Implement patterns and practices defined by DevOps and Cloud Centre of Excellence Support for Open Healthcare Standards HL7 FHIR is a standard for health care data exchange published by HL7 (HL7 and its members provide a framework for the exchange, integration, sharing, and retrieval of electronic health information). The CNCF Cloud Native Computing Foundation serves as the vendor-neutral home for many of the fastest-growing open-source projects Research and Evaluation Proof of Value The engineering team identified key Proof-of-Value initiatives to optimize the existing tech stack with GraalVM without code changes, iteratively running performance loads with identified heuristics, benchmark observations, and comparisons. Initial performance load on GraalVM and benchmarking it against conventional JDKs (OpenJDK) for feasibility analysis. This included two rounds for performance evaluation. As a leading healthcare company (Fortune #5), the peak season is typically at the start of the year for all health plans renewables in the US, hence the priority at that time was to support business peak load in tech frozen state for 1–2 months. This resumed towards the end of Q1 2021. GraalVM EE v21.x on JDK8 Actual load runs on Dev Environment Sandbox. Load Configuration Details: * Concurrency: 100 users * Ramp-up: 4 seconds * Duration: 4 hours, 14400 seconds The Clinical API was subjected to a medium spike load as part of the warm-up phase and peak throughput phase. The evaluation results are summarized in the table below. GraalVM performed better than Open JDK8 in most of the runs and gets better for longer runs. Conclusion This collaboration has been mutually beneficial. Specfically, from learning about the implications of GraalVM in the healthcare domain by optimizing its workflow and improving patient outcomes and experience. To learn more and get started with GraalVM, visit https://www.oracle.com/graalvm Thanks to Pratik Prakash, Senior Member of Engineering at UnitedHealth Group, and Amitpal Singh Dhillon, Regional Director for Oracle Labs in Asia-Pacific & Japan, for their help in writing this blog post. By Ali Mukadam","categories": ["clouddev","modernize"],
        "tags": ["graalvm","devops"],
        "url": "/tutorials/modernize-healthcare-graalvm",
        "teaser": ""
      },{
        "title": "Build, Move, and Modernize Applications",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/use-cases/modernize",
        "teaser": ""
      },{
        "title": "Sending Emails from OCI with Email Delivery Service in Node.js",
        "excerpt":"Oracle Cloud Infrastructure (OCI) [Email Delivery] is an email notification service that provides a fast and reliable managed solution for sending high-volume emails that need to reach your recipients’ inbox. Email Delivery provides the tools necessary to send application-generated email for mission-critical communications such as receipts, fraud detection alerts, multi-factor identity verification, and password resets. In this tutorial, we'll cover all the basics to get you up and running with the Email Delivery service! Set up Email Delivery 1. Go to IAM/user and select Generate SMTP Credentials as shown below: 1. Keep the credentials created in a safe place: 1. Create an Approved Sender (a real, existing email account to put in the from field): 1. Grab the connection details: 1. Now, test the code: console npm install nodemailer 1. Create a sendmail.js file: console var nodemailer = require('nodemailer'); async function main() { let testAccount = await nodemailer.createTestAccount(); let transporter = nodemailer.createTransport({ host: \"smtp.email.eu-frankfurt-1.oci.oraclecloud.com\", port: 25, secure: false, auth: { user: 'ocid1.user.oc1..aaaaaa...om', pass: 'BD..._', }, }); let info = await transporter.sendMail({ from: '\"javier...om', to: \"javi...om\", subject: \"ssh access to 10.0.2.94\", html: \"ssh -i deltakey -o ProxyCommand=\\\"ssh -i deltakey -W %h:%p -p 22 ocid1.bast...oud.com\\\" -p 22 opc@10.0.2.94\", }); console.log(\"Message sent: %s\", info.messageId); console.log(\"Preview URL: %s\", nodemailer.getTestMessageUrl(info)); } main().catch(console.error); Test the configuration Now that you have Email Delivery set up, let's verify that everything is working properly. In a console window, run: console node sendmail.js And that's it! If your local output is similar to what's shown above, you're all set and ready to receive notification emails! What's next If you’re curious about the goings-on of Oracle Developers in their natural habitat, come join us on our [public Slack channel]! And don't forget our [free tier], where you can try out what we just discussed. To explore more information about development with Oracle products: * [Oracle Developers Portal] * [Oracle Cloud Infrastructure] By Javier Mugueta","categories": ["frameworks","cloudapps"],
        "tags": ["open-source","oci","always-free","nodejs","javascript"],
        "url": "/tutorials/nodejs-sendmail-oci",
        "teaser": ""
      },{
        "title": "Getting Started with MySQL HeatWave on AWS",
        "excerpt":"We live in a multi-cloud world, and that's why MySQL HeatWave for Amazon Web Service makes so much sense if you need a massively parallel, high performance, in-memory query accelerator for the MySQL Database Service. A combination that accelerates MySQL performance by orders of magnitude for combined analytics and transactional workloads (OLAP and OLTP). The MySQL Database Service is built on MySQL Enterprise Edition, which allows developers to quickly create and deploy secure cloud native applications using the world's most popular open source database. Oracle designed this so developers can focus on the important things, like managing data, creating schemas, and providing highly-available applications. MySQL HeatWave for Amazon Web Services (AWS) is a fully managed service, developed and supported by the MySQL team in Oracle. Oracle automates tasks such as backup, recovery, and database and operating system patching. \"Worry less, crunch more,\" as we say! If you’ve never heard of HeatWave, think of it as a database query accelerator with boost buttons. As in The Fast and the Furious, when you want to pull ahead of the competition, you hit the NO2 and get the speed you need, right when you need it. And of course, this efficiency means it’s a little less expensive to run those big queries. One of the incredible things about Oracle MySQL HeatWave is the ability to run analytics directly against your existing transactional data, so there's no need to shuffle that data off to a separate system when you need to perform massively parallel analysis. To get started, we'll create a compartment and install MySQL Shell due to its extended capabilities over vanilla MySQL and create a small database so we can eventually connect it to HeatWave for analysis. Note that this is working within Oracle Cloud, but we’ll cover AWS setup in another tutorial to show you how you can leverage HeatWave in a multi-cloud scenario. What a time to be alive! Let's look at how to get started. If you're already developing in Oracle Cloud (OCI), you'll find it's relatively easy to get going, as HeatWave on AWS is integrated with OCI's Identity and Access Management system. When you sign up for HeatWave on AWS, you'll be directed to the OCI login page where you must sign in with an OCI Cloud Account. After signing in, you'll be directed to the OCI Console to complete the MySQL HeatWave on AWS sign-up process. When signing into the HeatWave Console, you are directed to OCI for authentication and then back to the HeatWave Console. To keep things simple, billing is still managed and monitored in OCI. Since we're just getting started, let's begin truly at the beginning and create a compute instance with the proper access rules and see how to create a HeatWave cluster in OCI. If you're already using AWS, we'll cover that in a separate tutorial. PREREQUISITES - An OCI account and Oracle Cloud Account name - Admin access - A compatible browser (Chrome 69+, Safari 12.1+, or Firefox 62+ or any browser that is Oracle Jet-approved) OVERVIEW MySQL HeatWave on AWS uses predefined Oracle Identity Cloud Service (IDCS) groups and policies to control user access to MySQL HeatWave on AWS and the type of access. You should have the ability to create and modify policies, users, and the like. Also, we are assuming you're creating the database and administering it, or at least getting the prep work done. Look at you, a one-stop shop! 1. Create a Compartment 2. Create a a VCN and configure for database access 3. Create users and groups (if you haven't already) 4. Create a Bastion Host compute instance 5. Connect and Install MySQL Shell 6. Create a MySQL database 7. Create a DB System with HeatWave-compatible shape 8. Activate HeatWave on AWS Wondering why this is a \"bastion host\"? You can read more about bastions in this article. >NOTE: Once you’re connecting databases and analytics, there’s a better production method for connecting, and that’s creating a Private Access Channel (OAC), which you can learn all about in the article, How to create OAC instances on OCI Native using multiple stripes or instances of IDCS In our example, we’re using a quick and dirty approach to set things up to use HeatWave. Now let’s get started with the basics! We begin by slicing off a piece of the cloud as our own little homestead. There are a couple of ways to do this, but one of the simplest is to create a Compartment (you could also start with a Compute instance). This is a “place for your stuff” within your tenancy and is quite flexible. As you might imagine, we need to create a group of users who can administer our system, and Identity and Access Management (IAM) is where you’ll go to configure this for any compartments you create. Create a Compartment Menu: Home > Identity & Security > Compartments I could have set all of this up in my root compartment, but a new compartment is better way to organize things. Creating compartments is a simple matter, and a necessary starting point to organize and configure your work. I've named mine something clever, like myheatwavetesting so I know what it's for. Create a VCN and configure for database access Menu: Home > Networking > Virtual Cloud Networks 1. Create VCN and subnets using Virtual Cloud Networks > Start VCN Wizard > Create a VCN with Internet Connectivity. The handy wizard will walk you through creating a network interface for your system, although there are lots of ways to configure this, let’s not get distracted. Notice that I chose the compartment I set up earlier, myheatwavetesting -- because that's important! 1. Now let's configure the VCN's security list to allow traffic through MySQL Database Service ports. Click on the Private Subnet for the VCN you created, then click the Security List for it. 1. Now click Add Security list. 1. We'll add some ingress rules needed to enable the right ports, 3306 and 33060. Here's the details: console Source CIDR: 0.0.0.0/0 Destination Port Range: 3306,33060 Description: MySQL Port 2. And click Add Ingress Rules. Looking good so far! Create users and groups (if you haven't already) We’ll need to set permissions and limit access somewhat, even in our “quick and dirty” example, but you can read all about managing groups here. Usually we'll create a group, create policies, then add users to the group. Let’s make friends with the Identity and Security options. 1. Create a group for your users. In my example I’ve created a group called databaseuser (just to be confusing, as I should have named it databaseadmins, but this was just a clever ploy to keep you on your toes). 2. Add users to the group. In our example, we’ll add ourselves to this group that will administer our compute instance running MySQL-shell. Of course, for a group you’ll first create all the users you need, add those into the group needing access at the levels you determine, and rest assured that you can set them loose with appropriate access controls. 3. We allow access by setting policies, allowing one group to have full access (admins), and a group with limited access (database users, for example). For MySQL HeatWave on AWS, there are some specific policy statements we can use, detailed in the charts below. This is just making it possible to configure and administrate our compartment, and defines the scope of the access applied to the database instance. For more on adding users and setting policies in OCI, refer to this documentation. Create a Bastion Host Compute Instance MENU: Main > Compute > Instances Be sure to select the compartment you set up earlier, under List Scope. Create Instance Click Create Instance (easy, right?) Name it something useful, and right now we'll leave the Availability Domain, Fault Domain, Image, and Shape as-is. You can use a free-tier compute! We’re going to use Oracle Linux, but one of the niceties here are the choices of compute shapes and Linux distributions to choose from. There’s even a developer distro, which comes pre-configured with key frameworks. For our purposes, we’ll want to make sure it’s set up to work with HeatWave. Plus, we'll use a Bastion Host for better security. Bastions provide \"restricted and time-limited access to target resources that don't have public endpoints,\" and you can read all about them in this overview article. Launching a Linux instance If you want to know more, here's a tutorial on launching a Linux instance, but I'll walk you through the basics now. Bear in mind that our compute instance can be pretty minimal, and there are free tier shapes that could work (our always-free tier VM is quite generous). Networking Scrolling down, you'll see the Networking section. Here you'll want to make sure to use the VCN you created previously, as well as make sure you're in the proper compartment. Generate SSH keys And of course, during this process you’ll generate SSH keys so you can access your computer instance remotely. You can do this within the Cloud Shell in OCI’s dashboard, or the SSH client of your choice. Let Oracle make it easy Also, the path of least resistance for creating a key pair will be letting Oracle generate one. The key pair will allow you to log in remotely and install MySQL-shell, etc. >NOTE: In many labs we’ll have you use the Cloud Shell, which is a convenient command line interface available directly in the OCI dashboard. I’m old school, so I’m just using Terminal on my Mac. You can use the SSH client of your choice, of course! For more information see: - Install Node Express on an Oracle Linux Instance - A video on working with SSH keys. Obtain public IP for compute instance Of course, you'll need the public IP for your compute instance, which is found in Compute > Instances > Instance details. Under Instance Access you'll find the public IP and username (opc) you'll need to connect, with a handy copy button. Now, we'll be able to connect via SSH or the Cloud Shell, and since you have a public IP, you can just ssh in to your compartment and the OCI Linux compute instance. As always, keep the private key in a safe place and chmod 400 the private key to keep it from being modified (and throwing a warning). Provision the Instance Go ahead and click Create. It'll take a moment for the provisioning to finish up, but when it's done the large square icon will turn green, meaning all systems are GO! Connect and install MySQL Shell To connect, let's use the handy Cloud Shell. It's a little Linux terminal embedded in the OCI dashboard (and it's adorable). 1. In the upper-right corner, click the Cloud Shell prompt icon and a command line will open at the bottom of the browser. 1. Drag and drop the previously saved private key into the cloud shell, uploading it to your home directory. 1. Under Instance Access, you'll see the public IP address, and the handy Copy button. Copy the public IP. 1. Now let's ssh in, first protecting the private key file. console chmod 400 .key 1. Then use your public IP address and username opc: console ssh -i .key opc@ 1. If asked to accept the fingerprint, type yes and hit enter. You've been added to the list of known hosts, congrats. We're in! If you see Tron, wave. 1. Now we install MySQL Shell; pretty easy these days. In my case, I used SSH to log into my compute instance (don’t forget you’ll need your private key) and used yum to install what I needed. Install the MySQL Client on the compute instance using the following command: sudo yum install mysql-shell 1. Once we create our HeatWave-compatible DB System, we'll connect to to it using the MySQL Client: mysqlsh --host -u -p For more information see: - Learning about MySQL Shell. - Learning about connecting database systems. Create a DB System Remember a little while ago when we mentioned the endpoint for your DB System? Let's set that up now. MENU: Menu > Databases > DB Systems >NOTE: Notice that the system warns you if you haven't already set up users, a VCN, and so on. That's nice. >Also, don't forget to check which compartment you'll create this in, again under List Scope on the left. 1. Click Create DB System. Double-check the compartment, give it a name, and select HeatWave (of course). 1. You'll create admin credentials. Be sure to save those somewhere handy but safe! 1. In Configure Networking, you'll use the compute instance created earlier, but we'll use the private subnet. Leave the default domain. 1. Go to Configure Hardware. Confirm that in the Configure Hardware section, the selected shape is MySQL.HeatWave.VM.Standard.E3. Also, confirm that: - CPU Core Count: 16, - Memory Size: 512 GB, - Data Storage Size: 1024 1. In the Configure Backup section you may leave the default backup window of 7 days. 1. Keep scrolling and click Show Advanced Options. 1. Go to the Networking tab. In the Hostname field, enter the exact name of your DB System. 1. Make sure port configuration corresponds to the following: - MySQL Port: 3306 - MySQL X Protocol Port: 33060 1. And... click Create! This time a yellow hexagon will appear, and eventually it'll turn green and your DB System will be up and running. Make some tea or grab some water, you've done a lot. Create a MySQL database Now, you'll want to create your database and import any data you need. HeatWave is really designed for big data sets needing fast analysis, so even though I’m importing the tiniest database ever, you can load up as much as you like (provided you have the storage for it). Plus, queries can be run in the cluster without offloading to a separate database. Whether you're deploying to OCI or AWS, we got you. Finally, the fun part! Import a .sql file. From the command line Type: mysql -u username -p database_name or , where you'll see the welcome page. Enter your Oracle Cloud Account name and click Continue. 1. Click Enable MySQL HeatWave on AWS. This takes you to a Admin page where you will go through a brief setup process. You may have to upgrade your account to paid with a credit card, and once complete, you'll go to the OCI Console. Try not to time this for the last minute, as provisioning may take a moment. 1. From the OCI Console navigation menu, select Databases. MySQL HeatWave on AWS appears on the Home tab under the Featured label. 1. Under MySQL HeatWave on AWS, click Administration, and you'll go back to the setup. 1. Now click Provision to (of course) provision MySQL on AWS. After the provisioning operation is completed, a message appears stating that MySQL HeatWave on AWS is ready and you are presented with options to open the MySQL HeatWave console, set up users, and view billing information. Summary - so far What we've done so far, all on OCI, is set up a Virtual Cloud Network with ports for MySQL use, created a Bastion Compute instance, then set up a MySQL database, and now we have an endpoint for our HeatWave on AWS instance, and HeatWave should be provisioned on AWS. Want to know more? Join the discussion in our public Slack channel! By ","categories": ["cloudapps"],
        "tags": ["mysql","database","heatwave","aws"],
        "url": "/tutorials/oci-getting-started-heatwave-aws-v30622",
        "teaser": ""
      },{
        "title": "Using OCI Cloud Shell & Bastion with MySQL Database Service",
        "excerpt":"In 2021, Oracle added a Bastion Service to Oracle Cloud Infrastructure (OCI). Along with it, the OCI Dashboard now features the ability to use a browser-based terminal: Cloud Shell. Today, we'll show you how to use these two components to connect from a browser to a MDS DB System. Key topics covered in this tutorial: - Setting up a Bastion Service - Starting a Cloud Shell - Initiating a Bastion Session - Connecting to MDS Let's get started! Find the MySQL DBs IP address First, we need the MySQL DB System’s IP: So, in this example, the MDS Instance we want to connect to has an IP of 10.0.0.99. Set up the Bastion Service Next, we'll create a new Bastion Service that will allow us to create a SSH Tunnel to our MySQL DB System. Connect to the Bastion Service Dashboard The Bastion Service’s dashboard is located in Identity & Security: >Note: If this is the first time you create a Bastion, the list will be empty and you just need to create one: Set up the VCN Now, we need to select the Virtual Cloud Network (VCN), the subnet, and a block of allowed IPs. Since we don't know the IP of the Cloud Shell, we'll just use 0.0.0.0/0: >Note: If you don't like to use 0.0.0.0/0, you need to add the public IP used by Cloud Shell with \\32: console curl ifconfig.me Sample output: console 1XX.XXX.XXX.XXX Create a session Now that the Bastion is created, we need to create a session that will be used to create the SSH Tunnel to MDS. But before creating the session, we'll start the Cloud Shell and generate a SSH Key we will use for the tunnel’s session. We'll take a look at that in the next section. Cloud Shell To Start Cloud Shell, you just select the shell icon at the top right corner of the OCI Dashboard: >Note: This will open the Cloud Shell in the browser. It may take some time to open the first time. Create SSH key In the Cloud Shell, we can now create the SSH Key we need using the following command: console ssh-keygen -t rsa As you can see below, the public key we need will be stored in ~/.ssh/id_rsa.pub: Bastion Session Now that we have all we need to create the Bastion Session for the SSH Tunnel, we can go back to the Bastion we created earlier and create a new session: After you select Creation Session, you will need to do the following: - set the SSH port forwarding session as Type - add the MySQL Database System’s IP - paste in the SSH Public Key Once the session is created (which may take upwards of two hours), you will have something that looks like this: Locate the ssh command If you select the kebob icon (the 3 vertical dots), you can view or copy the ssh command we need to run in Cloud Shell: >Notes: > >- -i is not really required since we only have one single key for the moment. >- The error message bind: Cannot assign requested address is not a problem. > This message only displays because because the Cloud Shell tries to bind on ipv6 too. If you want to avoid it, just add -4 between ssh and -i like this: > > console > ssh -4 -i > > >- Note the & at the end of the main ssh command. Connecting to MDS And finally, we can connect to MySQL Database Service’s instance from Cloud Shell simply by using the MySQL Shell: As you can see, it’s easy to connect from Cloud Shell once the Tunnel is ready! >Recommendation: To Dump & Load data to/from MDS, we recommend using a dedicated compute instance with multiple cores instead of Cloud Shell. What's next To explore more information about development with Oracle products: - Oracle Developers Portal - Oracle Cloud Infrastructure By ","categories": ["frameworks","cloudapps"],
        "tags": ["oci","mysql","always-free"],
        "url": "/tutorials/oci-shell-bastion-MySQL",
        "teaser": ""
      },{
        "title": "Installation Guide for OCI Monitoring",
        "excerpt":" > Note: This is an experimental environment. Feel free to try it, extend it, but most importantly, have fun with it! In this walkthrough, you'll install a basic Oracle Cloud Infrastructure (OCI) monitoring solution with these components based on Ansible in Oracle Linux 8. The setup is tested for: - OL8 running in ESXi - OL8 running in local VMware Workstation with NAT - OL8 running in Oracle Cloud Infrastructure Installed components by Ansible roles: - Docker - [Steampipe] - [Grafana] - [Prometheus] - Push Gateway - PostgreSQL How it works: 1. Execute the Python script 2. Steampipe gathers the information from Oracle Cloud Infrastructure 3. The return value is pushed to Prometheus Push Gateway 4. Prometheus scrapes the metrics from the gateway 5. Grafana reads the metrics from Prometheus For more information, see: - Signing Up for Oracle Cloud Infrastructure - Getting started with OCI Cloud Shell - [OCI CLI] Prerequisites - root access by password - /etc/hosts configured - Ansible and Git configured - Internet access - Oracle Cloud Infrastructure user with inspect permissions, including SSH PEM key and configuration Software installation: OL8 ESXi / OL8 VMware As user root: console yum -y install yum-utils yum -y install oracle-epel-release-el8 yum-config-manager --enable ol8developerEPEL yum -y install ansible git Software installation: OL8 Oracle Cloud Infrastructure As user opc: console sudo dnf upgrade sudo dnf -y install oracle-epel-release-el8 sudo dnf config-manager --enable ol8developerEPEL sudo dnf -y install ansible git Ansible SSH configuration for Oracle Cloud Infrastructure - Upload the opc's SSH private key to /home/opc/.ssh temporarily for installaton purposes - Change the Ansible checked out hosts file to: console [all:vars] ansiblesshprivatekeyfile=/home/opc/.ssh/ [monitoring] ansibleuser=opc ansiblepythoninterpreter=\"/usr/bin/env python3\" > After the installation, it's a good practice to remove the opc private key from your compute instance Steps 1. Login to Oracle Linux 8 as root 2. Clone the repository to a local folder such as /root/git 3. Change to subdirectory oci-monitoring 4. Update the Ansible hosts file with your IP and root password. ansiblesshpass is required for local connections 5. Run ansible-galaxy collection install -r roles/requirements.yml 6. Run ansible-playbook install.yml 7. As root, verify that all Docker containers are running: console docker ps Your output should be something like: console CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES f7f2e137f4a1 prom/pushgateway \"/bin/pushgateway\" About an hour ago Up About an hour 0.0.0.0:9091->9091/tcp pushgateway c6ecc72065c9 prom/prometheus \"/bin/prometheus --c…\" About an hour ago Up About an hour 0.0.0.0:9090->9090/tcp prometheus 3485de8cc1f9 grafana/grafana \"/run.sh\" About an hour ago Up About an hour 0.0.0.0:3000->3000/tcp grafana 8e821aa0044b turbot/steampipe \"docker-entrypoint.s…\" About an hour ago Up 30 minutes 0.0.0.0:9193->9193/tcp steampipe Network security The Ansible playbooks also open these ports in the VM for troubleshooting access: - 3000: Grafana - 9090: Prometheus - 9091: Prometheus Push Gateway - 9093: Steampipe Service OCI configuration 1. After the successful Ansible execution, put your personal OCI configuration and SSH key into the directory /home/steampipe/.oci. 1. Replace the dummy values. 1. Update the file /home/steampipe/config/oci.spc with the correct SSH key file name: > Note: Take care that both the owner and group of the OCI configuration file is steampipe Example: 1. Check the current directory: console pwd Your output should be: console /home/steampipe/.oci 2. Verify contents of the directory: console ls -l Your output should be something like: console total 8 -rw-r--r--. 1 steampipe steampipe 307 Aug 9 09:01 config -rw-r--r--. 1 steampipe steampipe 1730 Aug 9 09:01 jurasuedfuss-20210809.pem 1. Restart the Docker container for Steampipe: console docker stop steampipe docker start steampipe How to create the user for OCI access using the OCI CLI Next we'll create an OCI user for monitoring. For use in this section: - An existing OCI CLI setup for an tenant administrator is required to execute these steps. - The required SSH key in PEM format can be downloaded from the OCI web interface. - The user, group, and policy can be created inthe web interface as well. Now, all we need for Steampipe is the OCI config file for the new user and their SSH key in PEM format. Create User console oci iam user create --name ociuserreadonly --description \"OCI User with inspect all-resources.\" Create Group console oci iam group create --name ocigroupreadonly --description \"OCI Group with inspect all-resources.\" Add User to Group console $ oci iam group add-user \\ --user-id \\ --group-id Create Policy console $ oci iam policy create \\ --compartment-id \\ --name ocipolicyreadonly \\ --description \"OCI Policy with inspect all-resources.\" \\ --statements '[ \"allow group ocigroupreadonly to inspect all-resources on tenancy\" ]' Add API Key 1. Add your API key: 2. Download the created private key in PEM format: 3. Copy the configuration file preview. The values are used for the Steampipe OCI configuration. Steampipe OCI Regions To filter your regions, just edit the file /home/steampipe/config/oci.spc. For example: json connection \"ocitenantkestenholz\" { plugin = \"oci\" configfileprofile = \"DEFAULT\" # Name of the profile config_path = \"~/.oci/config\" # Path to config file regions = [\"eu-frankfurt-1\" , \"eu-zurich-1\"] # List of regions } Test Steampipe Here are some commands to verify if Steampipe is working as expected: >Note: All commands need to be executed as root. - list plugins console docker exec -it steampipe steampipe plugin list Your output should look something like: console +--------------------------------------------+---------+-----------------------+ | Name | Version | Connections | +--------------------------------------------+---------+-----------------------+ | hub.steampipe.io/plugins/turbot/oci@latest | 0.1.0 | ocitenantkestenholz | +--------------------------------------------+---------+-----------------------+ - confirm RUNNING state console docker exec -it steampipe steampipe \\ query \"select displayname,shape,region from ocicoreinstance where lifecyclestate='RUNNING';\" Your output should look something like: console +-----------------------------------+------------------------+----------------+ | display_name | shape | region | +-----------------------------------+------------------------+----------------+ | Instance-DB-1 | VM.Standard1.2 | eu-frankfurt-1 | | Instance-AS-1 | VM.Standard1.1 | eu-frankfurt-1 | +-----------------------------------+------------------------+----------------+ - confirm home region console docker exec -it steampipe steampipe \\ query \"select key,title,status from ociregion where ishome_region=true;\" Your output should look something like: console +-----+----------------+--------+ | key | title | status | +-----+----------------+--------+ | FRA | eu-frankfurt-1 | READY | +-----+----------------+--------+ Python Example Scripts In the subdirectory /home/steampipe/py, there are two basic examples of how to get the data from the Steampipe PostgreSQL service to Python3. Feel free to adapt the queries and files as needed. Also, keep in mind that the returned values are pushed to the Prometheus Gateway on port 9091 for further usage. | Script | Purpose | |-|-| | pgsql-query-bv-zurich.py | Summary of Block Volume in OCI Region Zurich | | pgsql-query-ci-running-zurich.py | Summary of running Instances in OCI Region Zurich | > Currently, you'll need to restart the Docker container before executing Python3 according to the error generated by running these scripts. In future iterations, we hope to have a solution, but right now this is an easy fix! 1. Manual execution and upload of the query result: console python3 pgsql-query-ci-running-zurich.py python3 pgsql-query-bv-zurich.py These will return the error noted above: console Something went wrong: no connection config loaded for connection 'oci' 1. Restarting Steampipe as root: console docker stop steampipe docker start steampipe Prometheus Push Gateway According to the Python script, new data is passed to the Prometheus Push Gateway on port 9091 and scraped by Prometheus port 9090. Check out this example for the Prometheus Gateway where data is loaded by jobs ociblockvolume and ocicompute: Grafana Grafana is reachable by address on port 3000 of your machine (e.g., your-machine-ip:3000). The default login and password is: - Username: admin - Password: welcome1 The Prometheus data source and a basic dashboard are deployed during the Grafana Docker setup process. - Here's an example for dashboard OCI Demo - eu-zurich-1: - And here you can see the pushed metric from the Python script by name: Troubleshooting Docker Logs To verify that Steampipe is running properly: console docker logs steampipe Steampipe Access Logs The foreign data wrapper logs are stored locally (not in the Docker container) in the directory /home/steampipe/logs: console drwx------. 11 steampipe steampipe 173 Aug 9 17:18 .. -rw-------. 1 9193 root 756701 Aug 9 19:57 database-2021-08-09.log drwxrwxr-x. 2 steampipe root 68 Aug 10 02:00 . -rw-------. 1 9193 root 3411203 Aug 10 07:19 database-2021-08-10.log What's next At this point, you should have your OCI monitoring solution up and running. Feel free to continue extending and exploring its capabilities! For more information about development with Oracle products, see: - [Oracle Developers Portal] - [Oracle Cloud Infrastructure] By Martin Berger","categories": ["modernize"],
        "tags": ["ansible","data-visualization","data-management","back-end"],
        "url": "/tutorials/ocimonitoring-w-steampipe-prometheus-grafana",
        "teaser": ""
      },{
        "title": "Deploying a Custom Nodejs Web Application Integrated with Identity Cloud Service for Unique Single Sign On UX",
        "excerpt":"In this post we are deploying a custom Node.js web application in the Oracle Kubernetes Engine (OKE). We want to show how to configure the custom web application so we have a unique Single Sign On user experience. First part Follow this tutorial here explaining how to enable SSO in the web app running locally. Second part Next we make some small changes to deploy on Kubernetes. Create a Dockerfile in the nodejs folder of the cloned project with the following: console FROM oraclelinux:7-slim WORKDIR /app ADD . /app RUN curl --silent --location https://rpm.nodesource.com/setup_11.x | bash - RUN yum -y install nodejs npm --skip-broken EXPOSE 3000 CMD [\"npm\",\"start\"] Create a K8s deployment file as follows: yaml apiVersion: v1 kind: Service metadata: name: idcsnodeapp spec: type: LoadBalancer selector: app: idcsnodeapp ports: - name: client protocol: TCP port: 3000 Deploy to k8s: console kubectl apply -f service.yaml Grab the url of the new external load-balancer service created in K8s and modify the file auth.js with the appropriate values in your cloud environment: console var ids = { oracle: { \"ClientId\": \"client id of the IdCS app\", \"ClientSecret\": \"client secret of the IdCS app\", \"ClientTenant\": \"tenant id (idcs-xxxxxxxxxxxx)\", \"IDCSHost\": \"https://tenantid.identity.oraclecloud.com\", \"AudienceServiceUrl\" : \"https://tenantid.identity.oraclecloud.com\", \"TokenIssuer\": \"https://identity.oraclecloud.com/\", \"scope\": \"urn:opc:idm:t.user.me openid\", \"logoutSufix\": \"/oauth2/v1/userlogout\", \"redirectURL\": \"http://k8sloadbalancerip:3000/callback\", \"LogLevel\":\"warn\", \"ConsoleLog\":\"True\" } }; Build the container and push to a repo you have write access to, such as: console docker build -t javiermugueta/idcsnodeapp . docker push javiermugueta/idcsnodeapp Modify the IdCS application with the public IP of the k8s load-balancer service: Create a k8s deployment file as follows: yaml apiVersion: apps/v1 kind: Deployment metadata: name: idcsnodeapp labels: app: idcsnodeapp spec: replicas: 1 selector: matchLabels: app: idcsnodeapp strategy: type: Recreate template: metadata: labels: app: idcsnodeapp spec: containers: - image: javiermugueta/idcsnodeapp name: idcsnodeapp ports: - containerPort: 3000 name: idcsnodeapp Deploy to K8s: console kubectl apply -f deployment.yaml Test the app and verify SSO is working: Hope this helps! 🙂 By Javier Mugueta","categories": ["frameworks","cloudapps"],
        "tags": ["open-source","oci","always-free","nodejs","javascript"],
        "url": "/tutorials/oke-nodejs-webapp-sso-ex",
        "teaser": ""
      },{
        "title": "Run and Integrate Open Source Solutions on OCI",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/use-cases/opensource",
        "teaser": ""
      },{
        "title": "Using Oracle's Machine Translation Services for NLP Analysis",
        "excerpt":"Introduction Welcome! This article is an overview of Oracle's Machine Translation Services (MTS) and how they complement the standard open-source NLP libraries out there today. Currently, this service is focused/created with the intention of helping Oracle internal teams translate texts in a secure way (see below), so this is not a readily available product. However, there are plans for the future to make it available for OCI users. Oracle Translate is an MTS made available by Oracle’s International Product Solutions. It's beneficial to use Oracle's service over others, especially for sensitive information, because other services, such as Google Translate, are known to harvest and collect requesting data which could infringe on company confidentiality and chain of custody of protected documents. Oracle's service does not store this information, and all transactions happen in a secure Oracle network environment. Let’s explore some examples of how Oracle's MT services are different from other translation services like Google's. If you don't yet have an OCI account, you can quickly sign up for one today by registering for an Oracle Cloud Free Tier account. Afterwards, check developer.oracle.com/linux for even more Python content. Authentication Authentication is performed against Oracle Cloud resources and handled by Oracle's Identity Cloud Service (IDCS), using Base64 encoding. The two main components of Oracle Translate are a client ID and a client secret. Using these components, we can perform authentication following this flow: python Defining variables IDCSSERVER = '' SCOPE = 'urn:opc:idm:myscopes' Production endpoint BASEURL = '' ENDPOINT = '{}/translation/api'.format(BASEURL) REALTIMEENDPOINT = '' NLPENDPOINT = '' data = loadconfigfile() basic_authorization = '{}:{}'.format( data'mttranslation', data'mttranslation' ) basicauthorization = base64.b64encode(basicauthorization.encode('ascii')).decode('ascii') request_url = '{}/oauth2/v1/token/'.format(IDCSSERVER) request_headers = { 'Authorization': 'Basic {}'.format(basic_authorization), 'content-type': 'application/x-www-form-urlencoded;charset=UTF-8', 'x-resource-identity-domain-name':'idcs-oracle' } requestdata = 'granttype=client_credentials&scope={}'.format(SCOPE) response = requests.post(requesturl, data=requestdata, headers=request_headers) apitoken = response.json().get('accesstoken') Our primary objective is to obtain the necessary API token to perform operations. Note that the config.yaml file mentioned to obtain the client ID and secret should follow this hierarchical structure: yaml mt_translation: MTCLIENTID: XXXX MTCLIENTSECRET: XXXX Once we have the API token, we can perform authenticated requests against Oracle's MT Translation service. Batch Translation In this case, we will translate a document in Markdown format this one as an example. The batch translation endpoint supports translating whole documents instead of making several requests. Also, requests can be submitted for multiple target languages at a time, making it possible that one request is translatable to every available language supported by the API with just one call. python def batchtranslation(apitoken, filepath, filename): \trequesturl = '{}/files?service={}&sourceLang={}&scope={}'.format(ENDPOINT, 'mt', args.sourcelanguagecode, args.targetlanguage_code) \trequest_headers = { \t\t'Authorization': 'Bearer {}'.format(api_token) \t} \t#print(requesturl, requestheaders) \tfiles = { \t\t'file': (filename, open(filepath, 'rb')) \t} \tprint('Request URL: {}'.format(request_url)) \tprint('Request headers: {}'.format(request_headers)) \tprint('Request file: {}'.format(dict(file=files['file']))) \tresponse = requests.post(requesturl, headers=requestheaders, files=dict(file=files['file'])) \tresponse_json = response.json() \tprint(response.status_code, response.json()) \tdrop = None \twhile drop is None: \t\tprint('Waiting for translated file to be ready...') \t\ttry: \t\t\tresponse = requests.get(response_json.get('pipeline').get('status')) \t\texcept Exception as e: \t\t\tprint(e) \t \t\ttry: \t\t\tprint(response.json()) \t\t\tdrop = response.json().get('map').get('en_drop') # varies with language. I used 'en' as English in this example. \t\t\tif drop is not None: \t\t\t\tprint('File result in {}'.format(drop)) \t\texcept KeyError: \t\t\tprint('Could not find the drop.') \t\t \t\ttime.sleep(10) After executing and periodically waiting for the resulting file to be ready, we get our processed data in about 40 seconds: console Request URL: Request file: {'file': ('intro.md', )} 202 {'id': 115162, 'status': 'CREATED', 'service': 'mt', 'source': {'originalPath': 'intro.md', 'language': 'en'}, 'scope': ['fr'], 'otpinstanceid': 343337, 'links': {'self': '', 'otpstatus': '', 'otpwordcount': ''}, 'pipeline': {'id': 343337, 'status': '', 'wordcount': ''}} Waiting for translated file to be ready... {'id': 343337, 'state': 'RUNNING', 'pipelineid': 1, 'start': '2021-11-17T00:24:57Z', 'updatetime': '2021-11-17T00:24:57Z', 'end': '', 'runningtimestr': '1s', 'updatedagoinstr': '1s', 'progress': {'preprocessing': 0, 'translation': 0, 'mergeanddelivery': 0}, 'map': {'translatedlangs': [], 'deferred_langs': [], 'originalFileName': 'intro.md'}, 'messages': []} Waiting for translated file to be ready... {'id': 343337, 'state': 'RUNNING', 'pipelineid': 1, 'start': '2021-11-17T00:24:57Z', 'updatetime': '2021-11-17T00:24:57Z', 'end': '', 'runningtimestr': '12s', 'updatedagoinstr': '12s', 'progress': {'preprocessing': 0, 'translation': 0, 'mergeanddelivery': 0}, 'map': {'translatedlangs': [], 'deferred_langs': [], 'originalFileName': 'intro.md'}, 'messages': []} Waiting for translated file to be ready... {'id': 343337, 'state': 'RUNNING', 'pipelineid': 1, 'start': '2021-11-17T00:24:57Z', 'updatetime': '2021-11-17T00:24:57Z', 'end': '', 'runningtimestr': '23s', 'updatedagoinstr': '23s', 'progress': {'preprocessing': 0, 'translation': 0, 'mergeanddelivery': 0}, 'map': {'translatedlangs': [], 'deferred_langs': [], 'originalFileName': 'intro.md'}, 'messages': []} Waiting for translated file to be ready... {'id': 343337, 'state': 'RUNNING', 'pipelineid': 1, 'start': '2021-11-17T00:24:57Z', 'updatetime': '2021-11-17T00:25:21Z', 'end': '', 'runningtimestr': '34s', 'updatedagoinstr': '10s', 'progress': {'preprocessing': 100, 'translation': 1, 'mergeanddelivery': 0}, 'map': {'languages': 1, 'translatedlangs': [], 'deferredlangs': [], 'endrop': '/s115162.zip', 'originalFileName': 'intro.md'}, 'messages': []} File result in /s115162.zip And with the final result we can download the .zip file with our translated file inside the folder. Real-Time Translation In this case, we're going to test with a few examples on the real-time translation endpoint. The list of available languages is defined as: python CURRENTLY SUPPORTED LANGUAGE CODES LANGUAGES = [ \t'en', 'fr-CA', 'pl', 'sv', 'ar', 'de', 'ro', 'zh-TW', 'pt-BR', 'it', 'ru', 'nl', 'ja', 'zh-CN', 'fr', 'ko', 'es-ww' ] Coming in FY22Q3: Norwegian, Danish, Czech, Finish, Turkish & in FY22Q4: Greek, Hebrew, Thai, Ukrainian, Croatian We predefine our languages in a list and only allow these language codes as input: python import argparse parser = argparse.ArgumentParser() parser.add_argument('-s', '--source-language-code', choices=LANGUAGES, required=True) parser.add_argument('-t', '--target-language-code', choices=LANGUAGES, required=True) args = parser.parse_args() Let’s define our real-time endpoint as follows: python def realtimetranslation(api_token, data): \tassert type(data) == type(str()) \trequesturl = '{}/translate/{}/{}?s={}'.format(REALTIMEENDPOINT, args.sourcelanguagecode, args.targetlanguagecode, urllib.parse.quote(data)) \trequest_headers = { \t\t'Authorization': 'Bearer {}'.format(api_token) \t} \tprint(request_url) \t\t \tresponse = requests.get(requesturl, headers=requestheaders) \tprint(response.status_code, response.content) Note that we quote our string to avoid inconsistencies with path parameters in the request URL. After executing this code in this fashion: bash python mt_translation.py --source-language-code \"en\" --target-language-code \"fr\" We see this result: bash /translate/en/fr?s=This%20is%20an%20example 200 b\"C'est un exemple\" We were able to translate a request in about 1.5 seconds. I created a benchmark function to measure the average request/response time: python import time def benchmark(): \tapitoken = getaccess_token \tfor x in range(500): \t\tt1 = time.time() \t\trealtimetranslation(api_token, text) \t\tprint('[BENCHMARK] +{}'.format(time.time() - t1)) Which produced these results: bash [BENCHMARK] +1.5139455795288086 [BENCHMARK] +1.480928897857666 [BENCHMARK] +1.4764995574951172 [BENCHMARK] +1.5293617248535156 ... ... [BENCHMARK] +1.4901740550994873 [BENCHMARK] +1.5180373191833496 [BENCHMARK] +1.489206314086914 [BENCHMARK] +1.4964230060577393 Each request takes about an average of 1.5 seconds to finish. Docker setup I've included a Docker file that only requires a config.yaml file to run. To download the Docker file, find it in this GitHub directory. The contents of config.yaml should be like this: yaml mt_translation: MTCLIENTID: XXXXXXXXXXXXXXXXXXXXXXXXXXX MTCLIENTSECRET: XXXXXXXXXXXXXXXXXXXXXXXXXXX MT_SERVICE: mt Build the image: bash docker build --pull --rm -f \"nlp-oracle-translation\\oracle\\Dockerfile\" -t oracle_translate \"nlp-oracle-translation\\oracle\" And run it: bash docker run -it -p 443:443 oracle_translate -s en -t fr -x \"This is an example\" Example run: bash λ docker run -it -p 443:443 oracle_translate -s en -t fr -x \"I love you\" 200 Je vous aime ! How can I get started on OCI? Remember that you can always sign up for free with OCI! Your Oracle Cloud account provides a number of Always Free services and a Free Trial with US$300 of free credit to use on all eligible OCI services for up to 30 days. These Always Free services are available for an unlimited period of time. The Free Trial services may be used until your US$300 of free credits are consumed or the 30 days has expired, whichever comes first. You can sign up here for free. License Copyright (c) 2021 Oracle and/or its affiliates. Licensed under the Universal Permissive License (UPL), Version 1.0. See LICENSE for more details. Written by Ignacio Guillermo Martínez @jasperan, edited by GreatGhostsss By Ignacio Martínez","categories": null,
        "tags": ["python","oci"],
        "url": "/tutorials/oracle-translation-nlp-analysis",
        "teaser": ""
      },{
        "title": "Projects",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/pages",
        "teaser": ""
      },{
        "title": "Personal Cloud Services",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/use-cases/personal",
        "teaser": ""
      },{
        "title": "Polyglot Application Observability",
        "excerpt":"Pinpoint (developed by Naver, South Korea’s largest web search engine company) is an APM (Application Performance Management) solution used by many organizations around the globe and actively used internally by Naver, where Pinpoint monitors over 10 billion transactions per day. This allows tracing transactions and data flows between multiple software components and identifies problematic areas along with potential bottlenecks. Integration The Pinpoint team developed a GraalVM agent using the Truffle API. GraalVM provides a faster runtime for applications and access to Truffle APIs to get access to the platform for enhancing interoperability. The integration between GraalVM and Pinpoint (visualized below) shows the auto-instrumentation of a polyglot application written in Java and Node.js with a MySQL database backend. To start using PinPoint and GraalVM for your applications, clone the following project from GitHub: $ git clone https://github.com/RoySRose/pingraalPrerequisite GraalVM preinstalled. Create your first node 1. Create the instrument jar $ mvn clean install -DskipTests=true2. 2. Run server.js $ node server.js3. 3. Run first node.js file: $ ./simpletool node helloworld.js After running these commands, you should be able to access the PinPoint dashboard. Results The results are displayed in three steps: 1. The integration allows you to collect and visualize performance metrics of your application, such as heap/non-heap memory usage, CPU usage, active threads, response time, and others. 2. From the Callstack view you can dig deep into distributed call stacks of a particular transaction from multiple servers in one view: 3. The inspector collects the following information: - Heap/Non-Heap Usage - GraalVM/System CPU usage - Transactions per second - Active Threads - Response Time - Open File Descriptors - Direct/Mapped Buffer - Data source Summary This collaboration was done thanks to the efforts of the GraalVM Labs Engineering team (led by Thomas Wuerthinger) and also Roy Kim from Naver Labs and his Pinpoint team. The integration between GraalVM and Pinpoint helps us to see and understand the need for Polyglot Application Observability. By Amitpal Singh Dhillon","categories": ["clouddev"],
        "tags": ["graalvm"],
        "url": "/tutorials/polyglot-application-observability",
        "teaser": ""
      },{
        "title": "Mr. Stark's Briefcase and ProximaSafe",
        "excerpt":"Welcome to ProximaSafe, an evolution of Smart Cities! Wait, what on Earth is Proxima and why is it Safe? Great questions. In 2018, Oracle developed ProximaCity, a physical product showcasing how cloud services can optimize the resource-intensive processes which make communities functional. This was an exciting outgrowth of Oracle's Smart City initiative. It was a big hit. Then there was a pandemic. ProximaCity needed to adapt from a sprawling physical model to something more streamlined. Optimization was still a priority, but safety came first. Enter ProximaSafe. ProximaSafe extends the goals of the old ProximaCity, but uses data to proactively defend against biological threats (monitoring environmental parameters and ensuring sanitation stations, for example). What to expect from this series This series walks you through creating a personal development lab which you can build at home, or just read to follow along. The theme of the lab, its use cases and sample data, borrows from the COVID pandemic. Using APIs, edge components, and sensors, you'll use OCI services to build safety-related use cases. The goal here is to enable teachers, students, developers, and tinkerers to use and improve this \"portable lab\" and develop new safety-oriented use cases to explore and expand its necessary back-end cloud services. The Briefcase Dream I've always been a fan of Marvel superheroes comics, long before the successful movie franchise known as MCU. Actually, around 1972–1975, I could find several Marvel comics carefully translated and lettered in my native language by visiting my friendly (actually, he wasn't) news vendor, strategically placed in the market street corner few hundred yards from where I lived. The hard limit — at the time — was the number of physical coins I was allowed to spend within a month without incurring into Mum's inevitable semi-verbal reproach. Few years later, my tastes and interests turned to the Golden Age Sci-Fi (Asimov and all of the Futurians, Heinlein, Bradbury ). The news vendor corner was always there, my monthly allowance was a tad more generous and I could smuggle sci-fi books in my school bag without arousing suspicion. With a notable exception: pooled with the books, a comic, the Iron Man episode where the iconic Mr. Stark briefcase was introduced to my 'magnets and miracles' world. I wish I had that briefcase available in a number of situations, getting a bad grade or running into the inevitable school bully: my curiosity about the relationship between miniaturization and technology remains a fixed point even today. Then, decades later, the briefcase idea surfaced again while having fun with an iteration of the Oracle's Proxima City model made by the Italian Innovation Team, a whimsical (as it's been called) construction made with Legos, Arduinos, sensors and Oracle Cloud Infrastructure services. During the 2020 lockdown we had no access to the city model, installed in the Customer Visit Center in Rome. I was struggling to find a way to show a functioning demo either via Zoom or bringing some artifacts into customer premises (no Star Trek transporter available, so far), and urged to prepare something smaller than a city model but powerful enough to show the concept. \"I should definitely do it\" was my inner morning echo while sipping my favorite daily americano brew. Changing the model to be portable also meant changing the use cases perimeter: the year 2020 required — for obvious reasons — a rapid transition from Smart City services to an extended and appropriate Safe City scenario. Requirements were clear: small enough to fit in my backpack, programmable, made with off-the-shelf components, easily connectable to OCI services, and capable to show the end-to-end message flow by means of physical interaction. The stretch goal was to enable teachers, students, developer and tinkerers to improve this 'portable lab' and develop new use cases oriented to safety, exploring and expanding the needed back-end cloud services. From Briefcase to Cloud, and Back Here's the link (Note: insert link) to the article series about what I call ProximaSafe, showing how to build some Cloud and Edge capabilities to make a testbed for safety-related applications (included, but not limited to) using a number of technologies and tools, such as: - Oracle Golden Gate Streaming Analytics, to analyze data in motion and detect anomalies and/or errors - OCI Streaming, the messaging backbone - OCI Functions, the serverless infrastructure used to enable the upstream messages to Edge - the M5Stack family of development boards, acting as Edge Components enabled to publish and subscribe to IoT topics - a Raspberry Pi 4, the inevitable Edge gateway taking care of communication between sensors & boards to Cloud — and viceversa - a suitable battery pack to power the whole stuff. The overall architecture designed for this mobile lab includes the representation of the edge (the boards inside the briefcase) and the communication flow edge -> OCI -> edge with return messages containing anomaly detections and alarms. So, given a number of messages that can be recognized as an uptrend/downtrend pattern or a number of messages within a time frame (maybe few seconds), the stream analysis within OCI may (or may not) throw an alarm, sent back to the edge to make something happen. Here's a short YouTube video. Three chapters, a common goal I've divided the whole path in chapters that will be published every fortnight or something, trying to avoid the 'War and Peace' syndrome (I'm not sure I achieved that). Here's the breakdown of the articles: - In chapter 1, we'll set up OCI resources to implement an even-driven engine, capable of analyzing the stream of events (and data) from the edge - In chapter 2, we'll set up the feedback mechanism to return alarms to the edge in order to make something happen locally In the final chapter (three is the magic number), we'll select and program the edge components (thingies in our briefcase) and we'll design some pipelines in the cloud corresponding to a number of use cases. It was fun to build it. The boy I used to be would be proud of it. The briefcase needs more engineering. I became no Tony, no superpowers, no millionaire, and — overall — my once dark goatee has become white (alas!), so the distant reflection of that boy slipping comics and books in the backpack while wondering about future technology maybe it's gone forever. My morning back pain reminds me that, indeed, it is. Well, here's the link to the first chapter, for it is time to think about the next thing to write. I still daydream about that magical briefcase, though. By ","categories": ["arvr"],
        "tags": ["oci","iot","streaming","serverless","rpi","full-stack"],
        "url": "/tutorials/proxima-safe/proxima-overview",
        "teaser": ""
      },{
        "title": "Building an API in Rust and hosting on Oracle Cloud Infrastructure",
        "excerpt":"It's impossible to work in cloud services without hearing about Rust, the system's programming language from Mozilla. It's used all over the internet by companies like Cloudflare, Facebook, and Discord. It's a strongly-typed yet flexible language that emphasizes strict guidelines around memory usage, making it blazing fast and resource lean. The language also focuses on developer productivity, which is evident in their tooling and [package ecosystems]. Rust is being used in places where C worked best, due in part to its recognizable syntax. It is also replacing some higher-level languages, like Node and Ruby. In this blog post, we will build a small API microservice using Rust. APIs are generally used in situations where services---a server and a client---must communicate with each other. Our API will represent the backend for an inventory bookstore, where books can be added, fetched, and removed. This app will be hosted on Oracle Cloud Infrastructure (OCI). OCI enables cloud-native containers to run in highly secure and performant environments that are also fully managed. This makes it a perfect match to host Rust applications, since the platform and the language are well-suited to solve similar problems. Prerequisites Before getting started, you'll need to install several software packages. First, you'll need Rust. Regardless of your operating system, the Rustup script is guaranteed to work and installs all the necessary tools you need to start building a Rust application. You'll also need to install Docker to test the app containerization locally. In order to deploy the app online, you'll also need a free OCI account. Getting started with the Rust code As with many programming languages, Rust comes with its own package management system called Crates. To start building our app, we will want an HTTP web framework to do the heavy lifting for us. There are many to choose from, but we'll use warp, as it's both popular and performant. Just as Node uses package.json to manage packages, Rust uses Cargo.toml. Create a new directory within which you can start building this project, and create a file named Cargo.toml in it. Paste these lines into that file: [package] name = \"server\" version = \"0.1.0\" license = \"MIT\" edition = \"2018\" [dependencies] tokio = { version = \"1\", features = [\"full\"] } warp = \"0.3\" serde = { version = \"1\", features = [\"derive\"]} serde_json = \"1.0\" Here, we're defining the general metadata of our package. We're specifying that we want to create a binary executable named server; we also have a list of dependencies that our project needs, including warp. Next, create a directory called src, and a directory called bin within that. Then, create a file called server.rs, and paste these lines into it: rust ![deny(warnings)] use warp::Filter; [tokio::main] async fn main() { // Match any request and return hello world! let routes = warp::any().map(|| \"Hello, World!\"); warp::serve(routes).run(([127, 0, 0, 1], 3000)).await; } We've defined a very basic Warp server, which will run on http://127.0.0.1:3000. When a user visits that page, they'll see a greeting. Go ahead and type cargo run on the terminal. Cargo will download all the dependencies you defined, then it'll compile them together with the server.rs file to create an executable. (All of that in just one command!) When it's finished, you'll see the following message: console $ cargo run Finished dev [unoptimized + debuginfo] target(s) in 0.06s Running target/debug/server Navigate your browser window to http://127.0.0.1:3000, which should show the greeting, thus confirming that the initial project setup has worked! Setting up an API Now that we have verified that our server runs correctly, it's time to build a more proper API. We want our API to get a list of books, add a new book, and remove a book. In a future blog, we will integrate with a backend database for storing and querying the data. To keep things simple in this tutorial, we'll just fake the data store by defining an array to store all of our books. Let's start by defining the structure of a Book. Rust has the concept of structs, which are akin to lightweight classes. Here's an example of what our Book class would look like: (All the code below replaces the code in the server.rs file.) rust use serde::{Deserialize, Serialize}; [derive(Clone, Serialize, Deserialize)] pub struct Book { title: String, author: String, year: u32, } We can then modify our main function to immediately set up a basic catalog of books that follow this structure. We will use a vector (which is like an expandable array), and store the list in memory: rust use std::sync::Arc; use tokio::sync::Mutex; pub type Db = Arc>>; [tokio::main] async fn main() { let mut book_catalog: Vec = Vec::new(); book_catalog.push(Book { title: \"The Hitchhiker's Guide to the Galaxy\".to_string(), author: \"Douglas Adams\".to_string(), year: 1979, }); book_catalog.push(Book { title: \"The Restaurant at the End of the Universe\".to_string(), author: \"Douglas Adams\".to_string(), year: 1980, }); book_catalog.push(Book { title: \"Life, the Universe and Everything\".to_string(), author: \"Douglas Adams\".to_string(), year: 1982, }); book_catalog.push(Book { title: \"So Long, and Thanks for All the Fish\".to_string(), author: \"Douglas Adams\".to_string(), year: 1984, }); book_catalog.push(Book { title: \"Mostly Harmless\".to_string(), author: \"Douglas Adams\".to_string(), year: 1992, }); let db = Arc::new(Mutex::new(book_catalog)); So far, so good? Right on! The next task is to add routes to this API. There are a number of patterns to implement this, but the one suggested by Warp takes a two-pronged approach: First, the routes are defined, and then, the implementation of those routes is defined. This way, the implementation can change, but the route information can be considered static and stable. Let's go ahead and define these routes. We'll drop the code first, and then provide a closer examination: rust mod filters { use super::Db; use super::Book; use super::handlers; use warp::Filter; use std::convert::Infallible; /// The routes, combined. pub fn constructbookroutes( db: Db, ) -> impl Filter + Clone { routegetbooks(db.clone()) .or(routepostbooks(db.clone())) .or(routedeletebook(db.clone())) } /// GET /books pub fn routegetbooks( db: Db, ) -> impl Filter + Clone { warp::path!(\"books\") .and(warp::get()) .and(with_db(db)) .andthen(handlers::getbooks) } /// POST /books with JSON body pub fn routepostbooks( db: Db, ) -> impl Filter + Clone { warp::path!(\"books\") .and(warp::post()) .and(json_body()) .and(with_db(db)) .andthen(handlers::createbook) } /// DELETE /books/:id pub fn routedeletebook( db: Db, ) -> impl Filter + Clone { warp::path!(\"books\" / u64) .and(warp::delete()) .and(with_db(db)) .andthen(handlers::deletebook) } pub fn with_db(db: Db) -> impl Filter + Clone { warp::any().map(move || db.clone()) } pub fn json_body() -> impl Filter + Clone { // When accepting a body, we want a JSON body warp::body::contentlengthlimit(1024 * 16).and(warp::body::json()) } } This might look like a lot of code, but we're really just redefining similar path structures in composite functions. Let's take a look at the first function: rust /// The routes, combined. pub fn constructbookroutes( db: Db, ) -> impl Filter + Clone { routegetbooks(db.clone()) .or(routepostbooks(db.clone())) .or(routedeletebook(db.clone())) } The only responsibility of constructbookroutes is to assemble a list of all the known routes. To make use of this, we must go back into our main function and change the final lines to look something like this: rust warp::serve(filters::constructbookroutes(db)) .run(([127, 0, 0, 1], 3000)) .await; Here, we're telling the warp server what our routes are, and passing along the in-memory DB we've created. Moving on to the next function: rust /// GET /books pub fn routegetbooks( db: Db, ) -> impl Filter + Clone { warp::path!(\"books\") .and(warp::get()) .and(with_db(db)) .andthen(handlers::getbooks) } We can ignore the function signature, as that's largely Warp specific requirements. Instead, let's look at the function line by line: - We're creating a path called /books - This path responds to GET requests - It makes use of the database we're passing it - And the actual logic is stored in a to-be-written function called get_books, within the handler's namespace The other two routes define POST and DELETE, which follow a very similar pattern. Let's move on to the logical implementation: rust mod handlers { use super::Db; use super::Book; use std::convert::Infallible; use warp::http::StatusCode; pub async fn get_books(db: Db) -> Result { let books = db.lock().await; let books: Vec = books.clone(); Ok(warp::reply::json(&books)) } pub async fn create_book( book: Book, db: Db, ) -> Result { let mut books = db.lock().await; books.push(book); Ok(StatusCode::CREATED) } pub async fn delete_book( id: u64, db: Db) -> Result { let mut books = db.lock().await; let mut iter = 0; let len = books.len(); books.retain(|_book| { let mut keep = true; if iter == id { iter += 1; keep = false; } iter += 1; keep }); // If the vec is smaller, we found and deleted a book! let deleted = books.len() != len; if deleted { // respond with a 204 No Content, which means successful, Ok(StatusCode::NO_CONTENT) } else { Ok(StatusCode::NOT_FOUND) } } } These routes don't do much! The GET function prints a list of all the books available; the POST function takes a new book and adds it to the vector; and the DELETE function removes a book based on its index in the vector. (See the end of the article for the GitHub containing all this code.) Go ahead and type cargo run in the terminal. Your project will recompile, and when it's finished, go ahead and enter curl http://localhost:3000/books in another terminal window. You should see a list of your books, and you can note that the other HTTP verbs work, too! Dockerizing the Rust server Now, we're ready to take this project and containerize it via Docker. Docker has evolved over the years to make this process extremely simple. The entire Dockerfile fits in less than a dozen lines of code: docker Using the Rust official image... FROM rust:1.60 Copy the files in your machine to the Docker image... COPY ./ ./ Build your program for release... RUN cargo build --release And run the binary! CMD [\"./target/release/server\"] We'll need to build the Docker container, which we can run with this command: console docker build -t server . And finally, we'll need to start the Docker container, which can be done like this: console docker run -p 3000:3000 --rm --name server_docker server* If you haven't seen the command for running a Docker container, then it's worth pointing out several things about the CLI flags. First, we're exposing port 3000 in our container to our localhost, as 3000. We could change these values if there were port number conflicts between our host machine and the Docker container, however, in this tutorial, that's not necessary. We're also naming our Docker as server_docker. This will make it easier to distinguish between logs and other systems' processes. After the Docker run command executes, try running the curl command again. You might see the following error: curl: (7) Failed to connect to localhost What does it mean? Well, when Docker launches the container, it assigns it its own IP address. And when the Rust server points to 127.0.0.1, it's opening a connection to itself, not the broader public world. The fix for this is to change the IP address used in our Rust code, from 127.0.0.1 to 0.0.0.0: rust warp::serve(filters::constructbookroutes(db)) .run(([0, 0, 0, 0], 3000)) .await; Stop the server by executing the docker stop command on your terminal. Then, rebuild and rerun the container. If you try the curl command again, you should see the API working as expected! Deploying to Oracle Cloud Infrastructure (OCI) At last, we reach the end of our tutorial: hosting our wonderful API online so that it's available across the internet. This is the easy part! In order for the OCI to load your Docker image, you will first need to push it to the Oracle Container Registry. You'll first need to know your Docker container's image ID to do that. Run the following command to get that information: docker images You should see a list like the following: console REPOSITORY TAG IMAGE ID CREATED SIZE server latest 8f2569fb8987 25 hours ago 2.83GB Take note of that image ID; we will use it when uploading the image to the Oracle Cloud Infrastructure. Next, follow these steps to learn more about performing the following actions: - docker login $REGIONKEY.ocir.io, which will log you into the Oracle Cloud Infrastructure Registry region you're using. Note that $REGIONKEY is determined by whichever region your account is using; see this list for the key which matches your region. - When prompted, your username is in the format of /. The tenancy name can be found under the Tenancy Details section of your administrative profile. - Next, type docker tag $IMAGEID $REGIONKEY.ocir.io/$TENANCY_NAME/server:latest, where: - $IMAGE_ID is the image ID provided by Docker. (In this example, it's 8f2569fb8987.) - $REGIONKEY and $TENANCYNAME are the same values provided earlier to log in. - Finally, type docker push $REGIONKEY.ocir.io/$TENANCYNAME/server:latest Our server image has now been uploaded onto the Oracle Cloud Infrastructure Container Registry; the final step is to instruct your Oracle Cloud service to pull that image and make use of it. We could do the longer process of setting up Kubernetes, but for such a small app, we can move much quicker if we simply load the image onto the VM directly. Let's go ahead and create an OCI Compute instance on which to run our container. Refer to the documentation here to learn more about how to do so...be sure to create your Virtual Cloud Network (VCN) with at least one public subnet (hint: the VCN Wizard is a cinch). Be sure to download your SSH private key and take note of your public IP address. After the instance is provisioned, we can pull our Docker image onto it. Follow these directions to learn how to SSH into your VM instance. Note that you may need to install Docker on your instance. You can verify whether this is required (or not) by entering the commands from this guide into your instance. Simply checking via the docker version command is enough to confirm Docker's presence: console $ docker version Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg. Client: Podman Engine Version: 4.1.1 API Version: 4.1.1 Go Version: go1.17.12 Built: Thu Aug 4 02:48:00 2022 OS/Arch: linux/arm64 With Docker installed, we can pull our image from the Oracle Cloud Infrastructure Registry. First, log in to the OCI Registry using the same credentials as before: console $ docker login $REGION_KEY.ocir.io Then, pull the image using the tag identifier which was created: console docker pull $REGIONKEY.ocir.io/$TENANCYNAME/server:latest Finally, we will need to make one essential security change, which is opening up a single network port to our machine so that the app is publicly accessible. To do that, you'll need to set up an Ingress rule and point it to port 3000 (HINT: this is done in the Networking configuration, within the VCN previously created). Here's a final screenshot of what setting up the Ingress rule looks like: Now that we've opened a rule on our network to allow ingress traffic over port 3000, we just need to open port 3000 in our local firewall to allow traffic through; run these two commands on the VM to do so: console $ sudo firewall-cmd --permanent --zone=public --add-port=3000/tcp $ sudo firewall-cmd --reload Once that's finished, we can run the Docker image with the same command we used when running it on our development machine: console docker run -d -p 3000:3000 --name serverdocker $REGIONKEY.ocir.io/$TENANCY_NAME/server:latest Voila! You can now call curl http://$PUBLICIPADDRESS/books and query the API, just like we did locally! Notice that we didn't need to install Rust or copy over any packages; that was all taken care of by Docker. Learning more We've only scratched the surface of the features OCI offers for containerized applications. There are many more features catered to modern DevOps practices, including: - Reliable database uptime and migrations - Observability and monitoring - Security and app isolation The speed and performance capabilities of Rust are also paired nicely with the availability and response speeds which are provided to apps running on OCI. It's a win-win for dev, ops, and everything in between. All the code used in this post can be found in this gist. For more information on how OCI can help you, be sure to check out our docs! By Oracle Developers","categories": ["clouddev","cloudapps"],
        "tags": ["open-source","devops","get-started","back-end","rust"],
        "url": "/tutorials/rust-on-oci/rust-building-an-api",
        "teaser": ""
      },{
        "title": "Getting started with Rust",
        "excerpt":" Welcome to a series of posts on \"Everything you need to know about Rust.\" We'll cover a lot over the course of the next few articles. By the end, you should be comfortable with writing and deploying your own Rust project and be ready to jump into advanced topics. This first article will introduce you to the motivations behind Rust, why you might use Rust, and a walkthrough of a simple Rust application deployed to Oracle Cloud Infrastructure. Later articles will cover the ecosystem and full suite of Rust tools, publishing Rust libraries (known as \"crates\"), and best practices and resources for becoming a Rust expert. Let's kick it off with a few fundamentals. What Is Rust? Rust is a general-purpose programming language developed specifically to run blazingly fast, enforce type safety, and provide concurrency, all in an easier-to-use language than its predecessors, such as C and C++. It's a relatively new language, with version 1.0 released in 2015. Rust enforces memory safety through its unique ownership system, which ensures that all resources are properly initialized and cleaned up, preventing you from using a resource before it has been initialized or after it has been freed. By enforcing such strict memory guarantees, Rust programs don't need to be garbage-collected. Rust also has strong support for multithreading. In other languages, multithreading can lead to concurrency problems such as resource deadlocks. But by using the ownership system, Rust tracks and catches potential data races at compile time, making it much easier to write safe, concurrent code. Many of Rust's goals are closely aligned with Oracle Cloud Infrastructure (OCI), which makes the two technologies a great pair if you're looking to provide greater security, performance, and flexibility to your development projects. Why Use Rust? While Rust is definitely a general-purpose language, there are some common use cases where it truly shines. First are networking and embedded devices. One of the main motivators for Rust's development was to create a language that would be suitable for use in systems programming contexts, such as operating systems, device drivers, and embedded systems. Since well-written Rust applications have a small footprint and writing safe, concurrent code is a basic part of developing in the language, Rust is perfect for low-resource deployments that are often found in both of these spaces. Next, since Rust became prominent when Mozilla sponsored it during a high point of web development, it should be no surprise that Rust has great web-related applications. Whether you need to have all the benefits of the language on the front end or back end of your web stack, you can use Rust. - For front-end functionality that needs high performance, developers often build in Rust and compile to WebAssembly to run in a browser. With client-side code that is known to be safe, you can be more certain of success for whatever high-performance tasks you need. - For back-end applications, Rust has several web frameworks, such as Actix Web and Rocket, that can provide the tools and libraries you need to quickly spin up fast and secure web applications that take advantage of Rust. Finally, Rust is also great for building command-line tools. With its easy distribution model (via crates.io, its package distributor) and the ability to run on many different CPU architectures, Rust allows safety and performance without having to worry about the intricacies of its deployment environment. How Is Rust Different from JavaScript? You may be wondering how Rust is different from JavaScript or other scripting languages. There are a few key differences to consider. - First, Rust is a statically typed language, while JavaScript is dynamically typed. This means that in Rust, you have to declare the types of variables ahead of time, while in JavaScript, you don't need to do this. - Second, Rust is compiled to native code, while JavaScript is interpreted by a virtual machine. This means that Rust programs will generally run faster than JavaScript programs, but they may take longer to compile. - Finally, Rust has several features that make it more suitable for use in systems programming contexts than JavaScript. These include its focus on safety and security, its low-level control over memory management and data layout, and its support for efficient code. Let's See It in Action Now that we have some background, let's get Rust up and running on an OCI instance and see how easy it can be to get started. If you don't have an OCI account, the first step is to create one. For simplicity and ease of following along, we'll start with an Always Free Instance and create a simple Actix web app. This app will be very similar to a demo Express.js application. It's a single-file app that responds with a greeting on a couple of GET routes and will echo the payload sent via POST on another route. Everything we use here is free, so no worries about costs. In order to bake a Rust app, you must first invent a computing universe If you really wanted to go quickly, you could start by creating a bare Linux instance on OCI, install Rust, then build your Actix app. However, if you want to be able to access your app from the internet, or do more than just write to the local drive, there's a bit more involved. We'll walk through the relevant steps below, but if you need more detailed information, check [this more thorough tutorial](https://docs.oracle.com/en-us/iaas/Content/GSG/Reference/overviewworkflow.htm#TutorialLaunchingYourFirstLinux_Instance). Initial setup Our first step is to choose a compartment where we'll put this sample app. If you've never created a compartment before, you'll have a default \"root\" compartment for your account, but it's a better idea to create a new one. We won't be creating much for this article, but having a dedicated compartment will make cleanup easier. It's simple to create one, so we'll make a sandbox compartment from the Identity & Security menu. Choose \"Compartments\" from the menu for Identity & Security Once you get to the list of compartments, you can make a new one and fill in these details (or something like them), but keep the Parent Compartment set to your root compartment. Now that you have a compartment that won't cause problems for other things in the rest of your account, let's get to work. An internet accessible network In any new OCI account, you'll start with a purely private network. This means that any resources you create will be able to talk to one another, but by default, none of them will be accessible to the rest of the internet. Since we're building a web app, that simply won't work for our needs, so we're going to need to create an internet-connected Virtual Cloud Network. OCI is designed to get us up and running quickly and offers just what we need through a convenient wizard on the networking page. If you choose our sandbox compartment from the sidebar and click the \"Start VCN Wizard\" button, you'll see something like this. Start the wizard and pick a name for your network (e.g. RustNet or whatever you feel like). Leave the CIDR blocks and other settings alone, then click next. You should see a number of gateways that will be created, as well as some security lists and route tables. Click create and watch OCI do its work. While you're here, open the Default Security List from the Security Lists tab and add a rule to allow traffic on port 8080 to your VNC. The settings should look like this: Building our compute instance Now we get to make our computer for building a Rust app. We'll use the basic defaults (they're set to use the Always Free options), and we'll download the SSH key that's generated with our instance. The defaults are good enough for our purposes, so once you've given your instance a name and have checked that your VCN is being used for this instance, just click the Create button. Once your instance has been provisioned, grab the IP address and get ready to start working in the cloud! Getting up and running with Rust By default, Rust is not installed in OCI instances. However, just as OCI is designed for ease of use and setup, Rust is constantly striving for developer efficiency, and the setup process is no exception. The rustup command is designed to make setting up Rust as painless as possible. Grab the installation command from the docs, and run it on your OCI instance. When you see \"Rust is installed now. Great!\" you're ready to go, with a full Rust tool suite installed. These tools include rustup itself, which will allow you to maintain your Rust tools in the future, rustc, which is the Rust compiler, and cargo, which serves as Rust's package manager. Cargo has many other uses too, including making new Rust apps and libraries from scratch. Make sure to check if there are any instructions to get your current shell configured properly. If there are, run those commands. If you like, you can verify that everything was installed correctly with: console rustc --version This should give you the most recent and stable version of Rust. Configuring our app Getting a new Actix app going is pretty straightforward. You start, as you do with many Rust projects, with: console cargo new hello-rust Cargo is Rust's dependency and app manager. It's very similar to npm for Node, Ruby's bundler tool, or PyPI for Python. In our case, it only creates a couple things for us. In a new directory called hello-rust, Cargo will have created the following structure: hello-rust\\ ├── Cargo.toml\\ └── src\\ └── main.rs In Rust, packages are called crates. The Cargo.toml file serves as a package manifest) and the src/main.rs file is our main source file. We won't need anything other than these two files for our Actix app. In the Cargo.toml file, add the following line to the \\[dependencies\\] section: [dependencies] actix-web = \"4\" Save your Cargo file and open the src/main.rs file. Replace what's there with: rust use actix_web::{get, post, web, App, HttpResponse, HttpServer, Responder}; [get(\"/\")] async fn hello() -> impl Responder { HttpResponse::Ok().body(\"Hello, Rust!\") } [post(\"/echo\")] async fn echo(req_body: String) -> impl Responder { HttpResponse::Ok().body(req_body)\\ } async fn manual_hello() -> impl Responder { HttpResponse::Ok().body(\"Hey there!\") } [actix_web::main] async fn main() -> std::io::Result { HttpServer::new(|| { App::new() .service(hello) .service(echo) .route(\"/hey\", web::get().to(manual_hello)) }) .bind((\"0.0.0.0\", 8080))? .run() .await } This might feel like a lot, but with just the actix-web dependency and these few lines of code, we now have a fully functioning Rust web app. This code, in the main function, is building an HTTP server that attaches to port 8080 of your instance. That server has two \"services\" defined as asynchronous functions which will respond with a greeting at the root route with a GET or echo back a response at the /echo route. We've also established a third route manually with the .route call for the manual_hello function. Because all of these are defined using the async keyword, the app can respond to multiple requests at once as long as there are threads available. What's nice is that we know the app is not going to have any memory safety issues because it compiles without errors. We've already opened a rule on our network to allow traffic over port 8080, but we also need to open port 8080 in our firewall to allow traffic through, so run these two commands: console $ sudo firewall-cmd --permanent --zone=public --add-port=8080/tcp $ sudo firewall-cmd --reload Return to the root of your app and run: console $ cargo run Once you do, your application will build and start running. If you grab the IP address of your instance and append port 8080, you should be able to get a \"Hello, Rust!\" from your browser. If you'd like to be a little more adventurous, you could send a POST to the /echo endpoint and get the request body you send echoed back. We've also manually set up a special hello at the /hey route. All of this in under 30 lines of Rust! Conclusion There's a lot more to learn, but hopefully you can see the power and ease of using Rust and OCI together. Future articles will dive deeper into Rust and the ecosystem. In the meantime, you might also enjoy getting a taste of what Rust can do by checking out The Rust Programming Language, affectionately referred to by Rust programmers as \"The Book.\" Alternatively, you can keep poking around with some of the amazing things you can do on OCI with the free trial credits you got for creating a new Oracle Cloud account. By Oracle Developers","categories": ["clouddev","cloudapps"],
        "tags": ["open-source","devops","get-started","back-end","rust"],
        "url": "/tutorials/rust-on-oci/rust-getting-started",
        "teaser": ""
      },{
        "title": "Building Safe, Fast, and Manageable Applications with Rust and Oracle Cloud Infrastructure",
        "excerpt":"Introduction No question---building and managing modern technology is complex. Datasets can be massive, expensive to compute, and require highly-performant processes to scale. Data security is critical to business sustainability and is difficult to execute. Architecting systems that can handle modern scale requires intelligent network load balancing and hardware availability. When you combine all of those challenges (and more!), it becomes a daunting task to imagine how to build an app that simply works. Choosing the tools that solve these issues becomes a crucial question for developers. In this piece, we'll make the case that by using Rust---a programming language that is fast, secure, memory-safe, and supports complex abstractions---in combination with OCI---a Platform-as-a-Service (PaaS) that provides fine-grained control, excellent security measures, and is optimized for performance---developers can have just the right tools to help tame the complexity of building modern software. We will start with brief backgrounds on Rust and OCI. Then, we'll dive into several use cases of how Rust and OCI are an excellent solution for modern software applications. Why Rust? Rust is an open source, general-purpose programming language optimized for safety, concurrency, and speed. Rust offers low-level memory access and, as such, can be used for systems programming. It also has a rich set of built-in types and interfaces that allow for code organization and reuse typically associated with higher-level languages. Safety - Rust is considered memory safe. This means it protects developers from writing code that results in bugs related to memory access, such as dangling pointers, null pointers, buffer overflows, and so on. These bugs are often exploited as security vulnerabilities. Rust achieves memory safety through features such as a strong type system and the famous Borrow Checker. Concurrency - Rust has built-in support for concurrent and parallel programming. Developers can easily spawn threads and pass messages between them. A number of abstractions, such as Atomic Reference Counter (Arc) and Tokio, protect against many of the classic dangers involved with concurrent programs. Combined with Rust's memory safety guarantees, this allows the compiler to catch many types of bugs before the code runs. The Rust team calls this \"Fearless Concurrency.\" Speed - In terms of performance, Rust is comparable to other lower-level languages like C and C++. Rust is compiled to native code, which results in a low memory footprint. Developers have access to many features that inform the compiler how types will be laid out in memory. Rust's type system also allows for Zero Cost Abstractions which means that many of the higher-level constructs do not come with a cost at runtime. Why Oracle Cloud Infrastructure (OCI) [Oracle Cloud Infrastructure] (OCI) is a set of complementary cloud services that enable you to build and run various applications and services in a highly available hosted environment. OCI provides high-performance compute capabilities (as physical hardware instances) and storage capacity in a flexible overlay virtual network that is securely accessible from your on-premises network. By abstracting away the complexities of safely managing infrastructure, OCI enables businesses to deploy complex, modern applications quickly and for a fraction of the cost of staffing an internal IT team. Now that we have a little background, let's look at three services offered on OCI, and explore how Rust works well with these features. Web and Cloud Native Applications A large percentage of applications today are deployed as cloud-native apps. OCI provides best-in-class tools for easily building, hosting, and maintaining these web and cloud-native applications. When deploying these apps, containerization has become the industry standard. OCI's [Container Registry] allows developers to store container images and then securely access those images at any time. CI/CD pipelines can be designed to build and push images to Container Registry automatically. Containers can then be built from stored images and orchestrated via OCI [Container Engine for Kubernetes]. Once the code is live, OCI offers resource monitoring with its Events Service. Cloud-based applications built on this type of infrastructure can take many forms, such as ecommerce websites, REST APIs, or backend systems managing IoT devices. Rust Regardless of the specifics of the business domain, security, usability, performance, and time to market are critical to most apps. As seen above, Rust is designed to handle these challenges, especially with cloud-native applications. Safety - Web and cloud-native apps have a much broader attack surface than isolated systems. Not only is the app publicly available, but when building software for the internet, developers typically use frameworks and libraries to solve common problems. This means that the attack surface extends to code that was authored by someone else. Rust's memory safety greatly enhances the security of an application and the software it depends on. Fewer vulnerabilities also mean fewer security updates, which means less maintenance and reduced costs. Concurrency - Poor app performance leads to higher costs, lower user retention, and the inability to execute expensive computations. This is especially true with cloud-based software that may deal with concurrency issues from socket connections to monetary transactions. Rust's promise of \"Fearless Concurrency\" allows developers to take advantage of OCI infrastructure to build correct and performant concurrent software. Speed - In addition to the built-in Rust features that increase execution speed, the Rust ecosystem also has several frameworks that encourage rapid development. ORMs exist for data modeling and persistence layer abstraction. Some notable web frameworks that are gaining popularity include Nickel.rs or Actix, both of which can take advantage of projects like Diesel for the database abstraction layer. Serverless Functions Serverless functions are short-running processes that perform discrete tasks, such as processing image data, persisting data to cloud storage, or sending emails. They are usually invoked in response to events and can be chained together to accomplish tasks of arbitrary complexity. When the code is invoked, resources are automatically allocated to achieve configurable performance characteristics. When the code stops running, resources are deallocated. This allows for a cheap and sustainable choice over other alternatives. OCI supports serverless functions with its Cloud Functions. Cloud Functions conform to Fn Project standards and utilize OCI Cloud Events to trigger the invocation of functions. Because Cloud Functions are based on standards from the Fn Project, the code is portable between OCI and other infrastructures that also conform to Fn Project standards. Cloud Functions has built-in support for several popular programming languages, such as Go, Node, and Ruby. Developers can use the Fn Project CLI to create, configure, and manage serverless functions easily. Rust Safety - Rust's compile time checks allow developers to write serverless functions with peace of mind knowing that the code is memory safe and free of many types of bugs prior to deployment. Rust also has a built-in test harness, providing even more confidence through the use of unit testing. Test code is not compiled into releases which keeps binaries small. Speed - The event-based nature of serverless functions allows for triggering the process on a predefined schedule. One example is sending a notification to an IoT device every minute. Rust's low memory footprint and emphasis on performance make it an ideal choice for serverless functions, as tasks can be completed quickly and with low resource consumption. This saves money and energy resulting in fast, sustainable, cheap software solutions. OCI Cloud Functions run inside docker images. Although Cloud Functions do not have direct support for Rust, developers can design and build images conforming to the Fn Project standards and deploy those images to the OCI infrastructure. Rust code can easily be compiled into executable binaries and built into container images for use in Cloud Functions. High Performance Computing Workloads Sometimes it's necessary to perform computational operations that are extremely resource-intensive. Situations like this require massive processing power, which translates to large numbers of cores working in parallel. Core clusters are connected over a network and often write output to some form of storage. For these types of programs, performance is key. OCI supports performance-intensive workloads with [High Performance Computing] services. Programs have access to potentially tens of thousands of cores connected over a low-latency network. Developers can choose to run code on bare metal or virtual machines. OCI also offers preconfigured virtual machines for ML/AI data science applications with access to large storage volumes. Rust Speed - Again, Rust's performance characteristics make it a natural fit for these situations. Having low-level access allows developers to finely tune how data structures are laid out in memory and how algorithms interact with resources. When dealing with resource-intensive programming, speed directly translates to reduced energy consumption and costs. Concurrency - Access to massive clusters of cores only increases productivity if the code running on the system can efficiently utilize as many cores as possible at any given time. Rust's support for parallel and concurrent programming has huge advantages here. Rust's memory safety and \"Fearless Concurrency\" allow developers to write complex parallel programs confidently and securely. Conclusion The complex nature of software development requires developers to choose tools wisely. The correct combination of infrastructure and programming language has huge implications for the success or failure of a software system. Successful modern software systems need to be secure, performant, and flexible enough to meet the changing needs of the domain in which they operate. OCI's commitment to, and investment in, fine grain control, security, and optimization is echoed by the Rust team and is reflected in the language and ecosystem. This makes Rust an excellent choice for building complex software on top of OCI. By Oracle Developers","categories": ["clouddev","cloudapps"],
        "tags": ["open-source","devops","get-started","back-end","rust"],
        "url": "/tutorials/rust-on-oci/rust-safe-fast-manageable",
        "teaser": ""
      },{
        "title": "Using Sybl to connect to your Oracle Database",
        "excerpt":"It's impossible to work in cloud services without hearing about Rust, the system's programming language from Mozilla. It's used all over the internet by companies like Cloudflare, Facebook, and Discord. It's a strongly-typed yet flexible language that emphasizes strict guidelines around memory usage, making it blazing fast and resource lean. The language also focuses on developer productivity, which is evident in their tooling and [package ecosystems]. Rust is being used in places where C worked best, due in part to its recognizable syntax. It is also replacing some higher-level languages, like Node and Ruby. In this blog post, we will build a small API microservice using Rust. APIs are generally used in situations where services---a server and a client---must communicate with each other. Our API will represent the backend for an inventory bookstore, where books can be added, fetched, and removed. This app will be hosted on Oracle Cloud Infrastructure (OCI). OCI enables cloud-native containers to run in highly secure and performant environments that are also fully managed. This makes it a perfect match to host Rust applications, since the platform and the language are well-suited to solve similar problems. Prerequisites Before getting started, you'll need to install several software packages. First, you'll need Rust. Regardless of your operating system, the Rustup script is guaranteed to work and installs all the necessary tools you need to start building a Rust application. You'll also need to install Docker to test the app containerization locally. In order to deploy the app online, you'll also need a free OCI account. Getting started with the Rust code As with many programming languages, Rust comes with its own package management system called Crates. To start building our app, we will want an HTTP web framework to do the heavy lifting for us. There are many to choose from, but we'll use warp, as it's both popular and performant. Just as Node uses package.json to manage packages, Rust uses Cargo.toml. Create a new directory within which you can start building this project, and create a file named Cargo.toml in it. Paste these lines into that file: [package] name = \"server\" version = \"0.1.0\" license = \"MIT\" edition = \"2018\" [dependencies] tokio = { version = \"1\", features = [\"full\"] } warp = \"0.3\" serde = { version = \"1\", features = [\"derive\"]} serde_json = \"1.0\" Here, we're defining the general metadata of our package. We're specifying that we want to create a binary executable named server; we also have a list of dependencies that our project needs, including warp. Next, create a directory called src, and a directory called bin within that. Then, create a file called server.rs, and paste these lines into it: #![deny(warnings)] use warp::Filter; #[tokio::main] async fn main() { // Match any request and return hello world! let routes = warp::any().map(|| \"Hello, World!\"); warp::serve(routes).run(([127, 0, 0, 1], 3000)).await; } We've defined a very basic Warp server, which will run on http://127.0.0.1:3000. When a user visits that page, they'll see a greeting. Go ahead and type cargo run on the terminal. Cargo will download all the dependencies you defined, then it'll compile them together with the server.rs file to create an executable. (All of that in just one command!) When it's finished, you'll see the following message: ❯ cargo run Finished dev [unoptimized + debuginfo] target(s) in 0.06s Running target/debug/server Navigate your browser window to http://127.0.0.1:3000, which should show the greeting, thus confirming that the initial project setup has worked! Setting up an API Now that we have verified that our server runs correctly, it's time to build a more proper API. We want our API to get a list of books, add a new book, and remove a book. In a future blog, we will integrate with a backend database for storing and querying the data. To keep things simple in this tutorial, we'll just fake the data store by defining an array to store all of our books. Let's start by defining the structure of a Book. Rust has the concept of structs, which are akin to lightweight classes. Here's an example of what our Book class would look like: (All the code below replaces the code in the server.rs file.) rust use serde::{Deserialize, Serialize}; [derive(Clone, Serialize, Deserialize)] pub struct Book { title: String, author: String, year: u32, } We can then modify our main function to immediately set up a basic catalog of books that follow this structure. We will use a vector (which is like an expandable array), and store the list in memory: rust use std::sync::Arc; use tokio::sync::Mutex; pub type Db = Arc>>; [tokio::main] async fn main() { let mut book_catalog: Vec = Vec::new(); book_catalog.push(Book { title: \"The Hitchhiker's Guide to the Galaxy\".to_string(), author: \"Douglas Adams\".to_string(), year: 1979, }); book_catalog.push(Book { title: \"The Restaurant at the End of the Universe\".to_string(), author: \"Douglas Adams\".to_string(), year: 1980, }); book_catalog.push(Book { title: \"Life, the Universe and Everything\".to_string(), author: \"Douglas Adams\".to_string(), year: 1982, }); book_catalog.push(Book { title: \"So Long, and Thanks for All the Fish\".to_string(), author: \"Douglas Adams\".to_string(), year: 1984, }); book_catalog.push(Book { title: \"Mostly Harmless\".to_string(), author: \"Douglas Adams\".to_string(), year: 1992, }); let db = Arc::new(Mutex::new(book_catalog)); So far, so good? Right on! The next task is to add routes to this API. There are a number of patterns to implement this, but the one suggested by Warp takes a two-pronged approach: First, the routes are defined, and then, the implementation of those routes is defined. This way, the implementation can change, but the route information can be considered static and stable. Let's go ahead and define these routes. We'll drop the code first, and then provide a closer examination: rust mod filters { use super::Db; use super::Book; use super::handlers; use warp::Filter; use std::convert::Infallible; /// The routes, combined. pub fn constructbookroutes( db: Db, ) -> impl Filter + Clone { routegetbooks(db.clone()) .or(routepostbooks(db.clone())) .or(routedeletebook(db.clone())) } /// GET /books pub fn routegetbooks( db: Db, ) -> impl Filter + Clone { warp::path!(\"books\") .and(warp::get()) .and(with_db(db)) .andthen(handlers::getbooks) } /// POST /books with JSON body pub fn routepostbooks( db: Db, ) -> impl Filter + Clone { warp::path!(\"books\") .and(warp::post()) .and(json_body()) .and(with_db(db)) .andthen(handlers::createbook) } /// DELETE /books/:id pub fn routedeletebook( db: Db, ) -> impl Filter + Clone { warp::path!(\"books\" / u64) .and(warp::delete()) .and(with_db(db)) .andthen(handlers::deletebook) } pub fn with_db(db: Db) -> impl Filter + Clone { warp::any().map(move || db.clone()) } pub fn json_body() -> impl Filter + Clone { // When accepting a body, we want a JSON body warp::body::contentlengthlimit(1024 * 16).and(warp::body::json()) } } This might look like a lot of code, but we're really just redefining similar path structures in composite functions. Let's take a look at the first function: rust /// The routes, combined. pub fn constructbookroutes( db: Db, ) -> impl Filter + Clone { routegetbooks(db.clone()) .or(routepostbooks(db.clone())) .or(routedeletebook(db.clone())) } The only responsibility of constructbookroutes is to assemble a list of all the known routes. To make use of this, we must go back into our main function and change the final lines to look something like this: rust warp::serve(filters::constructbookroutes(db)) .run(([127, 0, 0, 1], 3000)) .await; Here, we're telling the warp server what our routes are, and passing along the in-memory DB we've created. Moving on to the next function: rust /// GET /books pub fn routegetbooks( db: Db, ) -> impl Filter + Clone { warp::path!(\"books\") .and(warp::get()) .and(with_db(db)) .andthen(handlers::getbooks) } We can ignore the function signature, as that's largely Warp specific requirements. Instead, let's look at the function line by line: - We're creating a path called /books - This path responds to GET requests - It makes use of the database we're passing it - And the actual logic is stored in a to-be-written function called get_books, within the handler's namespace The other two routes define POST and DELETE, which follow a very similar pattern. Let's move on to the logical implementation: rust mod handlers { use super::Db; use super::Book; use std::convert::Infallible; use warp::http::StatusCode; pub async fn get_books(db: Db) -> Result { let books = db.lock().await; let books: Vec = books.clone(); Ok(warp::reply::json(&books)) } pub async fn create_book( book: Book, db: Db, ) -> Result { let mut books = db.lock().await; books.push(book); Ok(StatusCode::CREATED) } pub async fn delete_book( id: u64, db: Db ) -> Result { let mut books = db.lock().await; let mut iter = 0; let len = books.len(); books.retain(|_book| { let mut keep = true; if iter == id { iter += 1; keep = false; } iter += 1; keep }); // If the vec is smaller, we found and deleted a book! let deleted = books.len() != len; if deleted { // respond with a 204 No Content, which means successful, Ok(StatusCode::NO_CONTENT) } else { Ok(StatusCode::NOT_FOUND) } } } These routes don't do much! The GET function prints a list of all the books available; the POST function takes a new book and adds it to the vector; and the DELETE function removes a book based on its index in the vector. (See the end of the article for the GitHub containing all this code.) Go ahead and type cargo run in the terminal. Your project will recompile, and when it's finished, go ahead and enter curl http://localhost:3000/books in another terminal window. You should see a list of your books, and you can note that the other HTTP verbs work, too! Dockerizing the Rust server Now, we're ready to take this project and containerize it via Docker. Docker has evolved over the years to make this process extremely simple. The entire Dockerfile fits in less than a dozen lines of code: docker Using the Rust official image... FROM rust:1.60 Copy the files in your machine to the Docker image... COPY ./ ./ Build your program for release... RUN cargo build --release And run the binary! CMD [\"./target/release/server\"] We'll need to build the Docker container, which we can run with this command: console $ docker build -t server . And finally, we'll need to start the Docker container, which can be done like this: console $ docker run -p 3000:3000 --rm --name server_docker server If you haven't seen the command for running a Docker container, then it's worth pointing out several things about the CLI flags. First, we're exposing port 3000 in our container to our localhost, as 3000. We could change these values if there were port number conflicts between our host machine and the Docker container, however, in this tutorial, that's not necessary. We're also naming our Docker as server_docker. This will make it easier to distinguish between logs and other systems' processes. After the Docker run command executes, try running the curl command again. You might see the following error: curl: (7) Failed to connect to localhost What does it mean? Well, when Docker launches the container, it assigns it its own IP address. And when the Rust server points to 127.0.0.1, it's opening a connection to itself, not the broader public world. The fix for this is to change the IP address used in our Rust code, from 127.0.0.1 to 0.0.0.0: rust warp::serve(filters::constructbookroutes(db)) .run(([0, 0, 0, 0], 3000)) .await; Stop the server by executing the docker stop command on your terminal. Then, rebuild and rerun the container. If you try the curl command again, you should see the API working as expected! Deploying to Oracle Cloud Infrastructure (OCI) At last, we reach the end of our tutorial: hosting our wonderful API online so that it's available across the internet. This is the easy part! In order for the OCI to load your Docker image, you will first need to push it to the Oracle Container Registry. You'll first need to know your Docker container's image ID to do that. Run the following command to get that information: console $ docker images You should see a list like the following: REPOSITORY TAG IMAGE ID CREATED SIZE server latest 8f2569fb8987 25 hours ago 2.83GB Take note of that image ID; we will use it when uploading the image to the Oracle Cloud Infrastructure. Next, follow these steps to learn more about performing the following actions: - docker login $REGIONKEY.ocir.io, which will log you into the Oracle Cloud Infrastructure Registry region you're using. Note that $REGIONKEY is determined by whichever region your account is using; see this list for the key which matches your region. - When prompted, your username is in the format of /. The tenancy name can be found under the Tenancy Details section of your administrative profile. - Next, type docker tag $IMAGEID $REGIONKEY.ocir.io/$TENANCY_NAME/server:latest, where: - $IMAGE_ID is the image ID provided by Docker. (In this example, it's 8f2569fb8987.) - $REGIONKEY and $TENANCYNAME are the same values provided earlier to log in. - Finally, type docker push $REGIONKEY.ocir.io/$TENANCYNAME/server:latest Our server image has now been uploaded onto the Oracle Cloud Infrastructure Container Registry; the final step is to instruct your Oracle Cloud service to pull that image and make use of it. We could do the longer process of setting up Kubernetes, but for such a small app, we can move much quicker if we simply load the image onto the VM directly. Let's go ahead and create an OCI Compute instance on which to run our container. Refer to the documentation here to learn more about how to do so; be sure to create your Virtual Cloud Network (VCN) with at least one public subnet (hint: the VCN Wizard is a cinch). Be sure to download your SSH private key and take note of your public IP address. After the instance is provisioned, we can pull our Docker image onto it. Follow these directions to learn how to SSH into your VM instance. Note that you may need to install Docker on your instance. You can verify whether this is required (or not) by entering the commands from this guide into your instance. Simply checking via the docker version command is enough to confirm Docker's presence: console $ docker version Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg. Client: Podman Engine Version: 4.1.1 API Version: 4.1.1 Go Version: go1.17.12 Built: Thu Aug 4 02:48:00 2022 OS/Arch: linux/arm64 With Docker installed, we can pull our image from the Oracle Cloud Infrastructure Registry. First, log in to the OCI Registry using the same credentials as before: console docker login $REGION_KEY.ocir.io Then, pull the image using the tag identifier which was created: console docker pull $REGIONKEY.ocir.io/$TENANCYNAME/server:latest Finally, we will need to make one essential security change, which is opening up a single network port to our machine so that the app is publicly accessible. To do that, you'll need to set up an Ingress rule and point it to port 3000 (HINT: this is done in the Networking configuration, within the VCN previously created). Here's a final screenshot of what setting up the Ingress rule looks like: Now that we've opened a rule on our network to allow ingress traffic over port 3000, we just need to open port 3000 in our local firewall to allow traffic through; run these two commands on the VM to do so: console $ sudo firewall-cmd --permanent --zone=public --add-port=3000/tcp $ sudo firewall-cmd --reload Once that's finished, we can run the Docker image with the same command we used when running it on our development machine: console $ docker run -d -p 3000:3000 --name serverdocker $REGIONKEY.ocir.io/$TENANCY_NAME/server:latest Voila! You can now call curl http://$PUBLICIPADDRESS/books and query the API, just like we did locally! Notice that we didn't need to install Rust or copy over any packages; that was all taken care of by Docker. Learning more We've only scratched the surface of the features OCI offers for containerized applications. There are many more features catered to modern DevOps practices, including: - Reliable database uptime and migrations - Observability and monitoring - Security and app isolation The speed and performance capabilities of Rust are also paired nicely with the availability and response speeds which are provided to apps running on OCI. It's a win-win for dev, ops, and everything in between. All the code used in this post can be found in this gist. For more information on how OCI can help you, be sure to check out our docs! By Oracle Developers","categories": ["clouddev","cloudapps"],
        "tags": ["open-source","devops","get-started","back-end","rust"],
        "url": "/tutorials/rust-on-oci/rust-using-sybl-to-connect-oci",
        "teaser": ""
      },{
        "title": "Sample Page",
        "excerpt":"= Sample Page :url-asciidoctor: http://asciidoctor.org This is a sample page composed in AsciiDoc. Jekyll converts it to HTML using {url-asciidoctor}[Asciidoctor]. [source,ruby] puts \"Hello, World!\" By ","categories": null,
        "tags": null,
        "url": "/sample/",
        "teaser": ""
      },{
        "title": "Introduction to Steampipe on Oracle Cloud Infrastructure",
        "excerpt":"* Photo credit: Photo by Gabriela Palai from Pexels >Note: This post is the author’s “notes from the field,” a personal exploration of the topic, filled with all of the journey’s pitfalls and little “Aha!” moments. It’s not meant to be a tutorial in the strictest sense, but to illustrate the patterns and troubleshooting mindset it sometimes takes to successfully run a thing. Follow along and enjoy. So, what is Steampipe? [Steampipe] is an open source project from [Turbot] that was introduced in the early days of 2021. It was instantly notable for it's intuitive interface and versatile toolset for running SQL queries against cloud resources. It supports multiple cloud providers (plugins), a Postgres-like SQL query language, and provides an interactive CLI to navigate through resources. I had the opportunity to perform an evaluation of Steampipe, and wanted to share some of my experiences working with it and the Oracle Cloud Infrastructure (OCI) plugin. The tool seems to be growing quickly, so I'll try to link to their excellent documentation whenever I have a chance. Reference: See the [official Steampipe introduction] for more background. Getting started Installation instructions for Windows, Mac, and Linux are provided on the [Steampipe webpage]. The instructions walk you through using Homebrew if you're on a Mac, an installer script for Linux, and Windows Subsystem for Linux with Ubuntu on Windows systems. The OCI plugin Steampipe has a plugin which [adds support for Oracle Cloud Infrastructure] (OCI). You can install the plugin using: console steampipe plugin install oci Once you have the plugin installed, you're ready to run queries. References: - OCI plungin - To learn more about the OCI plugin, see the [documentation and examples at the table level]. - tables - You can also learn more about the tables available via the plugin using the .tables and .inspect commands in the interactive shell. Basic functionality The Steampipe [documentation] is a thorough reference to the commands and query syntax. I'm no SQL expert, and so I'll just touch on a few highlights to help you get started. Select content from a table The query syntax can start as simple as a \"select thing from table\" and extend far beyond that. For example: console select displayname from ocikms_vault Your output should look something like: console +-----------------------+ | display_name | +-----------------------+ | oci-vault1 | | oci-vault2 | | oci-myvault | Run queries from an external file One of the great features of Steampipe is that you can store queries in external files and then run the queries by specifying the filename on the command line. Example: Let's take a look at an example SQL file from the OCI Compliance mod (more on this later). 1. Let's create a file objectstoragebucketpublicaccessblocked.sql with the following content: sql select -- Required Columns a.id as resource, case when publicaccesstype like 'Object%' then 'alarm' else 'ok' end as status, case when publicaccesstype like 'Object%' then a.title || ' publicly accessible.' else a.title || ' not publicly accessible.' end as reason, -- Additional Dimensions region, coalesce(c.name, 'root') as compartment from ociobjectstoragebucket as a left join ociidentitycompartment as c on c.id = a.compartment_id; 1. Run this query on the command line: console steampipe query objectstoragebucketpublicaccessblocked.sql Your output will look something like: console +-----------------------------------------------------------------------------------+--------+---------------------------------------------------+--------------+---------------+ | resource | status | reason | region | compartment | +-----------------------------------------------------------------------------------+--------+---------------------------------------------------+--------------+---------------+ | ocid1.bucket.oc1.iad. | ok | jon-test-rep not publicly accessible. | us-ashburn-1 | mytest02-dev | | ocid1.bucket.oc1.phx. | ok | bootstrap_testing not publicly accessible. | us-phoenix-1 | mytest02-dev | | ocid1.bucket.oc1.phx. | ok | Inventory not publicly accessible. | us-phoenix-1 | root | | ocid1.bucket.oc1.phx. | ok | bucket-20210428-0949 not publicly accessible. | us-phoenix-1 | mytest03-dev | | ocid1.bucket.oc1.phx. | ok | mybucket not publicly accessible. | us-phoenix-1 | mytest01-dev | +-----------------------------------------------------------------------------------+--------+---------------------------------------------------+--------------+---------------+ Interactive Steampipe shell You can also run an interactive Steampipe shell, which offers more tools for navigating tables as well as tab completion: 1. Run the steampipe query command to enter the shell. 1. From the shell, you can run queries by typing them out, or by copying and pasting them from an external source. Other helpful shell commands (note that they all start with a dot): - .inspect - inspect the contents of a table - .tables - list the tables available to query - .output - change the output format (JSON, CSV, or table) Example - opening the CLI and listing the available tables: console steampipe query Your output should look something like: console Welcome to Steampipe v0.9.0 For more information, type .help > .tables ==> oci +--------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------+ | table | description | +--------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------+ | ociapigatewayapi | OCI Apigateway Api | | ociautoscalingautoscalingconfiguration | OCI Auto Scaling Configuration | | ocicloudguard_configuration | OCI Cloud Guard Configuration | | ocicloudguarddetectorrecipe | OCI Cloud Guard Detector Recipe | | ocicloudguardmanagedlist | OCI Cloud Guard Managed List | | ocicloudguardresponderrecipe | OCI Cloud Guard Responder Recipe | | ocicloudguard_target | OCI Cloud Guard Target | | ocicorebootvolumebackup | OCI Core Boot Volume Backup | | ocicoredhcp_options | OCI Core DHCP Options Using mods (OCI compliance) In general, mods are a collection of SQL scripts built for custom reporting. Compliance is an important aspect of what [my team works on], so that made finding an ocicompliance module an exciting discovery. The ocicompliance mod has Center for Internet Security (CIS) compliance reporting built on top of Steampipe, and that just scratches the surface of its possibilities. References: - Steampipe mods [can be found here]. - For additional information, including examples and instructions, be sure to visit [the oci_compliance page]. Install the mod Let's install the mod and run a check for a specific control in this example: 1. Install the mod by running: console git clone https://github.com/turbot/steampipe-mod-oci-compliance.git Your output should look something like: console Cloning into 'steampipe-mod-oci-compliance'... remote: Enumerating objects: 254, done. remote: Counting objects: 100% (254/254), done. remote: Compressing objects: 100% (184/184), done. remote: Total 254 (delta 121), reused 155 (delta 61), pack-reused 0 Receiving objects: 100% (254/254), 468.38 KiB | 949.00 KiB/s, done. Resolving deltas: 100% (121/121), done. 1. Check for a specific control: 1. cd steampipe-mod-oci-compliance/ 1. Run the following command: console steampipe check control.cisv1102_1 Your output should look something like: console + 2.1 Ensure no security lists allow ingress from 0.0.0.0/0 to port 22 ................................................................................... 26 / 77 [==========] | ALARM: Default Security List for cavcn contains 1 ingress rule(s) allowing SSH from 0.0.0.0/0. ......................................................... us-ashburn-1 cadev ALARM: Default Security List for Inventory_VCN contains 1 ingress rule(s) allowing SSH from 0.0.0.0/0. .................................................... us-phoenix-1 root ALARM: Default Security List for Primary VCN contains 1 ingress rule(s) allowing SSH from 0.0.0.0/0. ............................................. us-phoenix-1 oci-test ALARM: Bastion Security List contains 1 ingress rule(s) allowing SSH from 0.0.0.0/0. ............................................................. us-phoenix-1 oci-test OK : App Security List ingress restricted for SSH from 0.0.0.0/0. .............................................................................. us-phoenix-1 oci-test OK : Wazuh Security List ingress restricted for SSH from 0.0.0.0/0. ............................................................................ us-phoenix-1 oci-test OK : DMZ Security List - Egress ingress restricted for SSH from 0.0.0.0/0. ..................................................................... us-phoenix-1 oci-test OK : DMZ Security List - Ingress ingress restricted for SSH from 0.0.0.0/0. .................................................................... us-phoenix-1 oci-test * OUTPUT CUT * Wrapping Up Steampipe shows tremendous potential. It's definitely easier to use Steampipe to query OCI resources than it is to remember complex command line options. Results come back quickly, especially with a well-tuned query, and modules provide an additional layer of reporting and glue for the data. I look forward to working with Steampipe and adding this tool to my toolbox. To explore more information about development with Oracle products: - [Oracle Developers Portal] - [Oracle Cloud Infrastructure] By Jon DeCamp","categories": ["iac","opensource"],
        "tags": ["open-source","steampipe","iac","devops","get-started","field-notes"],
        "url": "/tutorials/steampipe/steampipe-intro",
        "teaser": ""
      },{
        "title": "Tracing A Node.js with OCI Application Performance Monitoring and Zipkin",
        "excerpt":"OCI Application Performance Monitoring (APM) provides a comprehensive set of features to monitor applications and diagnose performance issues. APM integrates with open-source tracing system tools (aka, open-source tracers) such as Jaeger and [Zipkin] allowing you to upload trace data from your application. It also supports context propagation between Application Performance Monitoring agents and open-source tracers. In this tutorial, we'll cover how to set up APM on your system, configure your Node.js for tracing, and run some sample queries using Trace Explorer. Configure APM 1. Go to APM > Administration 2. Select the Create APM Domain button and provide the information requested in popup window. Grab domain details In the APM domain details you created, get the Data Upload Endpoint URL and the autogeneratedpublic_datakey values, we’ll need them in the next step. Configure your Node.js app Before we get started, make sure to: - Configure Zipkin for your app - Follow the steps in the [Zipkin JS repo] to configure Zipkin for your app. - Set up OCI APM - Follow the step in this [document] to configure tracers for OCI. For the rest of this tutorial, we'll work with [sample code] in the Zipkin repo. First, clone the repo and then edit the sample code as noted below. Edit web/recorder.js First section of code console /* eslint-env browser */ const { BatchRecorder, jsonEncoder: {JSON_V2} } = require('zipkin'); const {HttpLogger} = require('zipkin-transport-http'); Replace the last line with: console const CLSContext = require('zipkin-context-cls'); Second section of code console const debug = 'undefined' !== typeof window ? window.location.search.indexOf('debug') !== -1 : process.env.DEBUG; Third section of code console // Send spans to Zipkin asynchronously over HTTP const zipkinBaseUrl = 'http://localhost:9411'; // data upload endpoint example is something like https://aaaa...aaapi.apm-agt.eu-frankfurt-1.oci.oraclecloud.com/20200101/observations/public-span?dataFormat=zipkin&dataFormatVersion=2&dataKey=QM...3D 1. Adjust BaseURL - Of course, the BaseURL will not be a localhost, so replace const zipkinBaseUrl = ';' with: console const httpLogger = new HttpLogger({ endpoint: '/20200101/observations/public-span?dataFormat=zipkin&dataFormatVersion=2&dataKey=', jsonEncoder: JSON_V2 }) 2. Remove the logger and add a tracer - 1. Remove this: console const httpLogger = new HttpLogger({ endpoint: ${zipkinBaseUrl}/api/v2/spans, jsonEncoder: JSON_V2 }); 2. And add this: console // Setup the tracer const tracer = new Tracer({ ctxImpl: new CLSContext('zipkin'), // implicit in-process context recorder: new BatchRecorder({ logger: httpLogger }), // batched http recorder localServiceName: 'mytest', // name of this application supportsJoin: false //Span join disable setting }); Remainder of code A this point, the rest should look like this: console function recorder(serviceName) { return debug ? debugRecorder(serviceName) : new BatchRecorder({logger: httpLogger}); } function debugRecorder(serviceName) { // This is a hack that lets you see the data sent to Zipkin! const logger = { logSpan: (span) => { const json = JSON_V2.encode(span); console.log(${serviceName} reporting: ${json}); httpLogger.logSpan(span); } }; const batchRecorder = new BatchRecorder({logger}); // This is a hack that lets you see which annotations become which spans return ({ record: (rec) => { const {spanId, traceId} = rec.traceId; console.log(${serviceName} recording: ${traceId}/${spanId} ${rec.annotation.toString()}); batchRecorder.record(rec); } }); } module.exports.recorder = recorder; Restart your node app APM Trace Explorer Go to APM Trace Explorer and run a query: Traces can be observed in the list! What's next That’s all! Quick and easy! If you’re curious about the goings-on of Oracle Developers in their natural habitat, come join us in our [public Slack channel]! And don't forget our [free tier], where you can try out what we just discussed. To explore more information about development with Oracle products: - [Oracle Developers Portal] - [Oracle Cloud Infrastructure] ","categories": ["frameworks","cloudapps"],
        "tags": ["open-source","oci","nodejs"],
        "url": "/tutorials/tracing-node-js-micro-service-oci",
        "teaser": ""
      },{
        "title": "Tracing A Node.js with OCI Application Performance Monitoring and Zipkin",
        "excerpt":"OCI Application Performance Monitoring (APM) provides a comprehensive set of features to monitor applications and diagnose performance issues. APM integrates with open-source tracing system tools (aka, open-source tracers) such as Jaeger and [Zipkin] allowing you to upload trace data from your application. It also supports context propagation between Application Performance Monitoring agents and open-source tracers. In this tutorial, we'll cover how to set up APM on your system, configure your Node.js for tracing, and run some sample queries using Trace Explorer. Configure APM 1. Go to APM > Administration 2. Select the Create APM Domain button and provide the information requested in popup window. Grab domain details In the APM domain details you created, get the Data Upload Endpoint URL and the autogeneratedpublic_datakey values, we’ll need them in the next step. Configure your Node.js app Before we get started, make sure to: - Configure Zipkin for your app - Follow the steps in the [Zipkin JS repo] to configure Zipkin for your app. - Set up OCI APM - Follow the step in this [document] to configure tracers for OCI. For the rest of this tutorial, we'll work with [sample code] in the Zipkin repo. First, clone the repo and then edit the sample code as noted below. Edit web/recorder.js First section of code console /* eslint-env browser */ const { BatchRecorder, jsonEncoder: {JSON_V2} } = require('zipkin'); const {HttpLogger} = require('zipkin-transport-http'); Replace the last line with: console const CLSContext = require('zipkin-context-cls'); Second section of code console const debug = 'undefined' !== typeof window ? window.location.search.indexOf('debug') !== -1 : process.env.DEBUG; Third section of code console // Send spans to Zipkin asynchronously over HTTP const zipkinBaseUrl = 'http://localhost:9411'; // data upload endpoint example is something like https://aaaa...aaapi.apm-agt.eu-frankfurt-1.oci.oraclecloud.com/20200101/observations/public-span?dataFormat=zipkin&dataFormatVersion=2&dataKey=QM...3D 1. Adjust BaseURL - Of course, the BaseURL will not be a localhost, so replace const zipkinBaseUrl = ';' with: console const httpLogger = new HttpLogger({ endpoint: '/20200101/observations/public-span?dataFormat=zipkin&dataFormatVersion=2&dataKey=', jsonEncoder: JSON_V2 }) 2. Remove the logger and add a tracer - 1. Remove this: console const httpLogger = new HttpLogger({ endpoint: ${zipkinBaseUrl}/api/v2/spans, jsonEncoder: JSON_V2 }); 2. And add this: console // Setup the tracer const tracer = new Tracer({ ctxImpl: new CLSContext('zipkin'), // implicit in-process context recorder: new BatchRecorder({ logger: httpLogger }), // batched http recorder localServiceName: 'mytest', // name of this application supportsJoin: false //Span join disable setting }); Remainder of code A this point, the rest should look like this: console function recorder(serviceName) { return debug ? debugRecorder(serviceName) : new BatchRecorder({logger: httpLogger}); } function debugRecorder(serviceName) { // This is a hack that lets you see the data sent to Zipkin! const logger = { logSpan: (span) => { const json = JSON_V2.encode(span); console.log(${serviceName} reporting: ${json}); httpLogger.logSpan(span); } }; const batchRecorder = new BatchRecorder({logger}); // This is a hack that lets you see which annotations become which spans return ({ record: (rec) => { const {spanId, traceId} = rec.traceId; console.log(${serviceName} recording: ${traceId}/${spanId} ${rec.annotation.toString()}); batchRecorder.record(rec); } }); } module.exports.recorder = recorder; Restart your node app APM Trace Explorer Go to APM Trace Explorer and run a query: Traces can be observed in the list! What's next That’s all! Quick and easy! If you’re curious about the goings-on of Oracle Developers in their natural habitat, come join us in our [public Slack channel]! And don't forget our [free tier], where you can try out what we just discussed. To explore more information about development with Oracle products: - [Oracle Developers Portal] - [Oracle Cloud Infrastructure] ","categories": ["frameworks","cloudapps"],
        "tags": ["open-source","oci","nodejs"],
        "url": "/temp_content/tracing-node-js-micro-service-oci",
        "teaser": ""
      },{
        "title": "Using the AWS Migration Service with HeatWave on AWS with WordPress as an Example",
        "excerpt":"Now let’s try something really fun. Let’s say you’ve already got a database using Aurora or RDS, in our case we’ll use a WordPress database, and you want to migrate it to MySQL HeatWave on AWS. First, we’ll need to have a MySQL DB System on HeatWave already, and a critical step is opening port 3306, shown below. Great! Now let’s head over to AWS. 1. Log in to your AWS console, and use search for “DMS” — Database Migration Service, it’ll be the top result. 2. Click Create replication instance. 3. Fill out a Name, Amazon Resource Name (ARN), description. You may use whichever shape you like, but keep in mind performance:cost here. Engine version should be fine, as well as storage (unless you have a truly massive database, in which case adjust accordingly). 4. For VPC, choose one you have previously set up with the appropriate access controls for development work. And make sure it is publicly accessible. For Multi AZ, we’ll use a single availability zone, dev or test workload. 5. Click to open the Advanced security and network configurations. The subnet group will follow the VPC you chose above, and the availability zone should be US-East. The security groups should be populated with any you created earlier, ensuring ports are available (LINK TK to DOCS on this). 6. It will take a few minutes for the replication instance to spin up. Coffee time! 7. Once it’s ready, we’ll need to get the Public IP address for our endpoint. Go to the Endpoints section. On the right you’ll see a Create Endpoint button, click it. 8. Select Source endpoint, and check Select RDS DB Instance. You should see the RDS Instance field populate with what’s available to you. 9. For Endpoint configuration, you’ll have the identifier, ARN, and source engine pre-filled, but we want to provide the access to endpoint database information manually, so click that radio button. Then, enter the appropriate database information for the db you’re moving over. 10. Then, you should be able to test the endpoint connection below. 11. Now, we’ll create a second endpoint as our destination. To do this, we return to the MySQL HeatWave on AWS Console, and click MySQL DB Systems. Then, click on the DB System you wish to use, and in Summary you will find the Host Name. Copy the string. 12. Go back to AWS console, and click to create another endpoint. Except this time we’ll create a Target endpoint! Naturally. 13. In Endpoint configuration, give it a name (identifier), choose MySQL under Target Engine. ARN is optional. 14. Once again, we’ll provide access information manually. The server name is the host name you previously copied. Port is 3306. The username/password will match the one you created for the target database. 15. Now you have your endpoint, username/password, and can use this when creating a HeatWave on AWS cluster to connect a DB System and run queries. Refer to the HeatWave on AWS documentation for more. >Note: MySQL Shell is the recommended utility for exporting data from a source MySQL Server and importing it into a DB System on the MySQL HeatWave on AWS. MySQL Shell dump and load utilities are purpose- built for use with MySQL DB Systems. For more on running queries with HeatWave, please refer to the MySQL documentation. Want to know more? Join the discussion in our public Slack channel! By ","categories": ["cloudapps"],
        "tags": ["mysql","database","heatwave"],
        "url": "/tutorials/using-aws-migration-service-wp-ex-devrel0622",
        "teaser": ""
      },{
        "title": "Virtual Desktop Infrastructure(VDI) for Unreal Engine 5 on Oracle Cloud Infrastructure with nVidia GPU",
        "excerpt":"In this blog you'll learn how to set up Virtual Desktop Infrastructure on Oracle Cloud Infrastructure (OCI). A few weeks ago, Unreal Engine 5.1 was released. There are a lot of improvements and many new cool features that, for me, are making this release kind of of a big deal. Let’s get started with setting this up. Configuration Setting up the GPU Compute shape. 1. First, log into your Oracle Cloud console 1. Once there, click on the hamburger menu 1. Click \"Compute\" 2. Click \"Instances\" 3. Click \"Create instance\" 4. Change name 5. Choose the compartment that you want to run it on 6. This guide will focus on Ubuntu OS, so select Canonical Ubuntu. 7. Next, select Bare Metal Machine and GPU shape 8. Tick Terms and Conditions (Please note that the GPU shape will be charged against your account event when instance is stopped) 9. Confirm by clicking on Select Shape Setting up networking 1. For the purpuse of this guide we will create new virtual network called \"GPUVCN\" but you can reuse your exisiting one if you wish 1. Also create new subnet and call it \"GPUSUBNET\" 1. Next, create new SSH keys (or upload your public key files.pub) Other settings 1. Running Unreal usually requires a lot of hard drive space, so I recommend selecting a boot volume starting from 500GB+. This may vary depending on your requirements 2. I also recommend setting up a high performance VPU as this will speed things up when compiling, calculating shaders etc. Hit create once you finish Security rules 1. Once our instance is provisioned, let's create network security rules to allow connections for Remote desktop and SSH 1. Next, create a new security rule by navigating to our VCN 1. Navigate back to our compute instance and add the new Security Rule Setting up remote desktop on your GPU Compute shape. The next steps will focus on enabling the Ubuntu GUI on the GPU shape. These steps can be done either in Terminal or Visual Studio Code. 1. In order to connect to your instance, you can follow this guide 1. Now that we are in, lets run few commands to set up our GUI for Ubuntu 2. Run the following to update and upgrade your OS and install nVidia drivers: console $ sudo apt-get update $ sudo apt-get upgrade $ sudo apt install libnvidia-common-515 $ sudo apt install libnvidia-gl-515 $ sudo apt install nvidia-driver-515 3. Install the Ubuntu GUI: console $ sudo apt install ubuntu-desktop 4. In order to enable remote access, let's install XRDP, an open-source Remote Desktop Protocol server: console $ sudo apt install xrdp 5. Install net tools so we can debug our connectivity: console sudo apt install net-tools 6. Change your port setting from tcp6 to tcp4: console sudo nano /etc/xrdp/xrdp.ini 7. Inside that config file, change port=3389 to port=tcp://:3389 8. The next step is to flush your iptables so that we can reset our linux firewall (https://www.comparitech.com/net-admin/beginners-guide-ip-tables/). You can do that by running: console $ sudo iptables -F 3. Save your config: console sudo netfilter-persistent save I recommend creating a new user and securing your admin account with a password, or whitelisting your external IP. You can find out how using this video we did a while back. You should now be able to access the virtual machine using the Remote Desktop solution of your choice. Just type your external IP to connect to your machine. Using Unreal Engine on your Cloud 1. Get Unreal Engine downloaded to your machine. Since we have a GUI interface, you can install it in terminal by running sudo apt install -y chromium-browser. 2. Open browser on your machine 2. Navigate to and log in/create an epic account 3. Download your preferred version of the engine and extract the zip file: 4. To open Unreal Engine, run /path/to/ue5.1/Engine/Binaries/Linux/UnrealEditor in terminal: Outro As a word of caution, I would say that Unreal Editor still require a bit of work in order to be as intuitive as it is on Windows. Hence, the use case here is probably focusing on accelerating the rendering for video generation and pixel streaming. By Oracle Developers","categories": [],
        "tags": [],
        "url": "/tutorials/virtual-desktop-infrastructure-for-unreal-engine-on-oci",
        "teaser": ""
      },{
        "title": "How to get Going on Oracle Cloud Compute",
        "excerpt":" This is the first installment in a five part series about Go and Oracle Cloud Infrastructure (OCI). This series discusses how Go applications can be created and run on OCI - in Compute Instances (VMs), containerized on Kubernetes, or as serverless Functions. The articles show how to automate the build and deployment of these Go applications using OCI DevOps. An important topic is how to use OCI services from Go applications - both those running on OCI as well as Go code running elsewhere. Some of the OCI services discussed are Object Storage, Streaming, Key Vault and Autonomous Database. In order to follow along with these articles, readers should have at least basic knowledge of how to create Go applications. It is assumed that readers have access to their own Go development environment. Some of the examples and screenshots will specifically mention VS Code as development tool. However, other editors and IDEs can be used as well. The Go code presented in these articles demonstrates a number of mechanisms in their simplest form for maximum clarity and with the least dependencies. Readers should not expect meaningful functionality or production ready code. The articles describe how to get Going on OCI and try out the examples. Readers will need to have access to an OCI tenancy with permissions to create the OCI resources discussed in these articles. Most of the resources used are available in the Always Free Tier (Compute Instance, VCN, Autonomous Database, Object Storage, Logging, Resource Manager) or have a free allotment tier for limited monthly usage (Functions, API Gateway, Streaming, Vault, DevOps). You'll need an Oracle Cloud Infrastructure Free Tier account. Start for Free. Going on OCI The first objective is to create a Compute Instance (a VM) on OCI and create and run a Go application in it. This simple Go application handles HTTP requests from outside the OCI tenancy. Subsequently, the logging from this Go application is channeled into the OCI Logging service where it is available for monitoring and analysis purposes. The steps in this article: * create an OCI Compute Instance and set up Network Security List rules * create a Go application in the new Compute Instance and run it to handle HTTP requests * extend the application to produce logging; then route this logging into the OCI Logging service The final state at the end of the article is shown in the next figure. Create and configure an OCI Compute Instance OCI offers a wide range in Compute Instances -- from bare metal to virtual and from fairly small and always free to quite substantial in terms of CPU and memory resources and associated costs. For the purpose of this article, we can use an always free shape for the VM. The image selected is Oracle Linux Cloud Developer 8, which comes with a wide range of tools and runtimes preinstalled. We will need to associate the Compute Instance with a VCN (a virtual cloud network) and a public subnet -- either preexisting ones or to be created along with the new compute instance. A public IP address is requested for the compute instance to allow external consumers to access the HTTP endpoint on which the Go application will process requests, and also to allow an SSH connection to create and run the application in the first place. Note: for serious activities, use of a Bastion server or hands-off operations through automated processes is recommended. An SSH Key Pair is generated and associated with the VM; we need the private key half of this pair in order to establish the SSH connection to the VM. To keep your OCI tenancy well organized and have a good overview of the resources created for these Go on OCI explorations, I recommend creating a dedicated compartment. These are free, can easily be created and allow fine grained administration. The compartment used in the examples below is called go-on-oci. Create Compute Instance Open the OCI console in your browser. Switch to the compartment in which you intend to create the Compute Instance. Type instance in the search bar. A popup appears that shows a heading Services and below the link Instances. Click this link. It takes you to an overview of the Compute Instances currently present in the compartment. Click on the button Create Instance. A simple wizard for Create Compute Instance is presented. It is a form that allows you to provide the specifications for the compute instance you want to provision. First, give it a name; for example go-app-vm. Verify that the indicated compartment is indeed the correct one. Unless you have a reason to customize the Placement settings you can accept the defaults. Click on the Edit link in the Image and Shape section. Next, click on the button Change image. A list of available VM images appears. Mark the checkbox for the image Oracle Linux Cloud Developer. Read the Terms of Use for the image and mark the checkbox to indicate you have reviewed the document. Click on Select image. The shape selected by default -- always free eligible AMD VM.Standard.E2.1.Micro -- is fine, so no need to change it. Next, turn your attention to the Networking settings. The default settings are acceptable. However, if you want to specify better names for the new VCN and subnet or associate the new VM with an existing VCN and (public) subnet, then click on the Edit link and define the appropriate settings. Make sure that a public IP address will be assigned. In order to connect to the VM once it is running we need to be able to establish an SSH connection. An SSH key pair is used to authenticate our connection to the VM. The VM has the public key and we need the private key to encrypt messages to the VM. Click on Save Private Key in order to download this private key to a local file. You will need this file later on, so make sure to hang on to it. You do not need the public key. The default Boot Volume settings are acceptable, so we can skip this section. You are now ready to have the Compute Instance provisioned. Click on Create to start this action. When you press the Create button, there will be a small pause while OCI accepts your provisioning request. Then a page is presented that shows the Provisioning (in progress) overview. After a little while -- typically within 1--2 minutes -- the Compute Instance is running and the page is refreshed with the runtime details. This page contains the public IP address for the new VM. You'll need this value. From this page, you can perform administrative tasks - such as stop, reboot, terminate, audit, add to instance pool, configure agents and create a custom image for additional VMs. Configure Rules in Subnet Security List In order to allow the necessary network traffic to and from the VM, we need to configure a few rules in the Security List for the Subnet to which the VM is connected. In order to do so, click on the subnet link on the Compute Instance overview page. Alternatively, type virtual in the OCI console's search bar, navigate to Virtual Cloud Network, click on the VCN created or selected for the VM, and click on the relevant subnet. Click on the security list for the subnet. Two categories of rules can be defined: Ingress rules that govern traffic coming into the VM and Egress rules for traffic that goes out of the VM. We need the following types of traffic allowed: * Ingress: port 22 - SSH connection from our development environment (public internet) * Ingress: port 8080 (or any other port that we select) - HTTP requests from external consumers of the HTTP server we will implement in Go * Egress: port 443 - outbound HTTPS traffic for fetching Go modules (for example with go get or go mod download ) and performing git commands (including git clone) Depending on whether you used an existing subnet or had a new one set up when the VM was created, you may need to configure some or all of these rules. > Note: use 0.0.0.0/0 as the Source CIDR in all cases. The next image shows creation of an Ingress rule for allowing incoming TCP traffic on port 8080, from any originating network address or port. Connect to Compute Instance with SSH There are various tools available that make managing SSH sessions very simple, such as MobaXTerm and Putty. From a Linux command line, the most straightforward way to open an SSH connection is probably: 1. Copy the downloaded private key file to current directory 2. Change the accessibility of the private key file console chmod 400 3. Initiate the SSH session: console ssh –i opc@ And to celebrate the success of making the connection, we could do a little looking around in our new OCI Compute Instance. For example, check what versions are available in the VM of some language runtimes: console git version go version java -version node -v python --version terraform -version oci --version You could even create a very simple first Go application inside the VM and run it as well. Type vi app.go to bring up the editor (or alternatively use the nano or vim editor). Enter Insert mode by typing i. Paste or type this Go code into the editor: go package main import \"fmt\" func main() { fmt.Println(\"Hello You\") } Type esc followed by :wq. This writes the file content and returns you to the command line. You can now run this simplest of Go applications with: console go run app.go The program is compiled and executed and the output will appear. You are now officially going on OCI. Create and Run Go application in a Compute Instance At this point, we have a running OCI Compute Instance that we can connect to via SSH from our local environment. The VM has a number of tools and language runtime environments that allow us to create, compile and run applications. We have prepared inbound network connectivity on port 8080 to the VM. Now let's create a Go application that listens to inbound [HTTP] requests and serves them with a simple response. Connect to Compute Instance via VS Code Remote SSH extension (optional) There are several tools that facilitate interaction with a remote server over SSH. You can, of course, pick the one you like best. One option that I have come to appreciate is an extension for Visual Studio Code: the Remote SSH Extension. If you use VS Code and are interested in this extension, here are the steps to get started with it: 1. Open the Extensions tab in VS Code. Type ssh in the search bar. A list of extensions will appear with the Remote SSH extension at or near the top. 2. Install the extension. 3. Click on the Remote SSH FS icon. Then click on the icon to create a new SSH FS connection. 4. Provide the name for the new configuration - for example go-on-oci - and click on Save A form is presented in which you provide details for the Compute Instance: the host's public IP address and SSH port (22), the username (which is opc on Oracle Linux images such as the one used for the Compute Instance), and the private key file. (this screenshot only contains the relevant fields; you can ignore other aspects of the configuration) 5. Click on Save. With this in place, you can open terminal windows on the OCI Compute Instance from within your local VS Code environment and also explore and manipulate the file system contents of the remote VM from inside VS Code. Apart from the obvious latency, developing against the remote Compute Instance on OCI is the same as working locally in VS Code. Create and Run a Web Server in Go on OCI Create a new directory on the remote OCI Compute Instance, called myserver -- either through the VS Code user interface or on the terminal command line with mkdir myserver. Create a file called my-server.goin this directory. Paste the following contents into this file: go package main import ( \"fmt\" \"log\" \"net/http\" ) const DEFAULTHTTPSERVER_PORT = \"8080\" func greetHandler(response http.ResponseWriter, request *http.Request) { log.Printf(\"Handle Request for method %s on path %s\", request.Method, request.URL.Path) if request.Method != \"GET\" { http.Error(response, \"Method is not supported.\", http.StatusNotFound) return } name := \"Stranger\" queryName := request.URL.Query().Get(\"name\") if len(queryName) > 0 { name = queryName log.Printf(\" -- query parameter name is set to %s\", name) } fmt.Fprintf(response, \"Hello %s!\", name) } func fallbackHandler(response http.ResponseWriter, request *http.Request) { log.Printf(\"Warning: Request for unhandled method %s on path %s\", request.Method, request.URL.Path) http.Error(response, \"404 path/method combination not supported.\", http.StatusNotFound) return } func main() { fileServer := http.FileServer(http.Dir(\"./website\")) http.Handle(\"/site/\", http.StripPrefix(\"/site/\", fileServer)) http.HandleFunc(\"/greet\", greetHandler) http.HandleFunc(\"/\", fallbackHandler) log.Printf(\"Starting server at port %s\\n\", DEFAULTHTTPSERVER_PORT) if err := http.ListenAndServe(\":\"+DEFAULTHTTPSERVER_PORT, nil); err != nil { log.Fatal(err) } } This Go program will start an HTTP Server that listens on port 8080 for incoming requests. Requests with the path /site are handled by the static content handler that returns files from the local website directory (which does not exist yet). Requests with the path /greet are handled by the greetHandler function. This function will return a response with a friendly Hello, followed by either the value of the query parameter name or the hardcoded string Stranger. Other requests are handled in the fallbackHandler function, which returns a 404 HTTP StatusNotFound result. Under directory myserver, create a subdirectory called website. Create file index.html in this subdirectory and paste the following content to this file: html My Website My Website full of interesting things Our Web Server is now ready for some action... or so it would seem. However, the Oracle Linux image we are using for the VM is configured to reject any inbound network request (resulting in \"Route to host not found\" error messages). This is an element of security hardening that comes on top of the Network Security List rules that we discussed earlier. In order to make the operating system accept the inbound requests for port 8080, you need to execute the following commands: console sudo firewall-cmd --permanent --zone=public --add-port=8080/tcp sudo firewall-cmd --reload And now the Web Server can really get going. Execute this command to run the application: console go run my-server.go After a few seconds -- compilation, initialization -- the application is running, as you can tell from the logging. Access the Go Web Server Try to access the Web Server from your local environment, using a curl command : console curl :8080/greet?name=Your+Name or in a web browser with this URL: http://:8080/greet?name=YourNameOrSomeoneElses and: http://:8080/site/index.html The response should come in, and the logging from the Go application should also indicate that the request that was handled. Connect Go application to OCI Logging service The Go application my-server writes log entries to mark important activities such as starting up the HTTP Server and handling individual HTTP requests. Currently, this logging is written to the standard output. During development, that might be useful. However, for code that runs inside a cloud based compute instance -- a VM that runs by itself -- it is less useful to write logging to this standard output. OCI provides facilities for collecting, monitoring, and analyzing logging from OCI-managed resources, as well as custom code running on OCI in a consistent way in a generic, all-encompassing environment. OCI Logging ingests log entries from files written in VMs, containers, serverless functions in addition to the logging produced by the OCI services themselves. In order to make our Go application's custom logging part of OCI Logging, we will update the Go application to write logging to a file. Subsequently, we will create the configuration on OCI required to feed this logging into the Logging service. Modify the code in my-server.go. Add the function: go func initLogging() (logfile *os.File) { syslogWriter, e := syslog.New(syslog.LOG_NOTICE, \"greet-app\") if e == nil { log.SetOutput(syslogWriter) } else { log.Println(e) } log.SetFlags(log.Ldate | log.Lmicroseconds | log.Llongfile) log.SetPrefix(\"greet-app: \") log.Println(\"Logging initialized upon application restart\") return logfile } Add an import of module os: go import ( \"fmt\" \"log\" \"log/syslog\" \"net/http\" \"os\" ) and add a call to initLogging() as first line in function main : go func main() { initLogging() ... Run the application again: console go run my-server.go This time, we will not get any feedback on the command line from the application. It is still writing log output, but now it is sent to the Linux syslog. Verify if logging is indeed still being produced using the following command on a different terminal into the OCI Compute Instance: console sudo tail -f /var/log/messages This will show a live feed of all system log messages, including those produced by my-server. Configure Custom Logging from Compute Instance There are a few hoops to jump through in order to get the logging written inside the compute instance from the Go application loaded into OCI Logging. The next diagram gives an overview of these steps. In short: * the Compute Instance needs permission to send logs to the Logging service; this is handled through a Dynamic Group and a Policy * a Log Group needs to be set up to provide a container for the log that the compute instance will write to * an Agent Configuration needs to be created that is associated with the Dynamic Group (for the Compute Instance), a custom Log (in the Log Group), and the log file path /var/log/* from which the logs are to be ingested * the Management Agent plugin needs to be enabled in the Compute Instance's Oracle Cloud Agent configuration 1. Dynamic Group go-on-oci-instances In order to create the Dynamic Group for all Compute Instances in the Compartment, we need the OCID (the Oracle Cloud [resource] Identifier) for the Compartment. To get it, type comp in the search bar. Click on the link Compartments in the Services section. This will list all visible compartments. Locate the compartment that contains the Compute Instance that you created earlier. Hover your mouse over the value for the compartment in the OCID column. Click on the Copy link to move the OCID into the clipboard. Type dyn in the search bar. Click on Dynamic Groups. Click on the button Create Dynamic Group. Enter the name for the Dynamic Group -- e.g. go-on-oci-instances -- and optionally type a description. Define the following rule which selects all compute instances which are part of the compartment (in this case only the compute instance go-app-vm): instance.compartment.id = '' Then press Create. 2. Create Log Group go-on-oci-logs Create the log group in OCI Logging. A log group is a logical container for organizing logs. All logs to be generated are associated with a log group. Type log g in the OCI console's search bar. Click on the link Log Groups in the Services section of the panel that is shown. An overview page is shown listing all Log Groups in the compartment. Click on the button Create Log Group. Creating a Log Group is very straightforward: provide a name and optionally a description. I have called the log group go-on-oci-logs; however, any name will do. 3. Create the Custom Log and Logging Agent Configuration To have the custom logs from the Go application on the Compute Instance sent to the Log Group, we need to create a Custom Log Agent Configuration. This Agent Configuration is an instruction for agents running on the Compute Instances about collecting log entries from specific files and sending them to a custom log in a Log Group. An Agent Configuration associates one or more dynamic groups (identifying compute instances) on which the specific log files should be ingested into the Log Group. Type log in the OCI Console searchbar. Click on the Logs link in the Services section. This takes you to the Logs page. Click on the button Create Custom Log. You now enter a two-step wizard for defining the Custom Log and Agent Configuration. Enter the name for the custom log, for example go-on-oci-log. The compartment should be set correctly -- if not, select the compartment that contains the Log Group you have created in the previous section. If not already set, also select the Log Group you have just created. Click on the button Create Custom Log to move to the next page. The second page collects details on the Agent Configuration. Type the name of the configuration, for example go-on-oci-log-agent-configuration. Optionally provide a description, such as Configuration for log agent to collect Linux system logs from compute instances in dynamic group. The Group Type should be set to Dynamic group. Select the Dynamic Group go-on-oci-instances that you created earlier. Now click on the button Create to create a policy for allowing the compute instances in the dynamic group to interact with the OCI Logging service. A message is presented with the name of and a link to the policy that was created. Its policy statement says allow dynamic-group go-on-oci-instances to use log-content in tenancy. Set the Input type to Log path. Type a value for Input name, such as linuxsystemlog. Add the File path /var/log/*. > Note: multiple file paths can be defined for ingesting log files from. The fields under the heading Log Destination will already be set; these specify the Custom Log in the Log Group and Compartment that were selected on the previous page. Click on the button Create Custom Log to create the Agent Configuration. 4. Enable the Management Agent plugin in the compute instance The Agent Configuration has been created. It applies to the Cloud Agent running inside the Compute Instance that runs the Go application. However, before that agent will actually sent any logs, we have to ensure that the Management Agent plugin is activated. Oracle Cloud Agent is a lightweight process that manages plugins running on compute instances. Plugins collect performance metrics, install OS updates, and perform other instance management tasks. Not all plugins are automatically activated. More details on the Cloud Agent and these plugins can be found in the OCI Documentation on Cloud Agent Type instance in the search bar in the console. Navigate to Instances. Drill down to the instance created earlier for running the Go application. Switch to the tab Oracle Cloud Agent. Compute Instances based on one of the predefined OCI images - like the Oracle Linux Developer image used for this instance - contain the Oracle Cloud Agent. This is a lightweight process that manages plugins running on the instance. Plugins collect performance metrics, install OS updates, and perform other instance management tasks. The Management Agent plugin needs to be enabled. It is this plugin that collects log entries inside the compute instance and sends them to OCI Logging. Enable the Management Agent plugin if it is not currently enabled. It may take some time for this plugin to go from disabled via status Starting to status Running. Once the plugin has the status Running, it will read new log entries to the files in /var/log/ and send them to the custom log go-on-oci-log in the Log Group go-on-oci-logs. To check on the status of the monitoring agent from within the VM, you can execute the statement in a remote SSH session to the Compute Instance: console systemctl status unified-monitoring-agent The output should indicate that agent -- a Fluentd based data collector for OCI -- is running. 5. Explore logs from Go Application in OCI Logging The logging agent will scrape and forward new log entries from the system logs in /var/log. The Go application writes its logging to the system logs, which means to /var/log/messages. Now make some noise by starting the my-server application if it is not still running and making some HTTP requests to the /greet path and/or the /site path. It will take 1-2 minutes before the logs that result from these requests are available in the Log Explorer in the OCI Console. To take a look at the logs in the console, type log in the search bar. Then click on Logs. The Logs in the compartment are shown. Click on the link for go-on-oci-log (or whatever name you assigned to the log). This takes you to the Log Explorer - a page for browsing and searching through the log entries. When you scroll down, there is a table with the most recently ingested log entries. In this table will be various Linux system log messages alongside the output from the my-server application. To track down the specific log output from my-server, we can search for the log prefix assigned in the Go code: greet-app:. Click on the link Explore with Log Search. The Log Search panel is shown. Enter the string greet-app: in the field Custom Filters and press enter. This string is now a search filter. Press the Search button to perform search over the indicated time range of log entries for this filter. The log entries can be expanded for closer inspection. Details are shown for when and where this entry was ingested and how it is stored in the OCI Logging service. The source (instance), source file and type are indicated as well. There are three timestamps available - one produced by the Go application, one from the Linux system log handler, and one from the Logging agent at the time of scraping the log entry. The delta between the first two is not meaningful and the application really does not have to add the timestamp. The difference between the second and third should be small. The fourth timestamp -- when the log record was created in the OCI Log and became available for scrutiny -- is not shown. It would not add meaning to the log entry. Note: The Cloud Agent is capable of parsing log entries. This means that a log entry does not end up as a single long string in OCI Logging, but instead is parsed into individual fields which can be queried using the Logging Query Language. Logging from Go in JSON or CSV format to application-specific logfiles instead of system logs can easily be done. However, on the Always Free Tier Compute Instance shape used in this article, the ability to parse custom logs is not available -- neither parsing these logs nor even reading custom log files. Conclusion In this article, we took the first few steps with Go on OCI. After provisioning a Compute Instance based on the Oracle Linux Cloud Developer image, we had to set the right network security rules in order to allow outbound traffic (https to fetch modules and perform git operations) and inbound traffic (SSH connection and incoming HTTP requests). Through an SSH connection and using the Go runtime available the VM, we created a simple HTTP Server that can be invoked from anywhere on the public internet. The final stage of the article discussed configuration of logging facilities that enabled the flow of application log entries to the OCI Logging service. The Cloud Agent that is preinstalled in the Compute Instance was configured to scrape and forward the desired log entries. In the next article, we will focus on automation of the software engineering process for a somewhat more complex Go application: how to use OCI DevOps for storing the source code, building the executable and storing it as deployable artifact, deploying that artifact to a Compute Instance, exposing an HTTP endpoint for that application through an OCI API Gateway and finally checking its health status after deployment. All using automated pipelines. Resources * Source code repository for the sources discussed in this article series * Documentation on OCI Compute * Document on OCI Networking - Security Lists * Oracle Linux Cloud Developer Image for Compute Instances * Documentation on OCI Logging * Documentation on OCI Cloud Agent * Documentation on OCI Dynamic Groups * VS Code - Remote Development using SSH By Lucas Jellema","categories": ["clouddev","cloudapps"],
        "tags": ["open-source","devops","get-started","back-end","go"],
        "url": "/tutorials/way-to-go-on-oci/way-to-go-on-oci-article1",
        "teaser": ""
      },{
        "title": "Go Software Engineering and Automation with Oracle Cloud Infrastructure DevOps",
        "excerpt":" This is the second installment in a five part series about Go and Oracle Cloud Infrastructure. This series discusses how Go applications can be created and run on Oracle Cloud Infrastructure (OCI) -- in Compute Instances (VMs), containerized on Kubernetes, or as serverless Functions. The articles show how to automate the build and deployment of these Go applications using OCI DevOps. An important topic is how to use OCI services from Go applications -- both those running on OCI as well as Go code running elsewhere. The OCI services discussed include Object Storage, Streaming, Key Vault and Autonomous Database. In order to follow along with these articles, readers should have at least basic knowledge of how to create Go applications. It is assumed that readers have access to their own Go development environment. Some of the examples and screenshots will specifically mention VS Code as a development tool. However, other editors and IDEs can be used as well. The Go code presented in these articles demonstrates a number of mechanisms in their simplest form for maximum clarity and with the least dependencies. Readers should not expect meaningful functionality or production-ready code. The articles describe how to get Going on OCI. To try out the examples, readers will need to have access to an OCI tenancy with permissions to create the OCI resources discussed in these articles. Most of the resources used are available in the Aways Free Tier (Compute Instance, VCN, Autonomous Database, Object Storage, Logging, Resource Manager) or have a free allotment tier for limited monthly usage (Functions, API Gateway, Streaming, Vault, DevOps). Introduction The first part of this article describes the provisioning of a Compute instance based on the Oracle Linux Cloud Developer image, opening it up for inbound and outbound network activity, creating and running a Go application that serves HTTP requests, and connecting logging produced by the application to OCI Logging. This part takes the software engineering, build, and deployment of the application to the next level, using the Compute instance created in the previous installment. Automation is the name of the game, and the OCI DevOps service is introduced for storing the Go source code, building the application executable and storing it as deployable artifact, and deploying that artifact to the Compute instance. The last step in this article is exposing an HTTP endpoint for that application through an OCI API Gateway. The detailed steps in this article: * Set up the OCI DevOps project with the artifact registry and source code repository * Load the application resources for the Go application as code resources for the OCI resources into the source code repository * Create OCI DevOps Build Pipeline for producing a deployable artifact from Go source code; include Go fumpting/linting and testing in the pipeline. Deliver the artifact to the OCI Artifact Repository. Trigger the pipeline manually, to see it in action; watch lint and test. Verify the artifact that is produced. * Create an OCI DevOps Deployment Pipeline that takes the built Go app artifact -- a binary executable -- and deploys it to the VM (and makes it run). Add a trigger of the Deployment Pipeline in the Build Pipeline. Manually trigger the build pipeline. The Go application is deployed to the VM. * Create API Gateway with a new API Deployment with a route for the HTTP endpoint exposed by the Go app that was just deployed to the VM. Check out HTTP calls to the now public API – see them handled. Apply a little request manipulation in the API Gateway's route to the Go application Create DevOps Project with Code Repository and Artifact Registry The Oracle Cloud Infrastructure (OCI) DevOps service is an end-to-end, continuous integration and continuous delivery (CI/CD) platform for developers. It provides code repositories, build servers that run automated CI/CD pipelines, an artifact registry for storing the deployable built artifacts and deployment pipelines that rollout new software to OCI environments. The starting point in OCI DevOps is a project -- a logical grouping of DevOps resources needed to implement a CI/CD workflow. DevOps resources can be artifacts, build pipelines, deployment pipelines, external connections (to GitHub or GitLab code repositories), triggers (definitions of events that should trigger a build pipeline or a deployment pipeline), and environments (into which deployment is performed -- a Function application, a group of Compute instances, or a Container Engine for Kubernetes (OKE) cluster). Using OCI DevOps is largely free. However, you do have to pay for the build server -- but only the pay-per-use compute costs and the network traffic to and from outside OCI boundaries. Running the Deployment Pipelines is not charged to you. Of course, the runtimes to which the applications are deployed need to be paid for, and the storage required for the Artifact Registry as well as the Container Registry is charged to your cloud account as well. Create DevOps Project Before we can create a DevOps project, we first need to create a Topic in the Notifications service. Messages representing events in OCI services, alarms that get triggered, or messages explicitly produced by custom application components are asynchronously published on a Topic and subsequently delivered to subscribers to the Topic. Project Notifications are published to a Topic to keep you apprised of important events and the latest project status. They also alert you if you need to take any necessary action such as approving a workflow. You must therefore create a Topic and add a subscription to the topic. Open the OCI console. Type notific in search field; then navigate to Notifications. Click on the button Create Topic. Enter the name of the topic -- for example devops-topic -- and optionally provide a description of the topic. Click on Create to have the topic resource provisioned. The newly created Topic is shown in the console. Click on the name to navigate to the details page. Click on Create Subscription to create a subscription and have emails sent to your email address for DevOps project events that require your attention. > Note: in addition to email, subscriptions can also be created for Slack, PagerDuty, web hook, SMS and OCI Functions. Click on Create to create the subscription. The new subscription is shown with the status Pending. The subscription only becomes active when the email address is confirmed. Check your mailbox. You should find an email from OCI that invites you to confirm your subscription. When you click the link in the email (or copy the url for the link to a browser window), you are taken to a web page which informs you that the subscription was confirmed. At this point, messages published to the topic will be relayed as email to the address you have subscribed with. Now, let's create a DevOps project. Ensure that the context compartment is the right one -- in my case go-on-oci, created in part 1 of this series. Type devops in the search field; then navigate to Overview. An overview is shown of all DevOps projects (probably none at this point). Click on the button Create DevOps Project. Type the name of the project, e.g. go-on-oci, and optionally enter a description: \"Resources for the Way to Go on OCI application\". Click on Select Topic and select the Topic you have just created. Then click on Create DevOps Project. The DevOps project is created. Its overview page is shown. One last step to complete the project definition: enable logging. Click on the button Enable Log. This takes you to a tab labeled Logs. Toggle the switch to Enable Log. This brings up a pane where the Log Group and the name of the Log are defined. In article 1 of this series, we created a Log Group go-on-oci-logs which can serve us now. If you want to use a different Log Group or do not have that group available (anymore), click on the link Create New Group. Provide a name for the Log. Then click on Enable Log. The new log is shown with status Creating. After a minute or so, the status will be be updated to Active. Now the DevOps project is fully primed, ready for some action. Note: to allow other users to access the DevOps Project, you need to set up the correct policies. Read details about how go get started with the DevOps service in the OCI Documentation. Create Code Repository OCI DevOps offers Git-style code repositories — similar to GitHub, GitLab or Azure DevOps. You only pay for storage, no additional charges for the Git repository overhead. You get a Git repo that you access securely from your command line or local Git GUI tool, just like you're used to. Or through a simple, straightforward browser UI that, for example, allows searching for sources, commits and inspecting pull requests. Although this UI is clearly not meant to be your next Git power tool, it can still be quite convenient for some quick browsing. On the DevOps project overview page, click on the card Create repository. Enter the name for the repository -- for example go-on-oci-repo and optionally a description. Then click on the button Create repository. After a few seconds, the details for the new code repository appear. The repository is empty at this point, and it cannot yet be cloned to your local environment. You first need to create an authentication token that can be used for connecting your local Git tooling to this OCI Code Repository. Before you create that token (or reuse a token), click on the button HTTPS. A command is now shown for cloning the new Git code repository. The command looks like: console git clone https://devops.scmservice.us-ashburn-1.oci.oraclecloud.com/namespaces/idtwlqf2hanz/projects/go-on-oci/repositories/go-on-oci-repo Note that your region key and namespace will be different. To create an Authentication Token, follow these steps: 1. In the top-right corner of the Console, open the Profile menu and click User Settings 1. Under Resources, click Auth Tokens 1. Click Generate Token. Enter a description that indicates what this token is for 1. Click Generate Token The new token string is displayed. Copy the token immediately to a secure location from which you can retrieve it later -- you will not see the token again in the Console. You can now clone the code repository to create a local copy on your computer, add or remove files, commit changes, and work on different branches by using Git operations. To clone the repository using HTTPS, copy the displayed URL to a local terminal window in an environment with git set up. Upon running the command, the clone operation initiates and you'll be prompted for the username. If you use direct sign-in for logging in to OCI, the username you need to enter is tenancy/username, e.g. unicorn-lab/archimedes@rockstars.nl. If you connect to OCI through an identity provider -- a federated login -- then the username required here consists of tenancy/identityprovider/username, e.g. lucascloudlab/oracleidentitycloudservice/lucas@rainbowmail.nl. When prompted for your password, paste the authentication token that you created earlier and saved for this purpose. The repository will now be cloned from OCI DevOps to your local machine and your access through Git is configured locally. To stop Git from prompting you for credentials with every operation against the remote repository, you can run console git config --global credential.helper store Then perform a git pull, log in one more time, and from now on git has the login details and will no prompt you again. As an aside: an OCI Code Repository can also be set up as a mirror for another Git repository on GitHub or GitLab. This means that changes to this existing Git repo are replicated to the Code Repository on OCI and can trigger the pipeline in OCI DevOps. The other benefit of doing this is it speeds up the build process: when the build needs to fetch the sources from the repo, it will be able to do so much faster from a nearby OCI Code Repository than from a further-removed GitHub or GitLab repository. Create Artifact Registry An artifact registry is used to store the deliverables from build pipelines and any other artifact that we need to perform successful deployments. More generally, an Artifact Registry is a repository service for storing, sharing, and managing software development packages. Artifacts are grouped into repositories, which are collections of related artifacts. Artifacts can be uploaded and downloaded, versioned, and hashed for identification and mutation check. To create a new Artifact Registry, type registr in the Console search bar. Click on the link Artifact Registry. Click Create repository. In the Create repository dialog box, specify details for the new repository, or at least specify its name, e.g. go-on-oci-artifacts-repo. Click on Create to create the new artifact repository. Deployment In a bit we'll talk about the Build Pipeline that takes sources from the code repository and uses them to produce correct and deployable artifacts and subsequently triggers a deployment pipeline. First we'll focus on the deployment process. In OCI DevOps projects, three elements are needed to perform a successful deployment: 1. A target environment must have been defined; this can be a Kubernetes cluster (an OKE instance), a Function or a Compute Instance (or group of instances, VMs or bare metal machines). The Deployment Pipeline needs specific IAM policies to be allowed to act upon target environment. 2. One or more artifacts -- files that are part of the application that is to be deployed, including scripts needed to run for preparing the deployment or runtime context. Artifacts are used from a repository, either an OCI Container Image Registry, or an OCI Artifact Registry repository. 3. The deployment pipeline that defines the steps to perform. For deployment to a Compute instance, we also need a deployment configuration file -- a YAML file that defines the commands to copy the artifacts to their specific locations on the Compute instance's file system and the statements to execute for configuring the environment and running the application. Environments & Policies In our case the deployment will take place on the same Compute instance we used before. We configure the deployment pipeline later on for environments defined in the DevOps Project. An environment in the DevOps project represents a real environment -- OCI Function, OCI Kubernetes Cluster or (group of) OCI Compute instances, such as in our case. Deploy stages in a pipeline of type Deploy -- Instance Group are associated with an environment of type Instance Group. Navigate to the Environments tab in the DevOps Project. Click on the button Create Environment. Click on the tile Instance Group. Provide a name, e.g. go-on-oci-vm, and optionally a description. Click on Next. On the second page of this Create environment wizard, click on Add instance. In the Instance selection pane that appears, locate the Compute instance, select it, and click on Add instance. Finally, click on the button Create Environment to complete the wizard. IAM Dynamic Groups and Permission Policies The Deployment Pipeline needs permissions to act upon the Compute instance into which it has to deploy. These permissions are defined through policies which grant the permissions to a dynamic group. The Deployment Pipeline is made member of that dynamic group -- the recipient of the policy's permissions. To create the dynamic group, type dyn in the search bar. Click on the link Dynamic Groups in the search results pane. On the overview page for dynamic groups, Click on the button Create Dynamic Group. Enter the name for the Dynamic Group for the Deployment Pipeline(s), e.g. deploy-pipelines-for-go-on-oci, and optionally type a description. Define the following rule which selects all deployment pipelines that are part of the compartment (in this case we have not even created a single deployment pipeline, but we soon will): All {resource.type = 'devopsdeploypipeline', resource.compartment.id = ''} Of course, replace with the identifier of the compartment you are working in. Then press Create. It is convenient to define the dynamic group in this broad fashion, simply including all resources in the compartment of type deployment pipeline. In a realistic environment, I recommend defining dynamic groups and policies that are as fine-grained as possible as to not grant more permissions than are needed. Next, to create a policy in the console: type poli in the search bar and click on Policies | Identity in the Services area in the search results popup. This takes you to the Policies overview page for the current compartment. The first policy defines the permission for the deployment pipelines to access resources in the compartment. Create a new policy, type a name, a description and the following statement: Allow dynamic-group deploy-pipelines-for-go-on-oci to manage all-resources in compartment The definition as shown here is again quite broad. You further restrict the access to resources by specifying the type of resource or other restrictive conditions. A second policy defines the permission for the deployment pipelines to retrieve artifacts from artifact registry repositories in the current compartment. Again, you need to provide a name and a description. Then define the policy statement as follows: Allow dynamic-group deploy-pipelines-for-go-on-oci to read all-artifacts in compartment We need another policy to make it possible to access the artifacts from the artifact registry repository. This policy defines the permission for the dynamic group of Compute instances -- as defined in the previous installment of this article as go-on-oci-instances -- to retrieve generic artifacts from artifact registry repositories in the current compartment. This may come as a bit of a surprise: should not only the deployment pipeline have the permission to read the artifacts? As it happens: deployment takes place on the Compute instance and artifact retrieval is done from that instance. Therefore the policy has to allow the instance to read the generic artifact. Again, you need to provide a name and a description. Then define the policy statement as follows: Allow dynamic-group go-on-oci-instances to read all-artifacts in compartment The deployment pipeline is executed on the Compute instances through the cloud agent running on the instance. This next policy enables use of the instance agent execution facility on the Compute instances in the dynamic group. Type a name -- go-on-oci-instances-can-run-command -- and a mandatory description. Then define the policy (replacing the placeholder with the actual name of the compartment): Allow dynamic-group go-on-oci-instances to use instance-agent-command-execution-family in compartment This diagram visualizes the dynamic groups and policies that are now in place. Oracle Cloud Agent and Run Command Plugin The Compute Instance Run Command plugin must be enabled on the VM, and the plugin must be running for the Deployment Pipeline to be able to have commands executed on the instance. You can check and enable this on the Oracle Cloud Agent tab in the Compute instance details page in the console. For the Compute Instance Run Command plugin, make sure the Enabled Plugin switch is in the Enabled setting. It takes up to 10 minutes for the change to take effect. Artifacts Before too long, we will be using a DevOps Build Pipeline to generate the deployable artifacts that the Deployment Pipeline can then take and install in the target runtime environment. However, for now we will settle for a hand crafted artifact -- created from the source code discussed in the previous installment of this series. Using any SSH client -- and perhaps most conveniently the SSH FS extension in VS Code -- open an SSH connection to the OCI Compute instance that was set up in the previous article. You may have called it go-app-vm. You created a directory myserver as part of the steps in this article. Navigate into this directory, that should contain the file my-server.go. Now build the Go application into an executable: go build -o my-server This produces a local executable file called my-server, of close to 7 MB. That may seem large. But consider: this file is the entire application. It can run by itself, without any special requirements on a preinstalled runtime environment or virtual machine. Run the file with the following command ./my-server The web server will be up and running. With this next command, you can verify that the executable is running as intended. curl localhost:8080/greet Let us now create a an artifact we can use for automated deployment later on. In the current directory -- that contains the executable my-server and the subdirectory website. These commands create the zip file my-server.zip, that will be the artifact used in the deployment pipeline we are about to create: zip -r my-server.zip website zip -rv my-server.zip my-server To show contents of new zip file, execute: zip -sf my-server.zip In order to deploy the application now contained in zip file in the deployment pipeline, we need to make sure this artifact is loaded into the artifact registry. Move the zipfile into the website directory under myserver. Make sure the application my-server is running, so we can use our own application to download the zip-file from the Compute instance. Download the zip-file to your local environment through a curl command on the command line or through the browser: http://:8080/site/my-server.zip Stop the my-server application process. In the OCI console, navigate to the artifact registry repository that was created earlier. Click on Upload Artifact. Type path -- my-server.zip -- and version -- 1.0. Select the my-server.zip file as the file to upload. Click on Upload. This will create the artifact in the registry. In order to also make this artifact available for deloyment in the current DevOps Project, it needs to be associated with the DevOps Project. Navigate from the DevOps Project's homepage in the console to the Artifacts tab. This is where artifacts -- container images from the OCI Container Image Registry or generic artifacts from an Artifact Registry Repository -- are identified as relevant for the DevOps Project. Click on the button Add Artifact. Specify a name -- for example my-server -- and select Generic Artifact as the type. Next, click on the Select button to select the Artifact Registry repository from which you want to select an artifact. Obviously, select the repository that was created earlier. Click on the second Select button to locate the specific artifact in this repository. Select the my-server.zip artifact that you uploaded earlier. Set the toggle Replace parameters used in this artifact to No: no placeholders need to be substituted with deployment time values in this zip file. Then click on Add to complete the association of the artifact with the project. Deployment Configuration File The deployment itself consists of a number of steps, such as retrieving the artifacts from the registry, unzipping the archive's contents to the right destination, setting the required file access privileges, creating an autostart service to make the application run whenever the VM is rebooted. These steps are defined in two ways: * through the deployment configuration file (in yaml format) describes the high level steps, such as artifact transfer to the target environment and command execution on that environment as well optionally human approval, explicit pauses and execution of an OCI function for fine tuning or validation; this configuration file can be defined as a special type of artifact and associated with the deployment pipeline or it can be defined as an inline artifact, as we will do at first * in shell scripts that are executed on the target environments to perform the fine grained installation and configuration steps; these scripts are defined as (part of) artifacts transfered to the target environment In this first deployment pipeline we will only use a very simple deployment configuration file, defined as inline artifact. Click once more on the button Add Artifact. In the Add artifact dialog, type a name: myserver-to-vm-deployment-configuration. Select Instance group deployment configuration as the type. Select Inline for artifact source. version: 1.0 component: deployment env: variables: version: ${appVersion} files: # This section is to define how the files in the artifact shall # be put on the Compute instance # the artifacts are copied to directory myserver in /tmp; if that directory does not exist, it will be created # artifacts that are archives (such as zip) are extracted as well into the target directory; a file in the root of the archive ends up in /tmp/myserver - source: / destination: /tmp/myserver steps: # This section is to define the scripts that each step shall run on the instance after file copy. - stepType: Command name: Kill my-server (if it is currently running) command: killall my-server || echo \"Process was not running.\" timeoutInSeconds: 30 - stepType: Command name: Remove directory yourserver - for a fresh and clean install command: rm -Rf /tmp/yourserver timeoutInSeconds: 60 - stepType: Command name: Copy directory myserver with all unzipped artifacts to newly created directory yourserver command: cp -R /tmp/myserver /tmp/yourserver timeoutInSeconds: 30 - stepType: Command name: Copy Log File for this deployment as static file under website command: cp /tmp/myserver/stdout /tmp/yourserver/website/deployment-log.txt timeoutInSeconds: 30 - stepType: Command name: Start My Server to serve HTTP Requests (as backgroundprocess) command: cd /tmp/yourserver && ./my-server & timeoutInSeconds: 60 - stepType: Command name: Remove deployment artifacts command: rm -Rf /tmp/myserver timeoutInSeconds: 60 Now it is time to create the deployment pipeline itself -- and link the two artifacts to the environment. Deployment Pipeline On the DevOps Project's overview page, click on the button Create pipeline. The Create pipeline form is presented. Type a name -- deploy-myserver-on-go-app-vm -- and optionally a description. Then click on the button Create pipeline. The deployment pipeline is now created -- though it is quite empty: not an environment into which it should deploy, no artifacts that are to be deployed and no configuration file to define the steps to execute. In the pipeline editor that appears, click on the Add Stage tile (or on the plus icon). The next page shows a list of stage types. Click on the tile labeled Deploy incrementally through Compute instance groups; although a mouthful it simply means (for our purpose): deploy stuff onto a single VM. Press button Next. Type the stage name, for example deploy-myserver-to-vm. Select the environment that was defined earlier for the target VM: go-on-oci-vm. Under Deployment configuration, click on Add Artifact. A list of all artifacts in the DevOps project of type Instance group deployment configuration is presented. Select the only entry -- that was created earlier. Press button Save changes to associate this deployment configuration with the deployment stage. Note that the button Add Artifact is no longer enabled: only a single deployment configuration can be defined in a stage. To specify the artifact(s) that are to be taken from an artifact store and put into the target environment, click on the second button labeled Select Artifact. Select the my-server artifact that was added to the DevOps project, representing the similarly named artifact my-server.zip:1.0 in the Artifact Registry repository go-on-oci-artifacts-repo. Click on Save Changes. Toggle radio group Rollout policy -- not useful in our case with only a single Compute instance as deployment target -- to Rollout by count and type 1* in the field *Instance rollout by count. Then click on button Add. The pipeline stage is created in the pipeline. And the pipeline can now be executed -- to retrieve the zip file with the my-server application from the artifact repository, download zipfile and extract its contents in the target VM and run the application as background process. When the deployment pipeline is done running, the application should be serving HTTP requests. Click on button Run pipeline. A page with an overview of the manual run of the deployment pipeline. It allows you to set a name -- to indicate the specific significance of this particular deployment. Parameter values can now be provided for use in this run of the deployment pipeline. Our pipeline does not currently have any such parameters defined so we can not set any values. In the future, we might well use parameters, for example to define the prefix for log output or the port on which my-server should listen for HTTP requests -- set as an environment variable or startup parameter in the deployment configuration. Now press button Start manual run. The deployment is started. An email is sent to the address that you subscribed on the notification topic, to alert you of the start of the run. A second mail will be sent to let you know about the result of the run. The next figure shows the configuration of the Deployment Pipeline in conjunction with the other OCI resources it depends on. The configuration When the deployment is finished successfully, send an HTTP request -- with curl or from a browser -- to port 8080 at the public IP of the VM: http://:8080/greet?name=Success When you receive a response to this request -- as you should -- it proves that the deployment has succeeded. Starting from zip file containing the binary executable of our Go application, without our hands touching the VM, we managed to apply some fine automation and bring the application alive. The deployment pipeline, targeted at the proper environment, leveraging the right artifact and steered by the proper deployment configuration managed to pull it off. Note however that the application is not auto-healing and will not automatically start when the VM is rebooted. We can configure the application as a Linux service that should automatically be started -- and we will do so when we put together the build pipeline for a somewhat more sophisticated application. At that point we will also make use of dynamically defined port for the application to listen on instead of the hard coded port 8080 that is currently used. Using the next URL path http://:8080/site/deployment-log.txt you are able to inspect the first few log entries from the deployment pipeline itself. This file is copied to the static website directory of my-server in the first step in the deployment configuration. The files indicates the directory on the Compute instance where the detailed logs are written: Deployment Log for deploymentId ocid1.... is created at /var/lib/ocarun/commands/wd/..../stdout. Note: root access is required to read this file. The deployment pipeline can do several additional things. Take parameter values we specify for each deployment and replace placeholders in the deployment pipeline and the deployment configuration with these values. Ask a user's approval to proceed. Run additional stages -- sequentially or in parallel. Do a blue/green or canary release to a group of Compute instances. You would be forgiven for thinking that this has been quite a lot of work for what is really a simple installation. If you would do this installation only once, then creating an SSH connection as we did in part one of this series and just performing the installation steps manually would be much more efficient than creating this pipeline to perform the installation. When however you want to install the application on multiple Compute instances that you not have SSH access to or you want to install the application multiple times, whenever a new version of the application becomes available and you want colleagues without Linux skills or access privileges to be able to perform the installation, having this deployment pipeline slowly begins to make sense. Once you know the steps in the pipeline are correct, the deployment can be run in a fully automatic manner without the risk of human errors, abuse of access privileges, lack of audit trail, lengthy wait times because of unavailability of staff. Suddenly the concept of deployment pipelines is more and more enticing. Assuming much more involved deployment processes with additional installation steps, larger numbers of artifacts, more complex, perhaps parameter driven environment configuration -- the value of automated deployment pipelines will be obvious. To get a glimpse of this effect, you could quickly create a new Compute instance in the compartment. Then define a new environment in the DevOps project for this Compute instance and associate the Deployment Pipeline with this environment. Then run the pipeline. With just a few simple steps, you have made the application run on this second Compute instance. Without resorting to SSH connections and command line operations. In fact, through only a simple console based process that you can teach to fairly non-technical staff. Build Pipelines for Automated Source to Artifact Build and Delivery In the previous section the artifact to be deployed was created manually and uploaded into the artifacts repository by hand. In the name of automation we want to use build pipelines for creating such artifacts. Starting from the source code of the Go application, the build pipeline will enlist a build server, copy the relevant sources to that server, perform the linting, code analysis, compilation, testing and packaging steps and save the resulting deployable artifacts to the artifacts repository. The build pipeline can subsequently trigger the deployment pipeline to take those artifacts and deploy them to a specified environment. The build pipeline itself can be triggered manually or by an event such as a commit or a merge-to-master of a pull request in the code repository. In this section we are going to create an OCI DevOps Build Pipeline. It is on the one hand associated with the Code Repository you have created in the DevOps Project and it delivers artifacts to the Artifact Registry repository on the other. The pipeline can contains for these activities: * Perform a managed build (execute build according to specification on a build server) a * Deliver the output of a managed build stage to an Artifact Registry repository * Wait (for a specified period of time) * Trigger a Deployment Pipeline while passing parameters to it The Build Pipeline we will create next will take sources for a Go application from Code Repository, perform lint, test and compile on the application and package the application with its resources in a zip file. This file is the artifact that is published to the Artifact Registry. Subsequently, the Deployment Pipeline is triggered -- to deliver this application to a Compute instance and make it run. Prepare Code repository Our pipeline will have precious little to build at present, because our Code Repository is quite empty. The source you can work with are available from the GitHub repository that has been prepared for this series of articles. You will want to get these sources into your OCI Code Repository. The easiest way of making that happen is simply by downloading the GitHub repo's content as a zip file and extracting its contents into your local clone of the go-on-oci-repo Code Repository. Alternatively, create a clone of the GitHub repository and copy all content (except the .git directory) to the directory that has the local clone of go-on-oci-repo. Then, when all sources are available as unstaged files in the local go-on-oci-repo, stage all files and commit. Then do a git push. To bring all the local files into the Code Repository on OCI. You may feel like inspecting the sources through the console to make sure that they are now truly yours, safely held in your cloud based repository. And ready to be used in a build pipeline. Assembling the Build Pipeline The Build Pipeline is a workflow definition, just like the deployment pipeline. It describes the steps to take whenever an automated build is required, the order for the steps and through parameters the context for the steps. It also describes the input -- the sources from the code repository -- and (where to store) the output. The real action happens in managed build stages, on a build server. We will now first create the Build Pipeline and then add three stages to it: managed build, publish artifact and trigger deployment pipeline. Before we can run the pipeline we then also need to take care of permissions through a dynamic group and some policies. Create the Build Pipeline On the overview page for DevOps Project go-on-oci, click on button Create build pipeline. A page is presented for specifying the name -- say build-myserver -- and a description. Press Create to have the build pipeline added to the DevOps Project. Click on the link build-myserver in the list to navigate to the details page. First Stage: Managed Build The first stage in any build pipeline is a Managed Build stage. This stage provides instructions for the pipeline to get hold of a build server, copy specified sources from code repositories to the server and run through a number of actions on that server. At the time of writing, we can use a single image for the build server. It is an Oracle Linux image (8 GB memory, 1 OCPU) that has a number of pre installed tools and language run times. For example: Git, Docker, Helm, OCI CLI, Node.js, Java and Go are on the build runner image. For Go, the current version is 1.16.5. If the build process requires additional or different versions of technologies, you will have to make their installation part of the build process. In the future, we will be able to choose between multiple build runner images (different composition and size) and bring our own images to use. Note: You are charged for using the compute shape (OCPU and memory) during the build run. Reference OCI Docs -- Build Runner Details Click on either the plus icon or the Add Stage card. The two step Add a stage wizard appears. On step one in the wizard, make sure that the Managed Build card is selected for the type of stage. Press Next. The second page is shown. Define a name for the build stage: build-source-to-executable. Optionally type a description. At present we cannot select a different build image, so we settle for the one available, which is fine for our purposes. The default name and location for the build specification is correct: file build_spec.yaml in the root of the repository. Click on the Select button under Primary code repository. We can now specify from which code repository the build will get its sources. Select OCI Code Repository as the Source Connection Type. Then select the go-on-oci-repo repository. We will work with source on the main branch, so do not change that default. Type myserver-sources as the value for Build source name. This managed build stage can use sources from multiple repositories. In the build specification, we can refer to each of these sources using the label defined as Build source name. Click on Save. Press button Add. This completes the definition of the managed build stage. This is all it takes to take sources and process into artifacts. Well, hang on, I hear you think. We may have indicated the sources to use, but we certainly did not say what to do with those sources. Whether any linting, testing, compilation and packaging into a zip file should be performed. And in fact -- we did stipulate exactly what should happen on the build server. It is right there -- in the build-spec.yaml file. We have not talked about that file yet and we certainly did not create it. But it pushed into the code repository and sitting there in the root directory of the project. It is this file that contains the instructions for the actual detailed steps executed on the build server. version: 0.1 component: build timeoutInSeconds: 6000 runAs: root shell: bash env: # these are local variables to the build config variables: SOURCE_DIRECTORY: \"myserver-sources\" # the value of a vaultVariable is the secret-id (in OCI ID format) stored in the OCI Vault service # you can then access the value of that secret in your build_spec.yaml commands vaultVariables: # exportedVariables are made available to use in sucessor stages in this Build Pipeline exportedVariables: - BUILDRUN_HASH steps: - type: Command name: \"Export variables\" timeoutInSeconds: 40 command: | export BUILDRUNHASH=echo ${OCIBUILDRUNID} | rev | cut -c 1-7 echo \"BUILDRUNHASH: \" $BUILDRUNHASH echo \"SOURCE-DIRECTORY: \" $SOURCE_DIRECTORY echo \"${OCIPRIMARYSOURCEDIR}\" ${OCIPRIMARYSOURCEDIR} echo \"fully qual sources\" ${OCIWORKSPACEDIR}/${SOURCE_DIRECTORY} echo \"myserver-version from build pipeline parameter\" ${MYSERVER_VERSION} go version - type: Command timeoutInSeconds: 600 name: \"Install golangci-lint\" command: | curl -sSfL https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh | sh -s -- -b $(go env GOPATH)/bin v1.37.1 - type: Command timeoutInSeconds: 600 name: \"Verify golangci-lint version\" command: | /root/go/bin/golangci-lint version - type: Command timeoutInSeconds: 600 name: \"Run go mod tidy for Go Application\" command: | go mod tidy - type: Command timeoutInSeconds: 600 name: \"Run go vet for Go Application\" command: | go vet . - type: Command timeoutInSeconds: 600 name: \"Run gofmt for Go Application\" command: | gofmt -w . - type: Command timeoutInSeconds: 600 name: \"Run Lint for Go Application\" command: | /root/go/bin/golangci-lint run - type: Command timeoutInSeconds: 600 name: \"Run Unit Tests for Go Application (with verbose output)\" command: | go test -v - type: Command timeoutInSeconds: 600 name: \"Build Go Application into Executable\" command: | go build -o my-server - type: Command timeoutInSeconds: 600 name: \"Zip my-server Application Executable along with website\" command: | zip -r my-server.zip website zip -rv my-server.zip my-server outputArtifacts: - name: myserver-archive type: BINARY location: ${OCIWORKSPACEDIR}/${SOURCE_DIRECTORY}/my-server.zip The build specification consists of three parts: 1. Setup -- who to run the script as, which shell to use, which variables to use 2. Build steps -- Shell commands to execute on the build server 3. Output Artifacts -- indicate which files at the end of all build steps are meaningful and to be made available to other steps in the pipeline (for example to publish as artifact) The build steps can be summarized as: 1. Print environment variables and currently installed Go version (on the vanilla build server) 2. Install golangci-lint 3. Verify success and version of golangci-lint installation 4. Run go mod tidy to organize the go.mod file with dependencies 5. Run go vet to run a first inspection on the Go Sources 6. Run go fmt to format the sources according to generic formatting rules 7. Run golangci-lint to lint (check) the sources against various linting rules 8. Run unit tests 9. Build sources into binary executable called my-server 10. Create zip file my-server.zip with the website directory and the executable my-server If you are eager to see the managed build stage in action, you can skip the next two sections for now, define the dynamic group and the policies described under IAM Policies and then press the button Start manual run. The pipeline grinds into action. A build server is acquired, the build specification is located and the steps in the specification are executed. The logs are shown and provide insight into the proceedings. After a little while, the build is complete. An output is created on the build server but not yet published to the artifact registry. It will now be lost when the stateless build server gets reset for a next run by us or someone else entirely. We will now define the stage that publishes the artifact, to allow us to really enjoy the fruits of our managed build. Reference: OCI Documentation -- Build Specification Reference Second Stage: Publish Artifact In the overview page for the build pipeline, click on the plus icon at the bottom of the current managed build stage. In the context menu that pops up, click on Add stage. The stage wizard appears. Click on Deliver artifacts. Then click on Next. Enter the name for this stage: publish-myserver-as-artifact. We need to select the artifact in the DevOps project that we want to publish. This artifact can be a container image in the image registry or a generic artifact in an artifact registry repository. The latter is the case. Click on button Select artifact(s). Select the my-server artifact that we used previously in the deployment pipeline. We will have our build pipeline produce fresh updates of this artifact. In the area Associate artifacts with build result we have to indicate for each of the artifacts selected which of the outcomes of a managed build stage is the source for publishing the artifact. The build-spec.yaml file defines an output labeled myserver-archive. This output refers to the my-server.zip file that is produced by the build process. Enter this label myserver-archive in the field Build config/result artifact name. The press the Add button to create the Deliver Artifacts stage. If you feel like it -- and already defined dynamic group and policies -- you could press the Start manual run button. This time the build will not only create a zip file on the build server (that is then lost forever) but it will store that zip file in the artifact registry repository. The Build run progress will inform you about it. And when the run is complete and you check the go-on-oci-artifacts-repo repository in the Artifact Registry, you will find the file my-server.zip with a creation timestamp that is very close to now. This is the work of the build pipeline. The deployment pipeline that we created earlier uses this very artifact as a starting point. With this freshly built artifact available, you can now manually start a run of the deployment pipeline to have the new Go application installed and started. Or you can wait a little bit longer and trigger the deployment pipeline from the a stage at the end of the build pipeline. When that is in place, triggering the build pipeline will result in an end to end build and deploy flow. Third Stage: Trigger Deployment Pipeline In the overview page for the build pipeline, click on the plus icon at the bottom of the Deliver artifacts stage. In the context menu that pops up, click on Add stage. The stage wizard appears. Click on Trigger deployment. Then click on Next. Type a name for the stage: trigger-deployment-of-myserver and optionally a description. Click on the button Select deployment pipeline. Select the pipeline deploy-myserver-on-go-app-vm (probably the only deployment pipeline there is to select). Details of the pipeline are shown -- such as parameters (none defined at this point) and artifacts used by the deployment. Click on button Add to complete the stage definition and add it to the build pipeline. This completes the build pipeline: it grabs sources, processes them into a deployable artifact, publishes the artifact to the registry and triggers the deployment pipeline to take it from there. If you have not yet set up a dynamic group and IAM policies, you will do so in the next section. And then we can run the end to end process of build, delivery and deployment. The next figure visualizes the build pipeline and its relation with the deployment pipeline. IAM Policies Build pipelines require permissions to do the things they're supposed to do. They need to read from a Code Repository and publish artifacts to an Artifact Registry, and they need to trigger a deployment pipeline. For all these actions, the build pipeline has to be granted permission through a dynamic group -- just like the deployment pipeline got its permissions. Create Dynamic Group for the Build Pipeline(s) To create the dynamic group, type dyn in the search bar. Click on the link Dynamic Groups in the search results pane. On the overview page for dynamic groups, Click on the button Create Dynamic Group. Enter the name for the Dynamic Group for the Deployment Pipeline(s), e.g. build-pipelines-for-go-on-oci, and optionally type a description. Define the following rule which selects all deployment pipelines that are part of the compartment (in this case we have not any deployment pipelines, but we soon will): ALL {resource.type = 'devopsbuildpipeline', resource.compartment.id = ''} Of course, replace with the identifier of the compartment you are working in. Then press Create. It is convenient to define the dynamic group in this broad fashion -- simply including all resources in the compartment of type build pipeline. In a realistic environment, I recommend defining dynamic groups and policies as fine-grained as possible so as to not grant more permissions than are needed. Define Policies that Bestow Rights on the Dynamic Group To create a policy in the console, type poli in the search bar and click on Policies | Identity in the Services area of the search results popup. This takes you to the Policies overview page for the current compartment. The first policy defines the permission for the build pipelines to deliver artifacts to the Artifact Registry. Define a name, a description and the following statement: Allow dynamic-group build-pipelines-for-go-on-oci to manage all-artifacts in compartment go-on-oci To send notifications for the build process, provide access to ONS (notification service) to the build pipelines: Allow dynamic-group build-pipelines-for-go-on-oci to use ons-topics in compartment go-on-oci And to allow the build pipelines to read deployment artifacts in the Deliver Artifacts stage, read DevOps code repository in the Managed Build stage, and trigger deployment pipelines: Allow dynamic-group build-pipelines-for-go-on-oci to manage devops-family in compartment go-on-oci With these policies in place, the build pipeline can be taken for a spin. Run the Build Pipeline The Build Pipeline can be triggered by events in the Code Repository. Simple events such as commits or slightly more complex events like merging to a specific branch (such as merge of a pull request to the main or master branch) can be set up to trigger the pipeline. It can also be triggered manually from the console. That is what we'll do next. Click on Start manual run in the overview page for the build pipeline. The build pipeline is kicked into action. It acquires a build server, retrieves the code and saves it on the build server. Then it performs the steps in the build specification. The outputs from this build process are published from the build server's file system, uploaded to the Artifact Registry, where they will be waiting for the Deployment Pipeline to come along and fetch them. And indeed this is what happens next: the build pipeline's final stage triggers the deployment pipeline. This has the artifact as its source and continues on to write the artifact to the Compute instance and perform the installation steps. You will receive emails about the start and completion of both the build pipeline and the triggered deployment pipeline. You may start to think about setting up routing rules to a specific mail folder for emails from OCI DevOps. When the Build Pipeline is completed successfully, the triggered deployment can still be running... or can have failed miserably. The build pipeline does not hear back from the deployment pipeline. Once you see that the deployment pipeline has completed successfully, we should be able to access the application that was deployed based on the artifact built based on the sources in the code repository. Validate if the application is indeed running. http://:8080/.... Introducing Parameters We want our sources, as well as our deployment artifacts, to be independent of their environment. The same artifacts can be deployed in different environments and work well, perhaps calling to different endpoints, listening to environment-specific ports, and writing to environment specific file system locations. At deployment time, the artifact can be combined with these environment-specific values. OCI DevOps Deployment Pipelines support this way of working. You can use placeholders in the Deployment Configuration file and in artifacts; these placeholders are replaced at deployment time with the value of the corresponding parameters or secrets in the OCI Vault. Moreover, the version indication of artifacts can be defined using a placeholder which is replaced at deployment time by the actual value of a parameter. Build pipelines can also work with parameters. Parameters can be used in the managed build -- replacing placeholders in the build specification. A build pipeline parameter can also be used to determine the version label of the artifact that is published by the pipeline and this parameter can subsequently be leveraged in the deployment pipeline to fetch that exact artifact version to be deployed. We will now make use of a parameter called MYSERVER_VERSION in three places: * in the custom path of a DevOps Project Artifact's definition * a parameter in the Deployment Pipeline * a parameter in the Build Pipeline And to show off a little, the parameter can also be used in the build_spec.yaml (because any build pipeline parameter is available as an environment variable in the build server session) and the inline configuration definition (where environment variables in the deployment environment can be defined based on pipeline parameter values). Placeholder in DevOps Artifact Version In the DevOps project, go to the Artifacts tab. Click on three dots for the artifact called my-server and select Edit from the menu. The Edit artifact dialog appears. Change Artifact Location to Set Custom Location. The field Artifact Path should have the value my-server.zip and the Version should be set to ${MYSERVERVERSION}. This value means that whenever the DevOps project refers to the artifact -- from a Build Pipeline or a Deployment Pipeline -- the specific version to use of the artifact is to be derived using the value of parameter MYSERVERVERSION. As a consequence, any reference to the artifact at a time when there is no value set for a parameter with this name is meaningless. Select Yes, substitute placeholders under Replace parameters used in this artifact to make sure that the value of the parameter is actually applied to compose the version (and ${MYSERVER_VERSION} is not regarded as a simple string). Press Save to apply the changes in the artifact definition. Deployment Pipeline Parameter Deployment Pipelines can have parameters associated with them. These parameters have a name, a default value, and a description. When a run is started for a deployment pipeline, the default values of the parameters can be overridden, setting appropriate values for that specific deployment. The values for the parameters are used to resolve ${placeholder} expressions which can be in the deployment specification, in the artifact's version definition, and in the content body of any of the deployed artifacts. To set parameter values, go to the Parameters tab on the Deployment Pipeline page. Add two parameters: 1. MYSERVER_VERSION to define the version of the my-server artifact to deploy; set the default value to 4.8 2. HTTPSERVERPORT to define the port on which the my-server application will listen to incoming HTTP requests; set the default value to 8090 The value for MYSERVERVERSION is immediately meaningful: the next time you run the deployment pipeline, the artifact to retrieve for the deployment is found using the artifact definition in the DevOps project, with its path set to my-server.zip and version defined as ${MYSERVERVERSION}. The value for version is deduced using the value of the parameter. If you run the deployment pipeline right now, it will fail because there is no artifact my-server.zip with version 4.8. We need to update the build pipeline to produce that artifact (or go to the artifact repository and manually set the version label to this value). The second parameter -- HTTPSERVERPORT -- has no effect yet. It is defined, and it is available when the deployment pipeline is started, but it is not yet used. We can change that. Go to the Artifacts tab in the DevOps project. Click on the link myserver-to-vm-deployment-configuration for the inline artifact with the deployment configuration. Then click on Edit to update the configuration. Add two environment variables, as shown next. This will cause two environment variables to be available during deployment, called VERSIONOFMYSERVER and HTTPSERVERPORT with their values derived from the deployment pipeline parameters. yaml version: 1.0 component: deployment env: variables: VERSIONOFMYSERVER: ${MYSERVER_VERSION} HTTPSERVERPORT : ${HTTPSERVERPORT} files: The environment variable HTTPSERVERPORT is meaningful: it is read in the my-server application and interpreted as the HTTP port on which the application is listening to incoming HTTP requests. The default value that was used until now is 8080. With the environment variable now set from the pipeline parameter that has 8090 as its default value, things will change upon the next deployment: the application will listen on port 8090 -- as it states in the deployment log. In the same line, the application also indicates its own version. That information was read from environment variable VERSIONOFMYSERVER. To verify that the environment variables are set as desired, you can add this step to the deployment configuration, which will write the two environment variables to the output: - stepType: Command name: Report command: echo myserver version to install in this deployment $VERSIONOFMYSERVER && echo port to make the application listen to $HTTPSERVERPORT timeoutInSeconds: 60 - stepType: Command Build Pipeline Parameters Build pipeline parameters can be used during the build process. The parameters are available as environment variables on the build server during the execution of the build process. Additionally, when a deployment pipeline triggered from a build pipeline has parameters with the same name as parameters defined in the build pipeline, then the value set to the build pipeline parameter is passed to the deployment pipeline and used for the corresponding parameter. This is used, for example, to pass the value of the parameter that identified the artifact version. Go to the Parameters tab for the Build Pipeline. Define a new parameter called MYSERVER_VERSION. Its description can be something like \"Version of the myserver artifact that is produced.\" The default value can be set to 4.8 (or anything else). When you run a build pipeline, you can override the default value for every parameter. End to End Parametrized Build and Deployment By defining the parameter MYSERVERVERSION for both the build pipeline and the deployment pipeline as well as in the artifact's version label, we have tied the two pipelines together. When we run the Build Pipeline and set the value for MYSERVERVERSION to a value such as 4.8 (or any other version label like string) then the build pipeline will produce the my-server.zip artifact with the version set to that value and the deployment pipeline will take that artifact version as its source for the deployment. During build -- as well as during deployment -- environment variables are available based on the parameter(s) defined for the pipeline. Run the build pipeline, and when prompted, provide a value for the parameter MYSERVER_VERSION. It can be 4.8, but 5.7 or 9.3 or anything else is fine, too. Inspect the logging for both the build pipeline and the deployment pipeline for the appearances of the environment variables. One of the final log lines in the deployment log reads: Starting my-server (version 4.8) listening for requests at port 8090 . The version indicated depends on the value you defined for the parameter MYSERVERVERSION. The port similarly depends on the parameter HTTPSERVER_PORT. Note that any port different from 8080 has a consequence: the myserver friendly greet service is currently not accessible from outside the Compute instance. For port 8080, we have defined an Ingress rule in the subnet's network security list (in part one of this series) and we have added this port to the firewall on the VM. With the application listening on a different port, we have to make sure that this port is opened up in a way similar to port 8080. Feel free to do so, or to revert back to port 8080 by changing the value of deployment pipeline parameter HTTPSERVERPORT to 8080 and running the pipeline again. > Note: Opening up the port in the Compute instance's firewall rules can be accomplished as part of the deployment by adding this step to the deployment configuration: > > > * stepType: Command > name: Report > runAs: root > command: firewall-cmd --permanent --zone=public --add-port=${HTTPSERVERPORT}/tcp && firewall-cmd --reload > timeoutInSeconds: 60 However, this command requires sudo permissions, and this means that you must grant sudo permissions to the Compute Instance Run Command plugin to be able to run the command. The plugin runs as the ocarun user and this user must be explicityly allowed to run all commands as sudo. See for details on how to realize this: OCI Documentation -- Running Commands with Administrator Privileges. API Gateway: Exposing Your Server to the World Even though the Compute instance to which the myserver application has been deployed has a public IP address, the service is currently not accessible from the outside. The network security that was set up in the first installment in this series allowed traffic to port 8080, but not to 8090. We don't want any traffic directly to our production servers, and public IP addresses for Compute instances are not generally a good idea. Services that should be accessible to consumers outside our cloud tenancy should be published as APIs on an API Gateway. Not only can we protect the VM from network traffic from the public internet, we also encapsulate our service's location and implementation, allowing us to change such implementation details without impacting the consumers of the API. Additionally, the API Gateway allows us to aggregate services into a single API, use better paths, headers and parameters, and enforce authorization. The API Gateway can perform request and response manipulation, enforce throttling rules to protect the backend service, and provide detailed insight in the usage of the API. Without going into all features the API Gateway offers, we will create an API Gateway with a Deployment that has a single route for exposing the service offered by application myserver, built from the Go application sources and deployed to the Compute instance. Create API Gateway Type gat into the search bar in the console. Click on the link Gateways | API Management. Click on the button Create Gateway. Enter a name for the new gateway, for example the-api-gateway. Accept the type Public. Select the same VCN used for the Compute instance and the same public subnet and do not enable network security groups. Accept the default setting under Certificate. Press Create Gateway. > Note: we are doing a very simple deployment using the smallest number of OCI cloud resources we can get away with. In a real world scenario, we would most probably not have the API Gateway on the same subnet as the Compute instance it is routing to because we're trying to achieve insulation of the backend VM from the public internet using the front end API Gateway. Create Deployment The API Gateway will now be created. Once that has been done, navigate to the Deployments tab. A Deployment on an API Gateway is a collection of (incoming) routes that are mapped to backend services to handle requests. Click on the button Create Deployment. The first page of a three-page wizard appears. Here we define the name of the deployment, e.g. myserver-api, and the path prefix used by all API Gateway requests to be handled in this deployment. For example: /my-api. Click on Next to move to the second page in the wizard. On this page, we define each of the routes this deployment will take care of. Let's start by defining just a single one. Type /welcome as the path. This means that requests to the API Gateway looking like https:///my-api/welcome will be handled by this route. Select the GET method -- the only HTTP method this route will be able to handle. Accept the Type HTTP. In the URL field type the HTTP endpoint for the service exposed by myserver on the Compute instance. At this moment, it is the same endpoint you have used in the browser to access the service, because right now the Compute instance has a public IP and the network configuration allows that direct traffic. With the API Gateway handling the inbound traffic, we can restrict network access to the VM, as long as the API Gateway can make requests to it. Click on button Next to go to an overview of the definition of the deployment. Click on Save on this third page to create deployment with its single route. > It takes a little while for the API Gateway to be updated with the deployment. Update Network Security and Invoke API On the Deployment Details page you'll find the (public) Endpoint for the deployment. CLick on the Copy link. Now paste the link in your browser's address bar, add /welcome to the URL, and press enter. https:///my-api/welcome This probably will not give the expected result, at least not if you expected success. The request we sent to the API Gateway is sent over HTTPS to the default HTTPS port of 443. The public subnet that we associated with the API Gateway was configured in the previous installment of the series to allow inbound traffic for port 20-22 (for SSH connections) and for port 80 (plain HTTP traffic). We now need to extend that definition to also allow ingress traffic to port 443. Type net in the search bar and click on link Virtual Cloud Networks in the services list. Click on the VCN that was created earlier and subsequently on the Subnet that was associated to the API Gateway. Click on the Security List. Click on Add Ingress Rule. Define a new Ingress Rule with Source CIDR set to 0.0.0.0/0, Destination Port Range set to 443, and optionally a description of the rule. Press button Add Ingress Rules to save the new rule. Now try again once more to access the /welcome route on the API from your browser: https:///my-api/welcome This time around, you should get the expected response. Adding a query parameter: https:///my-api/welcome?name=No+Stranger Requests to the public endpoint of the API Gateway and with the configured path prefix of my-api and the one supported route of welcome are forwarded to the API Gateway to the myserver application on the Compute instance, and the request is returned to us. In the next section we will show one example of request manipulation in the API Gateway. Connecting API Gateway to a private Compute Instance Because of our earlier work with the go-app-vm Compute instance, it has a public IP and is associated with a subnet that allows all kinds of inbound network traffic from any source on the public internet. However, in reality it will be more likely that we create a Compute instance that has no public IP and is associated only with a private subnet. In that case, the following are the high level steps to configure the API Gateway to route requests to the VM as a backend for an API route. There are three elements that need to be addressed. 1. Firewall on the VM : as before, make sure that the appropriate ports are added to the firewall on the VM (as we have seen and done before) 2. Network Route Table: configure the network route table to make sure that traffic can route between the public subnet with which the API Gateway is associated and the private subnet to which the VM is linked 3. Network Security Groups (NSG): create an NSG for the VM and one for the API Gateway. Add an ingress rule for the VM's NSG to allow traffic from the NSG defined for the API Gateway and, on the latter NSG, an egress rule to allow traffic to the VM's NSG. You can allow all ports, or limit it to the specific port(s) on which the service in the VM listens. The great thing about NSG is that you do not need to worry about IP addresses of the API Gateway or VM: just attach the NSG to the gateway and the VM with the rules in place and it will be configured. One Tiny Little Bit Of Api Gateway Magic The OCI API Gateway can help us in many ways with handling requests. From authorization, validation, and throttling to transformation of headers and parameter, routing and reporting, as well as caching. To give you a little taste of that, let's configure the deployment route to set a default value for the name query parameter if no value is provided in the request. At the moment, when the URL is invoked without a query parameter called name https:///my-api/welcome the response will be Hello Stranger! We will change that behavior, and ensure the response will be Hello Friend!. We can define validations and transformations on query parameters and headers. One simple transformation allows us to set a value for a query parameter if no value was set already. The API Gateway maybe called without query parameter name but in that case we will make sure the request to myserver will have that parameter, with the appropriate value of Friend. Go to the details page for the (API) deployment myserver-api. Click on Edit. Go to the second page with Routes. Click on the link Show Route Request Policies. Click on the Add button under Query Parameter Transformations. - Action: Set - Behavior: Skip (do not change value when already set) - Query Parameter Name: name - Value: Friend Click on Apply Changes. Then click button Next, then Save Changes. The deployment will be updated with the changes. When done, try again to access the URL in the browser, without the name query parameter: https:///my-api/welcome The response should be Hello Friend!, thanks to the magic of the API Gateway Query Parameter Transformation. Conclusion In this article, we continued our journey with Go on OCI. The main focus in this article was automation. Using the OCI DevOps services, we created pipelines for build and deployment of a Go application, from sources in the Code Repository to a deployed and running application on a Compute instance. We used API Gateway to expose our service in a decoupled way to external consumers for better security, reduced dependencies and improved operations. We saw a small example of API Gateway's capabilities for validating and manipulating requests and responses. In the next article, we will create Serverless Functions in Go and deploy them on OCI. And we will start using the Go SDK for OCI that enables us to interact with OCI services from Go applications. The first service will be the OCI Object Storage service for creating buckets and reading and writing files from a local Go application and from the Functions deployed to OCI. Resources Source code repository for the sources discussed in this article series [OCI Documentation -- Create DevOps project ](https://docs.oracle.com/en-us/iaas/Content/devops/using/createproject.htm#createa_project ) OCI Documentation -- Running a Command on an Instance OCI Documentation -- IAM Policies on Artifact Registry By Lucas Jellema","categories": ["clouddev","cloudapps"],
        "tags": ["open-source","devops","get-started","automation","back-end","go","iac"],
        "url": "/tutorials/way-to-go-on-oci/way-to-go-on-oci-article2",
        "teaser": ""
      },{
        "title": "Oracle Cloud Infrastructure Functions in Go and Using the OCI Go SDK for accessing OCI services from Go",
        "excerpt":" This is the third installment in a five part series about Go and Oracle Cloud Infrastructure. This series discusses how Go applications can be created and run on Oracle Cloud Infrastructure (OCI) in Compute Instances (VMs), containerized on Kubernetes, or as serverless Functions. The articles show how to automate the build and deployment of these Go applications using OCI DevOps. An important topic is how to use OCI services from Go applications, both those running on OCI as well as Go code running elsewhere. Some of the OCI services discussed are Object Storage, Streaming, Key Vault and Autonomous Database. In order to follow along with these articles, readers should have at least basic knowledge of how to create Go applications. It is assumed that readers have access to their own Go development environment. Some of the examples and screenshots will specifically mention VS Code as development tool. However, other editors and IDEs can be used as well. The Go code presented in these articles demonstrates a number of mechanisms in their simplest form for maximum clarity and with the least dependencies. Readers should not expect meaningful functionality or production ready code. The articles describe how to get Going on OCI. To try out the examples, readers will need to have access to an OCI tenancy with permissions to create the OCI resources discussed in these articles. Most of the resources used are available in the Aways Free Tier (Compute Instance, VCN, Autonomous Database, Object Storage, Logging, Resource Manager), or have a free allotment tier for limited monthly usage (Functions, API Gateway, Streaming, Vault, DevOps). Introduction The first part of this series describes provisioning of a Compute Instance based on the Oracle Linux Cloud Developer image, opening it up for inbound and outbound network activity, creating and running a Go application that serves HTTP requests, and connecting logging produced by the application to OCI Logging. Part two deals with software engineering, automation of build, and deployment of the application with the OCI DevOps service. This service is used for storing the Go source code, building the application executable, storing it as deployable artifact, and deploying that artifact to a Compute Instance. The second article also shows how to expose an HTTP endpoint for the application through an OCI API Gateway. This third part shows how to create serverless functions in Go and deploy them on OCI. The Go SDK for OCI is introduced -- first for local, stand alone Go applications and subsequently for use from functions -- leveraging resource principal authentication. This SDK is used to interact with the OCI Object Storage service for creating buckets and writing and reading files. Initially the function is built and deployed manually. A route is added to the deployment in API Gateway for invoking the function from a client external to OCI. Then an OCI DevOps Deployment Pipeline is created for deploying the function from an image in the Container Image Registry. Finally, a build pipeline is set up to take sources in the code repository, build and publish a container image, and then trigger the deployment pipeline for end-to-end build and deploy. OCI Functions in Go Serverless function in OCI are based on the technology of the open source Project Fn. The business logic of the function is written in your favorite language -- in this case Go -- and embedded in the Fn framework that handles the life cycle of the function and the interactions with the function. The Fn framework can be run anywhere: on your local machine, in a VM on any cloud, or on premises. Oracle Cloud Infrastructure offers a fully managed PaaS service OCI Functions for serverless functions based on that same technology. A Function is built into a container image. This image is pushed to a container image registry. To publish the function, this image is transferred to an Fn Server. Whenever the function is invoked, a container is started from the image and the request is processed. Containers are kept running for some time after handling an invocation, in a hot state ready to handle additional requests. When the number of concurrent requests increases, additional containers for the same function will be started by the Fn Server to ensure all requests can be handled. The attraction of functions for developers and application operators is the fact that no energy needs to be poured into designing, creating and managing the platform that runs the business logic. All focus can be on writing that logic. We will now look at creating the function in Go, building it into a container image, deploying and running it locally. Then we will take this function to the OCI Functions service and make it run cloud side. The Fn Development Environment To develop functions, you will need an environment that supports Project Fn. Fn is a lightweight Docker-based serverless functions platform you can run on your laptop, server, or cloud. You can install Fn easily on Linux or MacOS by following the instructions in Fn Project Tutorials -- Install Fn. You can choose to work on the go-app-vm compute instance that we have created in the first and used also in the second installment of this series. This Oracle Linux environment does not come with Fn set up, but installing it is fairly simple. Alternatively, you can work in OCI Cloud Shell. This browser-accessible environment is set up with Fn. For working with Fn in OCI Cloud Shell, see OCI Documentation Functions: Get Started using Cloud Shell. Develop an Fn Function In the development environment with Fn CLI installed, navigate to a directory where you want to create the function's subdirectory. On the command line, enter this command and execute it: fn init --runtime go --trigger http greeter A subdirectory called greeter is created. Navigate into it and check its contents: cd greeter ls -l The file func.yaml contains the meta data about the function, to be interpreted by the Fn framework when building, and later on when running the function. File go.mod contains the dependency the function has on the fdk-go package. The actual function itself is in func.go. The structure of the function and its interaction with the Fn runtime can be seen here: function main registers the function myHandler with the Fn runtime, which instructs and enables the runtime to invoke this function for any HTTP request received. The function receives the body of the request in an io.Reader parameter. It also receives the output of io.Writer, to which the response body can be written. The context.Context parameter ctx contains meta data for the original request, including HTTP headers, the full URL, the request method, and the function itself, including all configuration variables defined for it. Currently, the myHandler function decodes the request body, expecting it to contain a JSON payload with a field called name. It creates a Person with their name set to the value of this field or, in its absence, defaults to World. It then creates the expected response: a JSON object with a single field called message, which contains a string composed from Hello and the name value. Although it does not do anything truly spectacular, the function is sound and complete and we can locally deploy and invoke it. For this we need a local context and the local Fn server up and running. Check the contexts using: console fn list contexts This shows a list of at least one context -- possibly more than one. To work with the local Fn server, make sure that the default context is the active one. If needed to set the current context to default, use: console fn use context default Now create an application as the host for the function: console fn create app go-oci-app fn list apps If the first of these statements fails with connection refused, then the server is probably not yet running. Use the next command for starting the server, then try again to create the application. console fn start With the application successfully created, the function can now be deployed into it. The next command takes care of this deployment; it's preceded by the container image build process. console fn --verbose deploy --app go-oci-app --local Specifying --local does the deployment to the local server but does not push the function image to a Docker registry, which would be necessary if we were deploying to a remote Fn server. Because it unleashes an impressive amount of log messages to be produced, the --verbose flag is not one you will use all the time. However, it gives you pretty good insight into what's going on. Several container images are pulled, then a two-stage container build process is executed for creating a container image for the greeter function. Predefined Fn project images are used for the build stage (fnproject/go:1.15-dev at the time of writing) and as the foundation for the runtime image (fnproject/go:1.15). The final output will look like this: Updating function greeter using image greeter:0.0.2... Successfully created function: greeter with greeter:0.0.2 Successfully created trigger: greeter Trigger Endpoint: http://localhost:8080/t/go-oci-app/greeter The function image is called greeter:0.0.2. You will find this image in the local container registry with: console docker images | grep greeter The function can be invoked through the Fn CLI, using its name and application, like this: console fn invoke go-oci-app greeter The function expects a JSON payload that contains a name field, so let us provide it with exactly that: console echo -n '{\"name\":\"Clark Kent\"}' | fn invoke go-oci-app greeter --content-type application/json The output from the function deployment also gave us the Trigger Endpoint for the function. This is an HTTP endpoint to which we can send an HTTP request and have it trigger the function. We have no (visible) interaction with Fn, although the endpoint we invoke is really the Fn Server endpoint. The URL path tells Fn the application and specific function to trigger. console curl -X \"POST\" -H \"Content-Type: application/json\" -d '{\"name\":\"Mickey Mouse\"}' http://localhost:8080/t/go-oci-app/greeter Create OCI Function Now, let's create this Function on OCI instead of just in our development environment. The steps are very similar to the ones we used for creating the function locally; we only need to use a different context. Not the local context but one for OCI. Create Application Let's first create the application through the OCI Console. Type app in the search box and click on Applications > Functions in the Services Area. Click on button Create Application. Type the name for the application: go-on-oci-app. Select the VCN that was created in part one of the article series and its one public subnet. Then click on Create to create the application. Prepare Local Environment for OCI Interaction and Function Image Push Once the application is created, the General Information for the application is presented. The page also contains instructions for creating your first function, either in the OCI Cloud Shell, or in a local setup (which could, of course, also be the go-app-vm compute instance). If you are using the OCI Cloud Shell, the steps for creating this context are slightly different (and simpler) than when you work in a regular development environment. Feel free to follow the OCI Shell setup. In this article, we will take the other path, used for any local development environment. There are a number of steps to take in a local environment (in which Fn CLI was previously installed): 1. Set up an API Signing Key and store the private key in a .pem file in the local HOME/.oci directory and upload the public key to the OCI Console -- see instructions in OCI Documentation -- Required Keys. 2. Create file config in the .oci directory in the local environment; copy the Configuration File Preview snippet to the config file. Update the key_file entry in the file: make it refer to the private key's pem file. OCI Documentation -- SDK and CLI Configuration File. 3. In order to push container images to the OCI Container Image Registry, you need an authentication token. In part one of this article series, you created a token to use for logging in to the DevOps Code Repository from the git client. You can reuse that token for logging the Docker client into the container image registry, or you can can create a new authention token. In the latter case, see OCI Documentation -- Getting an Authentication Token. 4. You will need the OCI CLI. Instructions to install this tool can be found in the OCI Documentation: Working with the OCI Command Line Interface -- Quickstart . The OCI CLI will use the HOME/.oci/config file and the referenced private key for making secure connections to the OCI APIs. After these steps, you can try out the success of steps 1, 2, and 4 with this command, which should return a list of the compartments in your tenancy: console oci iam compartment list Optional: Create Container Image Registry Repository If the user account used for deploying the function has the necessary IAM permissions, then deployment will create the repository for the function images in the Container Image Registry. In case those privileges are not available or you want to prepare the repository, you can do so as follows. 1. Type regi in the search bar. Click on link Container Registry > Containers & Artifacts. 2. Click Create repository. Type the name of the repository: go-on-oci/greeter. This is comprised of the repository prefix and the name of the function, in which the repository will contain the images. Set the Access to Public. 3. Click on button Create repository. After a few seconds, a new and empty container image repository is created, ready to receive the function (container) images that we will push using the Fn CLI. Create a Context for OCI in Fn CLI Moving back to the command line of the local environment, we need to create a Fn context for the current compartment on OCI, and subsequently select it for use in Fn operations. Execute these commands (which you can copy from the Getting Started tab on the go-on-oci-app page): console fn create context go-on-oci --provider oracle fn use context go-on-oci Copy the commands under step 4 to update the context with the compartment OCID and the Oracle Functions API URL. In my case: console fn update context oracle.compartment-id ocid1.compartment.oc1..aaaaaaaaqb4vxvxuho5h7eewd3fl6dmlh4xg5qaqmtlcmzjtpxszfc7nzbyq fn update context api-url https://functions.us-ashburn-1.oraclecloud.com The command will be similar but different for you. Provide the unique repository name prefix. Use go-on-oci and specify the compartment that contains the image registry repository to which the function image must be published: console fn update context registry iad.ocir.io/idtwlqf2hanz/go-on-oci fn update context oracle.image-compartment-id Log into the Registry using the Auth Token as your password: console docker login iad.ocir.io In my case, the region I work in is Ashburn, identified by the region key iad.ocir.io. I am prompted for the username. This is a string that includes the namespace prefix included in the name of the Container Image Registry and each repository. Subsequently the password is requested. Here, you provide an authentication token set up for the user, which we used before in the previous article when the login was performed in the code repository. The next command shows a listing of the applications in the current Fn context: console fn list apps The list contains one application, called go-on-oci-app. The function greeter that was created, locally deployed, and invoked earlier can now also be deployed to the OCI Application to become a cloud-native, serverless function. The command we use for deployment is the same as we used before. Its effect is dramatically different due to the changed context. Instead of a local context, there is now a context based on an OCI Provider and linked to an OCI Tenancy and Compartment. The container image is pushed to the OCI Container Image Registry, and the Function is created in the OCI Function service. console fn -v deploy --app go-on-oci-app The output is similar to what was generated before, but the build process is exactly the same. Once the function container image is ready, things start to deviate. The image is pushed to the OCI Container Image Registry, and the function is deployed to the cloud. The related lines in the output: => exporting to image 0.0s => => exporting layers 0.0s => => writing image sha256:008dc3b990f1e69d67a7dd8649fbd63649d72f0bf1a161b2c2e073064f16c918 0.0s => => naming to iad.ocir.io/idtwlqf2hanz/go-on-oci/greeter:0.0.3 0.0s Parts: [iad.ocir.io idtwlqf2hanz go-on-oci greeter:0.0.3] Using Container engine docker to push Pushing iad.ocir.io/idtwlqf2hanz/go-on-oci/greeter:0.0.3 to docker registry...The push refers to repository [iad.ocir.io/idtwlqf2hanz/go-on-oci/greeter] ... e57f007acf74: Pushed 0.0.3: digest: sha256:bb4f2abde44d97517520571a21c407e349ddfc6572583a8ba53717436fd0b7f5 size: 1155 Updating function greeter using image iad.ocir.io/idtwlqf2hanz/go-on-oci/greeter:0.0.3... Successfully created function: greeter with iad.ocir.io/idtwlqf2hanz/go-on-oci/greeter:0.0.3 Fn: HTTP Triggers are not supported on Oracle Functions At this point, the function is in the cloud, and it can be invoked (still using the Fn CLI): console fn invoke go-on-oci-app greeter The first call will take quite some time because the function starts out cold and the underlying container image needs to be instantiated into a running container. Every subsequent invocation of the function will be much faster. Note that if you wait for ten minutes and the function goes cold, the container is stopped. This image describes the situation we have arrived at: You can check in the OCI Console for the evidence of what just happened. Type greeter in the search box in the console. Under Resources there will be an entry greeter > Functions. Click on the link to go to the page showing details for the function. You will find references to the function image, the memory setting and the endpoint for invoking the function. Under metrics you should find evidence of the call to the function made using the Fn CLI. In the search results for greeter, you'll also find the Container Repository go-on-oci/greeter. When you navigate to the repository, you'll find details for the image(s) published to it. Create API Gateway Route for Function OCI Functions can not just be invoked. Even though they have an HTTP endpoint that seems to suggest you can just call them from your browser or curl on the command line, it's not actually quite that simple. HTTP calls to functions need to be signed, and this signing process is not simple and straightforward. A better way to allow consumers to invoke functions is through an API Gateway. We used an API Gateway in the previous article to open up a public route to the myserver application running on a (potentially) private compute instance. Now we will do the same for the greeter function using an additional route in the API Gateway the-api-gateway and the deployment myserver-api created in the previous article. Setup IAM Access for the API Gateway The API Gateway needs to be allowed to invoke the function, using a policy that provides permission for the API Gateway to invoke functions. Create the policy for API Gateway to invoke functions. To go create a policy in the console: type poli in the search bar and click on Policies > Identity in the Services area of the search results popup. This takes you to the Policies overview page for the current compartment. The policy defines the permission for the API Gateways to access resources in the compartment. Create a new policy, type a name (invoke-function-for-api-gateway), a description, and the following statement: ALLOW any-user to use functions-family in compartment where ALL {request.principal.type= 'ApiGateway', request.resource.compartment.id = ' with the name of the compartment, which is probably go-on-oci. Replace with the identifier of the compartment you are working in. Define the Route for the Function in the Deployment on the API Gateway With the permissions taken care of, we can now define the route on the API Gateway. Type gat into the search bar in the console. Click on Gateways > API Management. Click on the link for the-api-gateway. Click on Deployments. In the list of deployments (which contains a single deployment), click on the link myserver-api. Click on button Edit to open the deployment specification. Click on the link for the second step: Routes. Scroll down and click on the button + Another Route. Type /greeting as the path for this new route. Select GET as the method and Oracle Functions as the Type (of backend). Select application go-on-oci-app and then set Function Name to greeter. Press Next. Then press Save Changes to apply the changes and make the new route real. Invoke the Function through the API Gateway With the new route set up and the deployment refreshed on the API Gateway, we can now make a simple, straightforward HTTP request to the public endpoint of the API Gateway, indirectly triggering the function greeter and receiving its response. Using this URL in any browser, you should be able to get the function's response: https:///my-api/greeting The response is a little underwhelming, but that's expected with such a simplistic function. Using curl, you can send a JSON payload to the function and receive a slightly more interesting response. console curl -X \"GET\" -H \"Content-Type: application/json\" -d '{\"name\":\"Mickey Mouse\"}' https:///my-api/greeting The response reads {\"message\":\"Hello Mickey Mouse\"}. So now we have established the end to end flow from the API Gateway to the serverless function. And we have a way to manually deploy the function based on the sources in our local development environment. To leverage our work, you can make some changes to the source code in func.go and then deploy the function once more -- a single command with the Fn CLI -- and invoke the greeting route on the API Gateway to see that our change is live. For example: change the line that sets the value for Msg to Msg: fmt.Sprintf(\"Warmest greetings from your function dear %s\", p.Name), Save the updated func.go source. Then execute these commands to deploy the updated function and subsequently invoke it: console fn -v deploy --app go-on-oci-app curl -X \"GET\" -H \"Content-Type: application/json\" -d '{\"name\":\"Mickey Mouse\"}' https:///my-api/greeting This should result in the improved response. The build and deploy process are condensed to a single manual command in a prepared environment. Next we will look at an automated deployment process for functions using OCI DevOps, followed by the preceding automated build process based on source in a code repository. Then we will move onto functions that do a little bit more than return simple greetings. Automated Deployment of Functions In the previous installment in this series, we saw the use of OCI DevOps Deployment Pipelines for deploying an application to a compute instance. Now we'll use a pipeline for automated deployment of a function. The overall approach and ingredients are similar. We need an artifact, a (target) environment, and the deployment pipeline with a Function Deployment stage, as well as IAM permissions for the pipeline to read the artifact and deploy the function. These ingredients in more detail: 1. An Artifact: the reference to, in this case, a specific Container Image in the OCI Container Image Registry, using the fully qualified path to the repository and the specific image and version. 2. An Environment: the reference to the Function I want to (re)deploy. The Environment in the case of Function deployment is not the compartment or an application in a compartment (as one might surmise), but the function itself which therefore already needs to exist before it can be deployed through an OCI DevOps Deployment Pipeline. (Note that the Function does not have to be useful -- it can be based on the Scratch container image.) 3. A Deployment Pipeline with a Deployment Pipeline Stage of type Function Deployment that connects the Artifact and the Environment. 4. A dynamic group that contains the deployment pipeline, and IAM policies that allow the dynamic group to read artifacts (such as function container images) and to deploy functions (broadly speaking, manage OCI resources). Create DevOps Artifact for the Function Container Image In the OCI Console, navigate to the home page for the DevOps Project go-on-oci. Open the Artifacts tab. Click on button Add artifact. Note that what we define here is a link or a proxy from the DevOps Project to an actual artifact, not the artifact itself. Enter greeter-function as the name of the DevOps artifact. The type should be set to Container image repository. The fully qualified path to the image consists of the region key, the repository namespace and prefix, the function name and the function version tag. In this case, use a placeholder for the version tag. The path is now defined as follows: ///greeter:${imageVersion Set the drop down field Replace parameters used in this artifact to Yes, substitute placeholders. Click on button Add to complete and save the definition of the artifact. Define the DevOps Environment for the Function Open the Environments tab in the DevOps project. It contains the go-on-oci-vm environment that was created for the deployment of myserver to the Compute instance (in the previous article). Click on button Create environment. In the first step, Basic information, click on the tile Functions - Create an environment for a Function. Enter greeter-function-in-app-go-on-oci-app as the name of the environment. Press Next to go to the second step with Environment details. Confirm the Region, Compartment, Application and Function -- you probably do not have to change any of these settings. If you do, ensure that function greeter in application go-on-oci-app is selected. Click on Create environment to save the definition. Create the Deploy Pipeline for Deploying the Function On the DevOps Project's overview page, click on Create pipeline. The Create pipeline form is presented. Type a name (deploy-greeter-function-to-go-on-oci-app) and optionally a description. Then click on Create pipeline. The deployment pipeline is created, though it's quite empty: not an environment into which it should deploy, no artifacts that are to be deployed, and no configuration file to define the steps to execute. In the pipeline editor that appears, click on the Add Stage tile (or on the plus icon). The next page shows a list of stage types. Click on the tile labeled Uses the built-in Functions update strategy. Press button Next. Type the stage name, e.g. update-function-greeter. Select the environment that was defined earlier for the function: greeter-function-in-app-go-on-oci-app. Under the heading Artifact, click on Select Artifact. A list of all artifacts in the DevOps project of type Docker Image is presented. Select the only entry, the one that was created earlier for the Function Container Image. Note that the button Select Artifact is no longer enabled: only a single container image can be associated with this stage. Click on Add. The pipeline stage is created in the pipeline. And the pipeline is now ready to be executed -- its definition is complete. Or is it? The artifact this pipeline makes use of is not unequivocally defined: the version label in the path for the container image contains the placeholder ${imageVersion}. To ensure the proper version is used for deployment, this placeholder needs to be replaced with the right value. And that is arranged by defining a parameter in the pipeline that is called imageVersion and is set to an existing version label. Click on the Parameters tab for the pipeline. Define a new parameter called imageVersion. Its default value can be anything, but it might as well correspond to an existing version label for the greeter function container image. Save the parameter definition. It would seem that the pipeline is ready to be executed, but we still have to make sure that it's allowed to do its job. Before you try anything rash, read the next section. Dynamic Group and Policies In the previous article, a dynamic group was defined for all deployment pipelines in the compartment. The new pipeline is automatically a member of that group. We also defined a policy that granted permissions to the dynamic group to read all artifacts, which includes (Function) Container Images in the compartment's Container Image Registry repositories. Another policy that was also already created grants the dynamic group the very broad permission to manage all resources in the compartment. We can benefit from the broad scope of that policy, as it also covers creation and update of functions. Run the Deployment Pipeline Run the Deployment Pipeline by pressing Run pipeline. Once the deployment is complete, you will see the green markers that proclaim success. However, there is no other obvious indication of this success because the end result is exactly the situation we had achieved with manual deployment of the function from the Fn CLI command line. To make things a little bit more interesting, we will make a change to the function's code. Then, build the container image for the function (locally) and push the new function image to the container image registry. Then we'll start the deployment pipeline once again; this time, when successful, it will render a new situation which we can experience by invoking the my-api/greeting route on the API Gateway. Change Function Implementation Make a small but visible change to func.go in your local environment: ensure that the response from the new version of the function looks noticeably different from the current version. Save the change. In the next sections, we will build a new version of the function container image from the changed source and eventually make it run on OCI Functions. Build a new Function Container Image (locally) These next commands will first modify the version label used to tag the function with an increase in the third digit (bm is short for bump). Next, the function container image is built using the changed sources. The third command lists the local container images, filtering on images with greeter in their name. Now please execute the commands. console fn bm fn build docker images | grep greeter You should be able to find the newly built image with its fully qualified name, including the OCI region key, the namespace, the repository prefix, and the function name greeter, with the version label appended. Tag Container Image with a New Version Label and Push to Registry Let's define a new identifier for the image, using this command that sets the version label to 0.1.0: console docker tag : :0.1.0 Then push the new function container image to the OCI Container Image Registry repository, using: console docker push :0.1.0 Note that at this point we have not redeployed the function based on this new version of the container image. All we did is build the image and push it to the registry on OCI. Invoking the OCI Function will not show any difference. Run the Deployment Pipeline (for the new Function Image) Run the deployment pipeline one more time. Set the value of parameter imageVersion to 0.1.0. When the pipeline is successfully completed, the new version of the function with all the exciting changes you applied to it is live. Invoke the Newly Deployed Function You can see the new function version in action by invoking it on the command line using Fn CLI: console fn invoke go-on-oci-app greeter (Because the Fn CLI's context is still go-on-oci from the Oracle provider and configured for the go-on-oci compartment that contains the greeter function, this call will be directed to the OCI Function, which at this point is based on new version of the container image.) Alternatively you can curl to the route on the API Gateway that invokes the function: console curl -X \"GET\" -H \"Content-Type: application/json\" -d '{\"name\":\"Mickey Mouse\"}' https:///my-api/greeting Automated Build of Functions Until now, we've built the function container image by hand in the local development environment using the Fn CLI. However, just as we did in the previous article for the Go application that was produced as an executable by a Build Pipeline, we will now turn the building of the function into an automated process. An OCI DevOps Build Pipeline is created to take sources from the Code Repository, run a managed build stage that produces a local container image, and then publish this image as an artifact to the Container Image Registry repository. As a last step, the Build Pipeline triggers the Deployment Pipeline to publish the latest definition of the function to the runtime OCI Functions environment. When all the elements are in place, the total interconnected set of OCI components looks as visualized in the next figure. The artifact and the deployment pipeline shown in this figure are already defined in the DevOps project, as are the Application, Function, and Container Image Registry repository for the images for the function. We will use the Code Repository set up in the previous article. All we need to create is the Build Pipeline build-greeter-function with its three stages. Create the Build Pipeline On the overview page for DevOps Project go-on-oci, click on button Create build pipeline. A page is presented for specifying the name -- e.g. build-greeter-function -- and a description. Press Create to have the build pipeline added to the DevOps Project. Click on the link build-greeter-function in the list to navigate to the details page. First Stage -- Managed Build The first stage in any build pipeline is a Managed Build stage. This stage provides instructions for the pipeline to get hold of a build server, copy specified sources from code repositories to the server, and run through a number of actions on that server. At the time of this writing, we can use a single image for the build server. It is an Oracle Linux image (8 GB memory, 1 OCPU) that has a number of pre installed tools and language run times. For building the function container image, it is relevant that the build server is equipped with both Docker and Fn CLI. Click on either the plus icon or the Add Stage card. The two-step Add a stage wizard appears. On step one in the wizard, make sure that the Managed Build card is selected for the type of stage. Press Next. The second page is shown. Define a name for the build stage: build-go-source-to-function-container-image. Optionally add a description. At present, we cannot select a different build image, so we settle for the one available, which is fine for our purposes. Set the Build spec file path to /functions/greeter/go-function-build-spec.yaml. This file contains the instructions for building the Go sources in the greeter function (or any other Go function) and finally building the function container image. Click on the Select button under Primary code repository. We can now specify from which code repository the build will get its sources. Select OCI Code Repository as the Source Connection Type. Then select the go-on-oci-repo repository. We will work with sources on the main branch, so do not change that default. Type go-on-oci-sources as the value for Build source name. A managed build stage can use sources from multiple repositories. In the build specification, we can refer to each of these sources' root locations using the label defined as Build source name. Click on Save. Press button Add. This completes the definition of the managed build stage. This is all that's needed to take sources and process them into artifacts. The detailed instructions executed by this managed build stage and on the build server are defined in the go-function-build-spec.yaml file. It is this file that contains the instructions for the actual detailed steps executed on the build server. yaml version: 0.1 component: build timeoutInSeconds: 6000 runAs: root shell: bash env: # these are local variables to the build config variables: SOURCE_DIRECTORY: \"go-on-oci-sources/functions/greeter\" FUNCTION_NAME: \"greeter\" # # the value of a vaultVariable is the secret-id (in OCI ID format) stored in the OCI Vault service # you can then access the value of that secret in your build_spec.yaml commands vaultVariables: # exportedVariables are made available to use in sucessor stages in this Build Pipeline # For this Build to run, the Build Pipeline needs to have a BUILDRUN_HASH parameter set exportedVariables: - BUILDRUN_HASH steps: - type: Command name: \"Export variables\" timeoutInSeconds: 40 command: | export BUILDRUNHASH=echo ${OCIBUILDRUNID} | rev | cut -c 1-7 echo \"BUILDRUNHASH: \" $BUILDRUNHASH echo \"fully qual sources\" ${OCIWORKSPACEDIR}/${SOURCE_DIRECTORY} echo \"container image version from build pipeline parameter\" ${imageVersion} go version - type: Command timeoutInSeconds: 600 name: \"Install golangci-lint\" command: | curl -sSfL https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh | sh -s -- -b $(go env GOPATH)/bin v1.37.1 - type: Command timeoutInSeconds: 600 name: \"Verify golangci-lint version\" command: | /root/go/bin/golangci-lint version - type: Command timeoutInSeconds: 600 name: \"Run go mod tidy for Go Application\" command: | cd ${OCIWORKSPACEDIR}/${SOURCE_DIRECTORY} go mod tidy - type: Command timeoutInSeconds: 600 name: \"Run go vet for Go Application\" command: | cd ${OCIWORKSPACEDIR}/${SOURCE_DIRECTORY} go vet . - type: Command timeoutInSeconds: 600 name: \"Run gofmt for Go Application\" command: | gofmt -w ${OCIWORKSPACEDIR}/${SOURCE_DIRECTORY} - type: Command timeoutInSeconds: 600 name: \"Run Lint for Go Application\" command: | cd ${OCIWORKSPACEDIR}/${SOURCE_DIRECTORY} /root/go/bin/golangci-lint run . - type: Command timeoutInSeconds: 600 name: \"Run Unit Tests for Go Application (with verbose output)\" command: | cd ${OCIWORKSPACEDIR}/${SOURCE_DIRECTORY} go test -v - type: Command timeoutInSeconds: 600 name: \"Build Go Function into Function Container Image\" command: | cd ${OCIWORKSPACEDIR}/${SOURCE_DIRECTORY} pwd fn build --verbose image=$(docker images | grep $FUNCTION_NAME | awk -F ' ' '{print $3}') ; docker tag $image go-function-container-image outputArtifacts: - name: go-function-container-image type: DOCKER_IMAGE # this location tag doesn't effect the tag used to deliver the container image # to the Container Registry location: go-function-container-image:latest The build specification consists of three parts: 1. Set up who to run the script as, which shell to use, and which variables to use 2. Build steps: Shell commands to execute on the build server 3. Output Artifacts indicate which files at the end of all build steps are meaningful and to be made available to other steps in the pipeline (for example to publish as artifact) The build steps can be summarized as: 1. Print environment variables and currently installed Go version (on the vanilla build server) 2. Install golangci-lint 3. Verify success and version of golangci-lint installation 4. Run go mod tidy to organize the go.mod file with dependencies 5. Run go vet to run a first inspection on the Go Sources 6. Run go fmt to format the sources according to generic formatting rules 7. Run golangci-lint to lint (check) the sources against various linting rules 8. Run unit tests 9. Build function sources into a Function Container Image using the Fn CLI (store in local image registry) 10. Tag the Function Container Image with the name go-function-container-image. This is the name used in the next stage to find the image to publish These steps are largely equivalent to the managed build defined in the previous article for the Go application that was finally turned into a binary executable deployed on a VM. Steps 9 and 10 are different -- these turn the Go application into a Function Container Image that is the final product of the build. Second Stage -- Publish Artifact In the overview page for the build pipeline, click on the plus icon at the bottom of the current managed build stage. In the context menu that pops up, click on Add stage. The stage wizard appears. Click on Deliver artifacts. Then click Next. Enter the name for this stage: publish-greeter-function-container-image. We need to select the artifact in the DevOps project that we want to publish. This artifact is the container image go-on-oci/greeter:${imageVersion}. Click on Select artifact(s) and select the container image. In the area Associate artifacts with build result, we have to indicate for each of the artifacts selected which of the outcomes of a managed build stage is the source for publishing the artifact. The build specification defines an output labeled go-function-container-image. This output refers to the container image that is produced on the build server by the function build process. Enter the label go-function-container-image in the field Build config/result artifact name. The press the Add button to create the Deliver Artifacts stage. Third Stage -- Trigger Deployment Pipeline In the overview page for the build pipeline, click on the plus icon at the bottom of the Deliver artifacts stage. In the context menu that pops up, click on Add stage. The stage wizard appears. Click on Trigger deployment. Then click on Next. Type a name for the stage: trigger-deployment-of-greeter-function-to-go-on-oci-app, and optionally a description. Click on the button Select deployment pipeline. Select the pipeline deploy-greeter-function-to-go-on-oci-app. Details of the pipeline are shown, including parameters (imageVersion) and the artifact used by the deployment. Click on Add to complete the stage definition and add it to the build pipeline. This completes the build pipeline: it grabs sources, processes them into a deployable artifact, publishes the artifact to the image repository and triggers the deployment pipeline to take it from there. Run Build Pipeline and Trigger Deployment Pipeline Click on Start manual run. Define a value for parameter imageVersion, e.g. 0.2.0. Click on the button to launch the build pipeline. It will now take a few minutes to complete the build pipeline and trigger the subsequent deployment of the newly built function image. When all is done and success is reported, you can invoke the route on the API Gateway that leads to the greeter function to check if the response is indeed the new one expected. console curl -X \"GET\" -H \"Content-Type: application/json\" -d '{\"name\":\"Mickey Mouse\"}' https:///my-api/greeting {\"message\":\"Extremely hot greetings from your automatically built and deployed function dear Mickey Mouse\"} This is a moment for a little celebration. We have achieved an automated end-to-end process that takes Go application sources from the Code Repository and delivers -- after linting, vetting, testing and building -- a live running new version of a serverless OCI Function. To produce further updates to the function, all we need to do is commit the change and trigger the build pipeline. And as a next step, we can even trigger the build pipeline automatically when a (specific) commit happens in the Code Repository. In the second part of this article, we will discuss using OCI services from Go applications in general, and from Go functions in particular. The use of the Go SDK for OCI is introduced and interactions with the OCI Object Storage Service is demonstrated. Go SDK for OCI -- Interaction with OCI Object Storage from Go applications Oracle Cloud Infrastructure is the platform that can build and run applications developed in Go. Either in compute instances or as serverless functions, and also containerized on a managed Kubernetes cluster (as we will discuss in part five of this series). It is good to realize that OCI is much more for Go development teams than a runtime platform. OCI offers many services that can be leveraged from applications. Services for storing files, relational or \"NoSQL\" data, for handling publication and consumption of messages. Services for transferring and analyzing data. And services that support operations on applications, such as monitoring. Interacting with OCI services can be done through the REST APIs available for every aspect of all services. Calling REST services with JSON payloads over HTTP is easy enough from a Go application. There is one complicating factor: these API calls need to be signed. The Oracle Cloud Infrastructure signature uses the \"Signature\" Authentication scheme (with an Authorization header), and the signing process is not exactly trivial. For details on this signing process, see OCI Documentation -- OCI REST API Request Signatures . Fortunately for development of Go applications that call out to OCI services, we can make use of the Go SDK for OCI. This open source development kit facilitates signing the OCI API requests. Using the SDK, the calls to OCI services are made as local calls using strongly-typed predefined structs instead of dealing with JSON request and response bodies. The Go SDK for OCI uses the same config file that we used earlier for the Fn CLI and the OCI CLI. The default location for this file is $HOME/.oci. This file directs the Go SDK to a specific tenancy and region for a user account using the private key half of the pair set up for the user. Go applications that use the Go SDK for OCI can simply build on this configuration without having to deal with any of the details. The documentation for the Go SDK for OCI can be found in the OCI Documentation -- SDK for Go. In this section, we will develop a simple Go application that uses the OCI Object Storage Service for creating and reading files. At first, this is a standalone application that can be compiled and run everywhere (as long as the OCI config file us available). Then we discuss running the Go application inside OCI -- on a VM or as Function. This special focus is relevant because when the Go SDK is used inside OCI, it can leverage the identity and privileges of the component running the application. That means that code running on OCI does not need to bring their own OCI config file, and thus is even simpler. Any Go Application talking to OCI Object Storage Service First, let us create a very simple Go application that can connect to OCI through the SDK. Next, we'll use this foundation to add the interaction with the Object Storage Service. The Simplest Go Client of OCI The simplest Go application that talks to OCI using the Go SDK for OCI is created as follows: 1. Create a directory for the application 2. Create go.mod file with dependencies on Go SDK for OCI package 3. Run go mod tidy to create go.sum file and download required packages 4. Create file oci-client.go with the code for talking to OCI The assumption is that the OCI configuration file that was discussed earlier in this article is located in the default location with the default name: $HOME/.oci/config. If the file is in a different location, you can make use of the function ConfigurationProviderFromFile in the package github.com/oracle/oci-go-sdk/v65/common, which accepts the custom location of the config file. The go.mod file contains this contents: go module oci-client go 1.16 require github.com/oracle/oci-go-sdk/v65 v65.2.0 The github.com/oracle/oci-go-sdk/v65 bit references the most recent (at the time of writing) version of the Go SDK. The sources are downloaded based on this reference. In the Go application, this means that we can leverage the packages in the SDK to access various services in the OCI portfolio. File oci-client.go contains this code that uses the common and identity packages: go package main import ( \"context\" \"fmt\" \"github.com/oracle/oci-go-sdk/v65/common\" \"github.com/oracle/oci-go-sdk/v65/identity\" ) func main() { c, _ := identity.NewIdentityClientWithConfigurationProvider(common.DefaultConfigProvider()) tenancyID, _ := common.DefaultConfigProvider().TenancyOCID() request := identity.GetCompartmentRequest{ CompartmentId: &tenancyID, } response, _ := c.GetCompartment(context.Background(), request) fmt.Printf(\"Name of Root Compartment: %v\", *response.Compartment.Name) } The first line in the function acquires the client that can subsequently be used for most interactions with OCI, such as the GetCompartment call that returns the details for the root compartment. The application can be executed using go run oci-client.go and will produce very simple output: Name of Root Compartment: Even though the result is not especially remarkable, the fact that there is output is proof that the Go SDK for OCI is working and is ready to be leveraged for more lofty activities. Note that the code completely ignores the errors that can occur. If you run into errors, replace the underscores with a variable to capture and handle that error. Go Application Creating and Retrieving Objects in and from OCI Object Storage Service The OCI Object Storage Service offers cheap and persistent storage for different types of data. Objects, large and small, can programmatically be stored on this service to be retained for short or long periods of time, and to be accessed programmatically or through direct URLs. Object Storage provides versioning, different retention rules, encryption of any stored data, object lifecycle management, automated time-based removal, and different storage tiers. Objects on Object Storage Service are organized in buckets. A bucket is a logical container of objects that lives in a specific compartment. Some operations can be performed on all objects in a bucket. The Go SDK for OCI offers functions and types that make it easy and straightforward to work with the Object Storage Service from Go applications. Operations to manipulate buckets and create, delete, and retrieve objects are readily available. For details on the Object Storage package in the Go SDK for OCI, check out this reference: Documentation for Object Storage Package in Go SDK for OCI. The source repository for this article contains the folder applications/store-n-retrieve, which has a simple Go application that connects to your OCI tenancy and creates a bucket, followed by the creation of an object and retrieval of that same object. The application uses the same DefaultConfigProvider that was used the previous section to sign requests to OCI APIs using the $HOME/.oci/config file. The dependency of this application on the Go SDK for OCI is defined in go.mod. The first part of the code creates an ObjectStorageClient instance. Many functions are defined on the underlying interface -- all supporting some form of interaction with the Object Storage Service. The ObjectStorageClient is created using common.DefaultConfigProvider() (just as before), using the default OCI configuration file with a reference to a file containing the private key. Function getNamespace is called to get the namespace for the current tenancy. Then ensureBucketExists is called with the name of bucket to create the bucket if it does not already exist. go package main import ( \"bytes\" \"context\" \"fmt\" \"io\" \"io/ioutil\" \"github.com/oracle/oci-go-sdk/v65/common\" \"github.com/oracle/oci-go-sdk/v65/objectstorage\" ) const ( bucketName = \"go-bucket\" // feel free to use a different name for the bucket compartmentOCID = \" Now the function is deployed and has its configuration. You may expect that a call to the function with this command will be successful: console fn invoke go-on-oci-app object-broker However, there is one final aspect to handle: the function uses OCI Object Storage Service APIs to manipulate buckets and objects, but it needs to have been explicitly granted permissions to do that! We use a dynamic group and two policies to achieve this. Permissions for Functions to manipulate Objects and Buckets Just as we used dynamic groups to create a grantee representing the deployment pipelines and the build pipelines, we need to also create a dynamic group that contains the functions to which we want to grant permissions. To create the dynamic group, type dyn in the search bar. Click on the link Dynamic Groups in the search results pane. On the overview page for dynamic groups and click on Create Dynamic Group. Enter the name for the Dynamic Group for the Deployment Pipeline(s), e.g. functions-in-go-on-oci, and optionally type a description. Define the following rule that selects all functions that are part of the compartment: All {resource.type = 'fnfunc', resource.compartment.id = ''} Of course, replace with the identifier of the compartment you are working in. Then press Create. To create a policy in the console: type poli in the search bar and click on Policies > Identity in the Services area in the search results popup. This takes you to the Policies overview page for the current compartment. The first policy statement defines the permission for the function to manage objects in the compartment. The second statement adds the permission for managing buckets. Define a name, a description, and the following statements: allow dynamic-group functions-in-go-on-oci to manage objects in compartment go-on-oci allow dynamic-group functions-in-go-on-oci to manage buckets in compartment go-on-oci The next figure depicts the permissions that now apply to the function when the policy containing these statements is saved: Now the function can be invoked and should be able to do its thing using the default names for bucket and object. console fn invoke go-on-oci-app object-broker Verify that the bucket is created and and contains the freshly created object using the console from the OCI URL to the buckets page. Add Route in API Gateway to Trigger Function To be able to invoke the object-broker function over HTTP from anywhere, we will again make use of the API Gateway. Type gat into the search bar in the console. Click on Gateways > API Management. Click on the link for the-api-gateway. Click on Deployments. In the list with deployments -- which contains a single deployment -- click on the link myserver-api. Click on Edit to open the deployment specification. Click on the link for the second step: Routes. Scroll down and click on + Another Route. Type /object-broker as the path for this new route. Select GET as the method and Oracle Functions as the Type (of backend). Select application go-on-oci-app, and then set Function Name to object-broker. Press Next, then press Save Changes to apply the changes and make the new route real. The end-to-end picture that is now configured from HTTP consumer through API Gateway to Function and finally bucket and object looks like this: Invoke the function from the browser or using curl on the command line using: console curl -X \"GET\" \"http:///my-api/object-broker?objectName=new-exciting-object.txt&bucketName=the-ultimate-collection\" Automated Build and Deployment This function object-broker was deployed manually on the command line using the Fn CLI. That worked fine of course. However, if you were now to start development on this function, going through multiple development cycles, you would probably want to introduce automation in the build-and-deploy process. Just as we have done before, you can easily set up the required elements in OCI DevOps to achieve automated pipelines for the deployment (of the function container image in the container image registry) and the build (starting from the code repository and resulting in a freshly-baked container image for the function). The main element that is specific to the function is the build specification file for the managed build stage in the build pipeline. This file is provided as go-function-build-spec.yaml in the same directory as func.go and func.yaml. After creating a DevOps artifact for the Function Container Image, an environment for the Function, and the two pipelines for build and deployment, the automated DevOps process set up would look like this: Conclusion One focus area in this article was serverless functions, written in Go and running on Oracle Cloud Infrastructure. The automated build and deployment of these functions was discussed, as was the use of API Gateway to provide access to the function to external HTTP consumers. The second main topic was the Go SDK for OCI for interacting with OCI services from Go applications. The article showed how to use Go code to access the Object Storage service to store and retrieve files. The two topics were combined in function object-broker. This OCI Function leverages resource principal authentication and permissions granted through a dynamic group. Through a runtime configuration, the function learns about current environment specific settings. In the next article, interacting with the Oracle Database will be the main topic. Creating a connection from a Go application to a local Oracle Database, as well as to an Autonomous Database running on OCI, and performing SQL operations with these databases from the comfort of your Go application. Further topics include working with an Oracle Wallet for proper management of database credentials, including the wallet in the deployment process, and combining interactions with the OCI Object Storage and the Autonomous Database services in a single application. Resources Source code repository for the sources discussed in this article series OCI Documentation -- Functions Fn Project Tutorials -- Introduction to Fn with Go Fn Project Tutorials -- Configure Context Go FDK Documentation By Lucas Jellema","categories": ["clouddev","cloudapps"],
        "tags": ["open-source","devops","get-started","automation","serverless","docker","back-end","go","iac"],
        "url": "/tutorials/way-to-go-on-oci/way-to-go-on-oci-article3",
        "teaser": ""
      },{
        "title": "Working in Go applications with Oracle Database and Oracle Cloud Autonomous Database",
        "excerpt":" This is the fourth part of a five part series about Go and Oracle Cloud Infrastructure (OCI). This series discusses how Go applications can be created and run on Oracle Cloud Infrastructure in Compute Instances (VMs), containerized on Kubernetes, or as serverless Functions. The articles show how to automate the build and deployment of these Go applications using OCI DevOps. An important topic is how to use OCI services from Go applications -- both those running on OCI as well as Go code running elsewhere. OCI services discussed include Object Storage, Streaming, Key Vault, and Autonomous Database. In order to follow along with these articles, readers should have at least basic knowledge of how to create Go applications. It is assumed that readers have access to their own Go development environment. Some of the examples and screenshots will specifically mention VS Code as development tool. However, other editors and IDEs can be used as well. The Go code presented in these articles demonstrates a number of mechanisms in their simplest form for maximum clarity and with the least dependencies. Readers should not expect meaningful functionality or production-ready code. This series describes how to get Going on OCI. To try out the examples, readers will need to have access to an OCI tenancy with permissions to create the OCI resources discussed in these articles. Most of the resources used are available in the Aways Free Tier (Compute Instance, VCN, Autonomous Database, Object Storage, Logging, Resource Manager) or have a free allotment tier for limited monthly usage (Functions, API Gateway, Streaming, Vault, DevOps). Introduction The first part of these series describes provisioning of a Compute Instance based on the Oracle Linux Cloud Developer image, opening it up for inbound and outbound network activity, and creating and running a Go application that serves HTTP requests and connecting logging produced by the application to OCI Logging. Part two deals with software engineering, automation of build and deployment of the application with the OCI DevOps service. This service is used for storing the Go source code, building the application executable and storing it as deployable artifact, deploying that artifact to a Compute Instance. The article also shows how to expose an HTTP endpoint for the application through an OCI API Gateway. Part three shows how to create serverless functions in Go and deploy them on OCI. The Go SDK for OCI is introduced -- first for local, standalone Go applications, and subsequently for use from functions, leveraging resource principal authentication. This SDK is used to interact with the OCI Object Storage service for creating buckets and writing and reading files. Part four, which you are reading right now, discusses the interaction between your Go application and an Oracle Database. This can be a local or on-premises database, a database running on some cloud vendor's IaaS instances, or an OCI Autonomous Database. Using the standard Go database/sql package with a driver for Oracle Database and feeding the required configuration details to the driver, it turns out that leveraging any Oracle Database from Go is quite straightforward. The Go application myserver discussed in part two is extended to interact with both an Autonomous Database instance on OCI and the OCI Object Storage service. The application uses the Go SDK for OCI to read files (and subsequently remove them) from a bucket on Object Storage and create database records in the Autonomous Database based on their contents. Local Go Application talking to Local Database The Go language has support for SQL interactions with relational databases built into it. The standard package database/sql includes types and functions for connecting to databases, executing transactions, canceling an operation in progress, and more. This same package can be used for working in the same way with some NoSQL databases such as MongoDB and Couchbase. A Go application that interacts with a database through this package does not need to have technical implementation details for a specific database product. These details are typically implemented in a driver for that database. The application imports the required driver for the database to connect to, and tells the standard package database/sql which driver to use and what the connection details are for the database. Most of the interaction with the database is the same, regardless the specific database technology; records are created, updated and deleted through SQL DML statements, and records are retrieved through SQL queries. The overall process is the same across databases, but the exact SQL dialect can vary. This is probably the only real obstacle for easily moving Go applications between different database products. The code discussed in this section is located in directory /applications/go-orcl-db in the code repository that accompanies this article series. Go Application does SQL -- DDL, DML and Query The simplest thing to do with an Oracle Database from a Go application is to query a single row. The code required for this looks something like this: go package main import ( \t\"database/sql\" \t\"fmt\" ) func sqlOperations(db *sql.DB) { \tvar queryResultColumnOne string \trow := db.QueryRow(\"SELECT to_char(systimestamp,'HH24:MI:SS') FROM dual\") \terr := row.Scan(&queryResultColumnOne) \tif err != nil { \t\tpanic(fmt.Errorf(\"error scanning query result from database into target variable: %w\", err)) \t} \tfmt.Println(\"The time in the database \", queryResultColumnOne) } The import statement makes the database/sql package available. Using the handle to sql.DB, a SQL query can easily be executed (QueryRow) and the result can be scanned into local variables. Quite simple and straightforward and database brand independent, except for the specific SQL statement, which in this case uses the Oracle specific systimestamp. For now, let's not dwell on where the db parameter comes from. In a little while we'll talk about database drivers, and that's where all will be revealed. A slightly more interesting function that creates a table, inserts a record, queries the record, creates to more records then queries all rows and finally drops the table is shown here. You will find this code in file oracle-database-client-app.go in the code repository. go package main import ( \t\"database/sql\" \t\"fmt\" ) const createTableStatement = \"CREATE TABLE TEMPTABLE ( NAME VARCHAR2(100), CREATIONTIME TIMESTAMP DEFAULT SYSTIMESTAMP, VALUE NUMBER(5))\" const dropTableStatement = \"DROP TABLE TEMP_TABLE PURGE\" const insertStatement = \"INSERT INTO TEMP_TABLE ( NAME , VALUE) VALUES (:name, :value)\" const queryStatement = \"SELECT name, creationtime, value FROM TEMPTABLE func sqlOperations(db *sql.DB) { \t_, err := db.Exec(createTableStatement) \thandleError(\"create table\", err) \tdefer db.Exec(dropTableStatement) // make sure the table is removed when all is said and done \tstmt, err := db.Prepare(insertStatement) \thandleError(\"prepare insert statement\", err) \tsqlresult, err := stmt.Exec(\"John\", 42) \thandleError(\"execute insert statement\", err) \trowCount, _ := sqlresult.RowsAffected() \tfmt.Println(\"Inserted number of rows = \", rowCount) \tvar queryResultName string \tvar queryResultTimestamp time.Time \tvar queryResultValue int32 \trow := db.QueryRow(queryStatement) \terr = row.Scan(&queryResultName, &queryResultTimestamp, &queryResultValue) \thandleError(\"query single row\", err) \tif err != nil { \t\tpanic(fmt.Errorf(\"error scanning db: %w\", err)) \t} \tfmt.Println(fmt.Sprintf(\"The name: %s, time: %s, value:%d \", queryResultName, queryResultTimestamp, queryResultValue)) \t_, err = stmt.Exec(\"Jane\", 69) \thandleError(\"execute insert statement\", err) \t_, err = stmt.Exec(\"Malcolm\", 13) \thandleError(\"execute insert statement\", err) \t// fetching multiple rows \ttheRows, err := db.Query(queryStatement) \thandleError(\"Query for multiple rows\", err) \tdefer theRows.Close() \tvar ( \t\tname string \t\tvalue int32 \t\tts time.Time \t) \tfor theRows.Next() { \t\terr := theRows.Scan(&name, &ts, &value) \t\thandleError(\"next row in multiple rows\", err) \t\tfmt.Println(fmt.Sprintf(\"The name: %s and value:%d created at time: %s \", name, value, ts)) \t} \terr = theRows.Err() \thandleError(\"next row in multiple rows\", err) } func handleError(msg string, err error) { \tif err != nil { \t\tfmt.Println(msg, err) \t\tos.Exit(1) \t} } This code is quite functional in nature. Apart from the SQL statements, there are no database-specific implementation details. Half of the code seems to be error handling. It should be not too hard to understand how this code manipulates the database, except for the fact that there is no database to work with yet, and (therefore) also no driver to make the connection and handle the communication. Let's remedy this by first running a local database and then adding a driver the database to the Go application. Run local Oracle Database There are many different ways to get a local Oracle Database up and running. The easiest way I have found uses a Docker container image that allows me to run a local database with one very simple and straightforward statement: console docker run -d -p 1521:1521 -e ORACLE_PASSWORD=TheSuperSecret1509! gvenzl/oracle-xe This runs an Oracle Database XE 21c instance (at least, that is what it does at the time of writing, when 21c is the latest available container image) and sets the passwords for SYS and SYSTEM to the indicated value. The database is available on port 1521 on localhost. Gerald Venzl of Oracle maintains a series of (Docker) Container Images that run a slim version of Oracle Database, the XE edition, which is free to use (up to 20GB of data and using a maximum of 2 CPU threads & 2 GB RAM). He describes these images and how to use them in an article titled Introducing gvenzl/oracle-xe: Oracle Database XE Docker images. Follow these steps to verify that the local Oracle Database is up and running: 1. Find the container identifier with docker ps | grep gvenzl. Then open a Bash shell in the container: \t\tconsole \t\tdocker exec -it /bin/bash \t\t 2. Now, connect to the database and run SQL statements: \t\tconsole \t\tsqlplus system/TheSuperSecret1509! as sysdba \t\t 3. Create a user (schema) to work with in the following sections, for example an innocent demo user: \t\tsql \t\tcreate user demo identified by demo default tablespace users temporary tablespace temp \t\t/ \t\tgrant connect, resource to demo \t\t/ \t\t Now, it's time to connect to this database from the Go application. > Note: You may be interested in installing the Oracle VS Code extension that allows making connections to Oracle Databases -- local and remote -- browse their contents and interact with them similar to SQL Developer and other desktop tools. Add a Driver for Oracle Database There is no official Go driver for Oracle Database, at least not one published or endorsed by Oracle. The unofficial list of Go database drivers has four entries for Oracle Database. Three require installation of the Oracle Client libraries, and one does not. Let's first work with that last one, called go-ora, a pure driver that by itself handles all communication with the database. Details about go-ora are available from the go-ora homepage on GitHub. We will subsequently also look at godror, a driver which requires the libraries to be installed and the one that seems most prominent among the Oracle Database drivers for Go. The go-ora driver can be added to the Go application with: console go get github.com/sijms/go-ora/v2 This downloads the package and adds a require entry to the go.mod file. For me that looks like this: go module oracle-database-client go 1.16 require ( \tgithub.com/sijms/go-ora/v2 v2.4.16 // indirect ) In a new file called pure-oracle-database-client.go (although Go does not really care about the name) in the same directory as file oracle-database-client-app.go, the following code handles the interaction with the local Oracle Database through go-ora. The driver package is imported and the call to sql.Open, which references oracle implicitly, selects go-ora as the driver of choice. Parameter dbParams consists of a map of configuration settings (including the username and password), database host and port, and service name to use for making a connection. The connection string is composed using these elements and used in the call to sql.Open. The subsequent call to db.Ping is the first attempt to really establish communications with the database. When this call is successful, we are ready for some real database actions. go package main import ( \t\"database/sql\" \t\"fmt\" \t\"net/url\" \t_ \"github.com/sijms/go-ora/v2\" ) func GetSqlDBWithPureDriver(dbParams map[string]string) *sql.DB { \tconnectionString := \"oracle://\" + dbParams[\"username\"] + \":\" + dbParams[\"password\"] + \"@\" + dbParams[\"server\"] + \":\" + dbParams[\"port\"] + \"/\" + dbParams[\"service\"] \tdb, err := sql.Open(\"oracle\", connectionString) \tif err != nil { \t\tpanic(fmt.Errorf(\"error in sql.Open: %w\", err)) \t} \terr = db.Ping() \tif err != nil { \t\tpanic(fmt.Errorf(\"error pinging db: %w\", err)) \t} \treturn db } Connecting and Running The database is running, and we have a function that works with a pure Oracle Database driver. Now, let's return to oracle-database-client-app.go and tie it together. Add function main in this file. It calls GetSqlDBWithPureDriver to create a sql.DB instance using the database connection details defined in map localDB. Modify these values to align with your database configuration. The function call sets the db variable with *sql.DB, which can be used for further SQL operations. When all database interactions are complete, the connection should be closed to release the resources. To make sure this is done, the defer in function main immediately following the call to GetSqlDBWithPureDriver is added with the call to db.Close(). The call to function sqlOperations which passes db brings us to the function we discussed two sections ago where the database is really interacted with. go var localDB = map[string]string{ \t\"service\": \"XE\", \t\"username\": \"demo\", \t\"server\": \"localhost\", \t\"port\": \"1521\", \t\"password\": \"demo\", } func main() { \tdb := GetSqlDBWithPureDriver(localDB) \tdefer func() { \t\terr := db.Close() \t\tif err != nil { \t\t\tfmt.Println(\"Can't close connection: \", err) \t\t} \t}() \tsqlOperations(db) } Run the application from the command line using go run *.go. The output will look like: console go run *.go Inserted number of rows = 1 The name: John, time: 2022-04-25 05:31:02.489289 +0000 UTC, value:42 The name: John and value:42 created at time: 2022-04-25 05:31:02.489289 +0000 UTC The name: Jane and value:69 created at time: 2022-04-25 05:31:02.506039 +0000 UTC The name: Malcolm and value:13 created at time: 2022-04-25 05:31:02.509098 +0000 UTC Working with the GoDrOr driver A popular alternative to go-ora is the Go package godror (formerly called goracle, but renamed because of trademark issues — Oracle Corporation does not want anyone to use oracle in their names). This package also provides an Oracle Database driver which database/SQL can use when a sql.Open is performed for either oracle or godror. This package, unlike go-ora, requires an Oracle Instant Client library to be installed on the system running the Go application. Installation of Oracle Instant Client Library The GoDrOr driver uses Oracle Database Programming Interface for C (ODPI-C). It's an open source library of C code -- maintained by Oracle Corporation -- that simplifies the use of common Oracle Call Interface (OCI) features for Oracle Database drivers and user applications. When using GoDrOr we do not need to install ODPI-C or be even aware of its existence. However, the environment in which our Go application, including the driver, is running on needs to contain Oracle Client libraries. The simplest Oracle Client is the free Oracle Instant Client (see the Oracle Instant Client overview page). Only the \"Basic\" or \"Basic Light\" package is required. Oracle Client libraries are also available in any Oracle Database installation or full Oracle Client installation. Detailed installation instructions for Linux can be found in the Oracle Database Documentation for Release 21c -- Database Client Installation Guide for Linux -- Installing Oracle Instant Client. ODPI-C dynamically loads available Oracle Client libraries at runtime. The Oracle Client libraries are searched for in the same directory as the ODPI-C library (or application binary). If they are not found, they're searched for in the standard operating system search path, e.g. PATH on Windows or LDLIBRARYPATH on Linux. Finally, on platforms other than Windows, $ORACLE_HOME/lib is also searched. For details on ensuring the ODPI-C can find the Oracle Client libraries, please check out ODIP-C GitHub Home -- ODPI-C Installation. Modifying the Go application to work with GoDrOr The changes we have to make to the application in order to switch from using go-ora to godror are minimal. First, the godror driver is added to the Go application with: console github.com/godror/godror This downloads the package and adds a require entry to the go.mod file. Next, create a new file called godror-based-oracle-database-client.go in the same application directory -- which is very similar to pure-oracle-database-client.go, which contains details for connecting through the go-ora driver. The contents of this new file: go package main import ( \t\"database/sql\" \t\"fmt\" \t_ \"github.com/godror/godror\" ) func GetSqlDBWithGoDrOrDriver(dbParams map[string]string) *sql.DB { \tvar db *sql.DB \tvar err error \tconnectionString := \"oracle://\" + dbParams[\"username\"] + \":\" + dbParams[\"password\"] + \"@\" + dbParams[\"server\"] + \":\" + dbParams[\"port\"] + \"/\" + dbParams[\"service\"] \tdb, err = sql.Open(\"oracle\", connectionString) \terr = db.Ping() \tif err != nil { \t\tpanic(fmt.Errorf(\"error pinging db: %w\", err)) \t} \treturn db } The import for the godror package is different compared to the import of go-ora. The rest of the code is exactly the same as before. > Note: when we use the Oracle Wallet and change to encrypted communications with the Autonomous Database, there will be more differences between the code used with the two drivers. Finally, to make the application stop using go-ora and start using godror, we just need to comment out or remove one line and add another in function main, calling GetSqlDBWithGoDrOrDriver: go func main() { \t//db := GetSqlDBWithPureDriver(localDB) \tdb := GetSqlDBWithGoDrOrDriver(localDB) Run the application again with go run *.go and you will find the same output as before. The fact that the Oracle Instant Client is involved now is not noticeable. The behavior is apparently unchanged, even though there could be non-functional differences such as the performance of certain operations. Database Transaction Management What is not immediately obvious from our previous discussion is that we never actually committed data to the database. All SQL actions took place in a single database session. The two DDL operations that created and dropped the table implicitly committed the transaction, but none of the insert statements were committed. Some databases have an autocommit setting -- some even as their default -- that turns every DML operation into a transaction that is automatically committed. Not so with the Oracle Database. In order to commit the changes made to database records, these changes must be explicitly committed -- or rolled back in case their lasting effects are not desirable after all. In our Go application we can work with database transactions explicitly -- by beginning a transaction (sql.Tx) on a database, executing DML statements on that transaction, and finally either committing or rolling back the transaction. For example: go ctx := context.Background() tx, err := db.BeginTx(ctx, nil) err = tx.ExecContext(ctx, DMLStatement, ... bind parameter values) err = tx.ExecContext(ctx, AnotherDMLStatement, ... bind parameter values) err = tx.Commit() // or tx.Rollback() Go Application talking to OCI Autonomous Database Making a Go application talk to a local (or any traditionally connected) Oracle Database was not so hard. Databases that are configured for encrypted communications from clients that use the Oracle Wallet -- such as Oracle Autonomous Database instances on OCI -- are no more difficult to interact with. We need to extend our code to work with the Oracle Wallet file, and of course we need to run an Autonomous Database instance and acquire the associated Oracle Wallet. Run free Autonomous Database on OCI To run an Autonomous Database Instance is almost simpler than running a local database. An ATP instance can be created in several ways (including through OCI CLI and Terraform), but the most straightforward method for your first time is probably through the OCI browser console. > Note: Tim Hall provides a good description in his article Oracle Cloud : Autonomous Transaction Processing (ATP) -- Create Service, and there are many more to be found. Let's create your always free ATP instance: Type aut in console's search box, navigate to Autonomous Database | Features, click on button Create Autonomous Database. In the creation form, provide a display name (maybe go-on-oci-db) and database name, select Transaction Processing as the workload type, toggle the Always Free toggle to active, provide a password for ADMIN (and remember it well), accept Network Access Type Secure access from everywhere and make sure checkbox Require mutual TLS (mTLS) authentication is checked. After pressing the button Create Autonomous Database to create the database, the provisioning status is presented: It takes less than a minute for the database to be available. Download Database Wallet with Connect Details We need the database wallet that contains the SSL certificates required for the mTLS interaction. Download the wallet for the ATP instance. First click the DB Connection button in the ATP page in the OCI Console, then click on Download wallet. Provide a password for the wallet; this may be needed for reading the wallet later on. Hang on to this password as well, though I have not needed this password for the steps described in this article. Save the zip file, we'll use it soon. Create Demo user account in Autonomous Database You may want to create a demo user account in the autonomous database. You can do this with these steps: 1. On the ATP details page, click on button Database Actions. Connect as user admin and use the password that you used when configuring ATP. 2. In the Database Actions Launchpad, click on the tile SQL. The SQL Worksheet is opened. \t 3. Paste the statements below into the worksheet and the icon for running the script (or use the F5 button on your keyboard). These statements create a user (schema) to work with in the following sections, just as we did in the local database: \tsql \tcreate user demo identified by thePassword1 default tablespace users temporary tablespace temp \t/ \tgrant connect, resource to demo \t/ \tALTER USER DEMO QUOTA UNLIMITED ON DATA \t/ \t Modify Go application with ATP and Oracle Wallet connection details When the Oracle Database that I want to interact with needs to be connected to with the use of an Oracle Wallet, then I need to pass the file system location of the Oracle Wallet to the driver. More precisely, I need to specify the path to the directory that contains the file cwallet.sso that is part of the wallet. The wallet is typically provided in a zip-archive. This archive should be extracted (or at least this file should) and the path to the directory that contains the file is what will be called the walletLocation. At this point, extract cwallet.sso from the wallet zip file and move this file to a location that is accessible from the Go application -- it might even be in the same directory as the Go application itself. This is not the best practice for production grade applications, but for the purposes of this article it will suffice. The connection details for the autonomous database that need to be provided consist of the same set of elements used earlier for the local database. The database service name can be found in the tnsnames.ora file in the wallet zip file or on the ATP DB Connection page in the OCI Console as service_name. The value for the server property is available as the host in these locations. When the properties are gathered, the following map definition can be added in file oracle-database-client-app.go, right under localDB: go var autonomousDB = map[string]string{ \t\"service\": \"k8j2fvxbaujdcfygoonocidbmedium.adb.oraclecloud.com\", \t\"username\": \"demo\", \t\"server\": \"adb.us-ashburn-1.oraclecloud.com\", \t\"port\": \"1522\", \t\"password\": \"thePassword1\", \t\"walletLocation\": \".\", // when the *.sso file has been moved into the application directory; otherwise provide the absolute directory path } Go interaction with Autonomous Database using Driver go-ora The go-ora driver configuration needs to be extended a little to include the wallet location in the connection string and configure the secure communication protocol. go func GetSqlDBWithPureDriver(dbParams map[string]string) *sql.DB { \tconnectionString := \"oracle://\" + dbParams[\"username\"] + \":\" + dbParams[\"password\"] + \"@\" + dbParams[\"server\"] + \":\" + dbParams[\"port\"] + \"/\" + dbParams[\"service\"] \tif val, ok := dbParams[\"walletLocation\"]; ok && val != \"\" { \t\tconnectionString += \"?TRACE FILE=trace.log&SSL=enable&SSL Verify=false&WALLET=\" + url.QueryEscape(dbParams[\"walletLocation\"]) \t} \tdb, err := sql.Open(\"oracle\", connectionString) ... To run the application against the Autonomous Database and do its TEMP_TABLE acrobatics in the cloud, we need to change the main function slightly: go func main() { \tdb := GetSqlDBWithPureDriver(autonomousDB) \t//db := GetSqlDBWithGoDrOrDriver(localDB) ... That is: replace the localDB reference in the call to GetSqlDBWithPureDriver with autonomousDB. Now run the application again with go run *.go. The results will be exactly the same as before against the local database, but they will probably take a bit longer to be produced as now latency is introduced in each of the database interactions. Go interaction with Autonomous Database using Driver godror The godror driver uses a slightly different setup for working with an Oracle Wallet compared to go-ora. The function GetSqlDBWithGoDrOrDriver in file godror-based-oracle-database-client.go is extended to handle this case: go func GetSqlDBWithGoDrOrDriver(dbParams map[string]string) *sql.DB { \tvar db *sql.DB \tvar err error \tif val, ok := dbParams[\"walletLocation\"]; ok && val != \"\" { \t\tdb, err = sql.Open(\"godror\", fmt.Sprintf(user=\"%s\" password=\"%s\" \t\tconnectString=\"tcps://%s:%s/%s?wallet_location=%s\" \t\t , dbParams[\"username\"], dbParams[\"password\"], dbParams[\"server\"], dbParams[\"port\"], dbParams[\"service\"], dbParams[\"walletLocation\"])) \t} \tif val, ok := dbParams[\"walletLocation\"]; !ok || val == \"\" { \t\tconnectionString := \"oracle://\" + dbParams[\"username\"] + \":\" + dbParams[\"password\"] + \"@\" + dbParams[\"server\"] + \":\" + dbParams[\"port\"] + \"/\" + dbParams[\"service\"] \t\tdb, err = sql.Open(\"oracle\", connectionString) \t} \terr = db.Ping() ... To run the application with the godror driver against the Autonomous Database and do its TEMP_TABLE acrobatics in the cloud, we need to change the main function slightly: go func main() { \t//db := GetSqlDBWithPureDriver(autonomousDB) \tdb := GetSqlDBWithGoDrOrDriver(autonomousDB) ... Now run the application again with go run .go. The results will again be exactly the same as with the *go-ora driver, but it seems (at least in my environment) that actions through go-ora are substantially faster than the same actions through godror. Deploy Go Application talking to Autonomous Database to OCI The Code Repository contains an application called data-service, in directory /applications/data-service. This application is an extension of the myserver application that we worked with in articles one and two of this series. The application still handles HTTP requests as it did before, and this time also implements a simple data API. Using PUT, POST and DELETE requests, the application can be used to create, update and remove person records from a table called PEOPLE in an Oracle Database. Using GET requests, the current details for any person can be retrieved. We will first take a brief look at the interesting elements in the application, and then run it locally. The next step is making this application run on OCI in a Compute instance. You will find that there is nothing very special about an application that has Autonomous Database interaction when it comes to deployment on OCI. Or, at least not until the next installment in this series where we will use OCI Key Vault to securely hold the Oracle Wallet details that -- thanks to the instance principal based authorization -- the application can retrieve at runtime. For now, however, the wallet is included in the source code repository and processed in the build and deployment pipelines. That is not a good practice and will be rectified in the next article. Once the application is deployed, we verify if we can access it with direct access to the compute instance. To apply a good best practice regarding (not) publicly exposing services directly, we then extend the API Gateway with one more routes that lead to the data-service, and specifically its database-founded capabilities. The final situation we achieve on OCI at the end of this section looks like this: Inspect data-service and configure for your ATP instance File data-server.go is new in the application. It contains all logic for interacting with the database and handling any HTTP request to the application that comes in on path data; the DATA_PATH. The registration in function main of the DataHandler function integrates the data handling capabilities. go \thttp.HandleFunc(DATA_PATH, DataHandler) Function main is also extended with these initialization steps: go func main() { \tdb := GetSqlDBWithGoDrOrDriver(autonomousDB) \tdefer func() { \t\terr := db.Close() \t\tif err != nil { \t\t\tfmt.Println(\"Can't close connection: \", err) \t\t} \t}() \tInitializeDataServer(db) \t... At the start of the my-server application, a database connection is created and data server is set up. The application uses the godror driver. Note that we make use of the fact that the Compute instance that is the deployment target was created (back in part one of the series) on the Oracle Linux Cloud Developer image and has the Oracle Instant Client preinstalled. All the application needs added in order to run is: 1. Copy your cwallet.sso file to the root directory of the application 2. Define your Autonomous Database connection details in data-server.go You can then locally run the application, using console go run *.go The application starts and reports for duty. In a separate terminal window, you use curl statements to interact with the Person API. These HTTP requests will cause two records to be created -- for Mickey Mouse and Bugs Bunny. Mickey's record is updated once. Both records are retrieved once. Then both are deleted. The final GET request returns no data. Feel free to add curl requests or not execute all. You can check, in the SQL Worksheet for example, if the Person API has created the database records that are expected. console curl -X \"PUT\" -H \"Content-Type: application/json\" -d '{\"name\":\"Mickey Mouse\", \"age\":93, \"comment\": \"Cartoon Character\"}' localhost:8080/data curl -X \"PUT\" -H \"Content-Type: application/json\" -d '{\"name\":\"Bugs Bunny\", \"age\":84, \"comment\": \"an animated cartoon character created in the late 1930s by Leon Schlesinger Productions (later Warner Bros. Cartoons) and voiced originally by Mel Blanc.\"}' localhost:8080/data curl -X \"POST\" -H \"Content-Type: application/json\" -d '{\"name\":\"Mickey Mouse\", \"age\":93, \"comment\": \"Cartoon Character and Movie Star, created in 1928 by Walt Disney and first appearing in Steamboat Willie; he is the mascot of The Walt Disney Company. His partner is Minnie and he has a pet dog called Pluto \"}' localhost:8080/data curl -X \"GET\" localhost:8080/data?name=Mickey+Mouse curl -X \"GET\" localhost:8080/data?name=Bugs+Bunny curl -X \"DELETE\" -H \"Content-Type: application/json\" -d '{\"name\":\"Bugs Bunny\"}' localhost:8080/data curl -X \"DELETE\" -H \"Content-Type: application/json\" -d '{\"name\":\"Mickey Mouse\"}' localhost:8080/data curl -X \"GET\" localhost:8080/data?name=Mickey+Mouse Commit, Push and Build leading to Deploy and Run This variant of the myserver application is ready to use and the code is already in the OCI DevOps Code Repository, as all code from the article source repository on GitHub was committed to the OCI Code Repository in the second article in this series. However, you have added file cwallet.sso (the Oracle Wallet for your Autonomous Database instance) and you have updated file data-server.go to provide database connection details. Before the build pipeline can be used on OCI to build and subsequently deploy the application, you first need to add the new file, commit both the changed and this added file, and push the changes to the OCI DevOps Code Repository. After these git add, commit and push actions, the Code Repository go-on-oci-repo should contain your cwallet.sso and the data-service.go that you modified. You can now reuse the build pipeline build-myserver that was setup in article two when we first discussed OCI DevOps Build Pipelines. However, the current pipeline expects the build specification file in the default location, and that will not do for the amended myserver application. 1. Open the details page of the Build Pipeline build-myserver in the OCI Console. Open the details for the managed build stage. Click on Edit. \t 2. Change the value in the field Build spec file path to /applications/data-service/build_spec.yaml, the build specification that is modified to build the extended version of myserver. Click on Save. \t 3. Start a build run. Set a new version for the parameter MYSERVER_VERSION if you want to. The pipeline will produce a new artifact -- a zip file with the executable built from the Go sources in directory /applications/data-service and containing the wallet file and website subdirectory. The pipeline will trigger the deployment pipeline that will bring the artifact to the Compute instance, copy the application to the /tmp/yourserver directory, and run the application. It starts listening for HTTP requests on the port specified by the deployment pipeline parameter HTTPSERVERPORT (or on 8080 if the parameter is not set). You can access the Person API on the public IP address for the VM, if that is still exposed: console curl -X \"GET\" :8095/data?name=Mickey+Mouse You can create a route on the API Gateway to provide proper public access to the Person API. Make sure that you add all methods that the API handles to the route definition. When the deployment is updated, the Person API is available at https:///my-api/person?name=Mickey+Mouse. Curl and other HTTP tools like Postman can be used to interact with the API, using all methods to create, update, retrieve and delete person records. Go Application on OCI interacting with Autonomous Database and Object Storage service The final step in this article combines the interaction with the OCI Object Storage service (introduced in the previous article) with operations on an Autonomous Database instance in a single Go application that is first ran locally and then deployed to and executed on a compute instance in OCI and exposed through API Gateway. The functionality provided: send an HTTP GET request with the names of an object and a bucket on Object Storage; the object should be a JSON file that contains data on people in the following format: json [ { \"name\": \"Jasper\", \"age\": 19, \"comment\": \"Haute Couture\" }, { \"name\": \"James\", \"age\": 3, \"comment\": \"Golden retriever\" } ] The file will be read and records will be created in table PEOPLE in the Autonomous Database for each of the JSON entries. All you need to add to the application in order to run: 1. Copy your cwallet.sso file to the root directory of the application 2. Define your Autonomous Database connection details in data-server.go 3. Edit my-server.go -- set the correct value for compartmentOCID 4. Upload the file website/sample-persons.json to a bucket on the Object Storage service (feel free to edit the file or to upload a different file with similar contents) You can then locally run the application, using console go run *.go The application starts and reports for duty. In a separate terminal window, you use curl statements to interact with the new Persons file processor API. An HTTP request should pass in the name of the bucket and the object that contains the JSON data to process. The service will fetch the file, parse its contents and create or update records the PEOPLE table in the autonomous database. console curl \"localhost:8080/people?objectName=sample-persons.json&bucketName=the-bucket\" Using a call to the data API you can inspect the data records: console curl localhost:8080/data?name=Martha And you can do the same in the SQL Developer Worksheet: You may be interested in the function PeopleJSONProcessor, which handles the record creation (or update) in table PEOPLE. It uses an Oracle-specific Bulk DML approach -- syntax supported by the godror driver -- where arrays of values for each of the bind parameters are passed in and all records are created in a single DML statement. Quite efficient. go func PeopleJSONProcessor(peopleJson []byte) { \tvar persons []Person \tjson.Unmarshal(peopleJson, &persons) \tnameVals := make([]string, len(persons)) \tageVals := make([]int, len(persons)) \tdescriptionVals := make([]string, len(persons)) \tfor i, person := range persons { \t\tageVals[i] = person.Age \t\tnameVals[i] = person.Name \t\tdescriptionVals[i] = person.JuicyDetails \t} \tdatabase.Exec(MERGE INTO PEOPLE t using (select :name name, :age age, :description description from dual) person \t\tON (t.name = person.name ) \t\tWHEN MATCHED THEN UPDATE SET age = person.age, description = person.description \t\tWHEN NOT MATCHED THEN INSERT (t.name, t.age, t.description) values (person.name, person.age, person.description) , \t\tnameVals, ageVals, descriptionVals) } Now let us bring this application to OCI, to the Compute instance we have also used in the previous section. Some steps are needed as preparation: 1. Edit file object-processor.go: change the value of const RUNWITHINSTANCEPRINCIPALAUTHENTICATION from false to true (this flag is false when running locally and true when running on a Compute Instance in OCI where Instance Principal authentication and authorization is used) 2. Git actions: add cwallet.sso and commit this new file along with the changed files my-server.go, object-processor.go and data-service.go; push the commit to OCI Code Repository 3. Make sure an IAM policy exists that allows the dynamic group go-on-oci-instances to read objects in the compartment; this makes it possible for the application running on the VM to call out to Object Storage Service to read the JSON file with person records . The policy statement could read: Allow dynamic-group go-on-oci-instances to read objects in compartment go-on-oci After these git add, commit and push actions, the Code Repository go-on-oci-repo should contain your cwallet.sso and the data-service.go that you modified. You can now reuse the build pipeline build-myserver that we used before. Just as we did earlier, we have to update the reference to the build specification file. 1. Open the details page of the Build Pipeline build-myserver in the OCI Console 2. Open the details for the managed build stage 3. Click on Edit 4. Change the value in the field Build spec file path to /applications/people-file-processor/build_spec.yaml, the build specification that is modified to build the extended version of myserver 5. Click on Save Start a build run. Set a new version for the parameter MYSERVER_VERSION if you want to. The pipeline will produce a new artifact -- a zip file with the executable built from the Go sources in directory /applications/people-file-processor and containing the wallet file and website subdirectory. The pipeline will trigger the deployment pipeline that will bring the artifact to the Compute instance, copy the application to the /tmp/yourserver directory, and run the application. It starts listening for HTTP requests on the port specified by the deployment pipeline parameter HTTPSERVERPORT (or on 8080 if the parameter is not set). You can access the API on the public IP address for the VM -- if that is still exposed. It's better to add a route on the API Gateway to expose access to the personnel file processor: 1. Navigate to the details for the-api-gateway 2. Open the tab Deployments 3. Click on Edit 4. Open the second step for Routes and add a new route. Define the Path as /personnel-file-handler. The GET method should be supported. The type of the route is HTTP. The URL is: http://:8095/people. Check the value of the HTTPSERVERPORT on the deployment pipeline deploy-myserver-on-go-app-vm. If it is not set to 8095, then modify the URL value to use the proper port number for the application. Press Next, then Save changes. It will take a moment for the API Gateway's Deployment to be refreshed. Once it has, the service that reads a file from an Object Storage bucket, processes the JSON content and creates records in table PEOPLE in the Autonomous Database instance for the entries in this file can be triggered with a simple HTTP GET request to https:///my-api/personnel-file-handler?objectName=sample-persons.json&bucketName=the-bucket. The effect of making the call can be inspected through the person API that reads records from that same table in that same database: https:///my-api/person?name=Jasper. The end-to-end picture of what is now deployed on OCI is shown in the next figure. What is not shown in the picture and is important to note: * the Oracle Wallet file deployed with the application (in the artifact built from the source in Code Repository) * hard coded reference to the compartment in the application * the policy that grants permission to the compute instance to read objects In the next article we look at OCI Key Vault, a service that offers a much better way to store the Oracle Wallet and make it available to the application at deployment time (or even runtime). Conclusion This article demonstrated how one can interact with an Oracle Database from Go applications, either a traditional one or the autonomous kind. We have seen how a connection from a Go application to a local Oracle Database -- as well as to an Autonomous Database is made -- using a driver and possibly supporting libraries, and how DDL and DML SQL operations can be performed with these databases from the comfort of the Go application. Using an Oracle Wallet for proper management of (autonomous) database credentials was discussed. An application running on an OCI Compute Instance, interacting through the Go SDK for OCI with the Object Storage Service, and manipulating the Autonomous Database -- automatically deployed through OCI DevOps -- provided the finale of the article. The fifth and last article in this series series adds two more OCI services with which Go applications can interact: OCI Streaming -- a high volume streaming message broker that allows for decoupled interactions between different microservices and other components -- and the OCI Key Vault for managing secrets such as Oracle Wallet with database credentials. This article also introduces a third type of application platform next to VM and serverless function, in the shape of the managed OCI Kubernetes Enginer (OKE), and it shows how DevOps Deployment Pipelines can deploy our Go applications to OKE in an automated way. Resources Source code repository for the sources discussed in this article series Go Dev -- Tutorial: Accessing a relational database General introduction to working with the database/sql package Connecting a Go application to Oracle Database Using an SQL database in Golang Oracle Cloud : Autonomous Transaction Processing (ATP) -- Create Service Go Dev Docs -- Accessing relational databases List of drivers for Go database/sql -- to use a specific database product from Go Go Oracle Database Driver go-ora -- a pure client that does not need additional libraries Go Oracle Database Driver godror -- uses Oracle Client libraries Go DRiver for ORacle User Guide Download Oracle Client libraries (free Basic or Basic Light package) -- needed with godror at run time Installation instructions for GoDrOr package Some instructions on installing Oracle Instant Client Download page for Oracle Instant Client. Oracle Database Documentation for Release 21c -- Database Client Installation Guide for Linux -- Installing Oracle Instant Client How to Connect a Go program to Oracle Autonomous Database on the blog My Experiments with Java by Pallab Rath Using the Go Language for Efficient Oracle Database Applications (slidedeck) -- Anthony Tuininga, Oracle OpenWorld 2019 By Lucas Jellema","categories": ["clouddev","cloudapps"],
        "tags": ["open-source","devops","get-started","automation","iac"],
        "url": "/tutorials/way-to-go-on-oci/way-to-go-on-oci-article4",
        "teaser": ""
      },{
        "title": "Hooking Go applications into OCI Streaming, using OCI Key Vault and Go Deployment on OKE",
        "excerpt":" Welcome! This is the fifth and final part of a five-part series about Go and Oracle Cloud Infrastructure. The series discusses how Go applications can be created and run on Oracle Cloud Infrastructure, either in Compute Instances (VMs) containerized on Kubernetes, or as serverless Functions. From start to finish, we'll walk you through the process of automating the building and deployment of these Go applications using OCI DevOps. A particular focus will be on showing you how to use Oracle Cloud Infrastructure (OCI) services from Go applications, both those running on OCI as well as those running Go code elsewhere. Throughout this series, we'll discuss various OCI services, including Object Storage, Streaming, Key Vault, and Autonomous Database. Just like it says on the tin, the ultimate goal of this series is to make sure that you have everything you need to get Going on OCI! Prerequisites In order to follow along with these articles, you should at least have a basic understanding of how to create Go applications. We'll also assume that you have access to your own Go development environment. And while some of the examples and screenshots specifically mention VS Code as a development tool, don't worry, you're perfectly fine using any other editors and IDEs you feel more comfortable with. Examples To work with the examples presented throughout the series, you'll also need to have access to an OCI tenancy with permissions to create the OCI resources discussed in these articles. Fortunately, most of the resources used are either available to you in the Always Free Tier (Compute Instance, VCN, Autonomous Database, Object Storage, Logging, Resource Manager) or have a free allotment tier for limited monthly usage (Functions, API Gateway, Streaming, Vault, DevOps). Worth mentioning For the sake of clarity, we've done our best to present mechanisms in their simplest form and with the least amount of dependencies. While this allows us to demonstrate some of the diverse ways in which you can use Go on OCI, it also means that you shouldn't expect meaningful functionality or production ready code. Just the right amount to get you going! Introduction The first part of this series begins by describing how you can provision a Compute Instance based on the Oracle Linux Cloud Developer image and then open it up for both inbound and outbound network activity. It goes on to show you how to create and run a Go application that serves HTTP requests, and later how to connect it to OCI Logging. Part two deals with software engineering and demonstrates how you can use the OCI DevOps service to automate the build and deployment of your application. It illustrates how the OCI DevOps services is used for storing the Go source code, building the application executable, and then storing it as an artifact which can deployed to a Compute Instance. The article wraps up by showing you how to expose an HTTP endpoint for the application through an OCI API Gateway. Part three shows you how to create serverless functions in Go and then deploy them on OCI. Next, you're introduced to the Go SDK for OCI, first for local, stand-alone Go applications and subsequently for use from functions. In each case, resource principal authentication is leveraged. The key takeaway here is that the SDK is used to interact with the OCI Object Storage service for both creating buckets and reading/writing files. The fourth article discusses the interaction between your Go application and an Oracle Database. This database can be a local, on-premises one, one running on a cloud vendor's IaaS instance, or an instance of an Oracle Cloud Infrastructure Autonomous Database. This fifth and final article in this series introduces two more OCI services for your Go applications to interact with: OCI Streaming (a high-volume streaming message broker that allows for decoupled interactions between different microservices and other components) and the OCI Key Vault for managing secrets, such as Oracle Wallet with database credentials. This article also introduces a third type of application platform (beyond the VM and serverless function covered in part one) in the form of the managed OCI Kubernetes Engine (OKE). It also shows you how DevOps Deployment Pipelines can deploy your Go applications to OKE in an automated way. Publishing Messages from Go application to OCI Streaming In this scenario, we'll work with the [OCI Event Streaming Service], a managed Pub/Sub message broker service similar to Apache Kafka. In this service, events are published to Streams (aka Topics) and can be consumed from those Streams, either by one or multiple consumers. Consumption can be managed in several different ways to provide content that's most relevant to you: starting at a specific time or after a specific offset, opening up to include as many events as are available, or narrowing to only the newest. >NOTE: It's important to keep in mind that consumers have to actively come to the service to collect messages; there's no push mechanism that will notify listeners whenever new messages have been published to the Stream. However, there's an upside to this approach and it's what helps distinguish the Event Streaming Service from others. Here, there's no need for a dedicated subscription. All a consumer has to do is create a cursor (similar to a session or a [long running] query) and start pulling messages within the context of that cursor. As for the Streams themselves: * They can be used for messaging, ingesting high-volume data such as application logs, operational telemetry, web click-stream data, or other use cases in which data is produced and processed continually and sequentially in a publish-subscribe messaging model. * Messages are retained for up to 7 days. * While there are limits on how much data they can handle per second, in practical terms these limits are actually fairly high (1MB per partition per second, 1MB maximum message size, five consume calls per second). With that in mind, let's take a look at what'll be covering in this next section. We'll first create a Stream (aka Topic), try it out in the OCI console, and then create a local Go application that can publish messages to the Stream (once again using the Go SDK for OCI). Create a Stream Even with everything they can do, creating a Stream in the Console is remarkably easy, as is Producing a Test Message and Consuming the Test Messages. In fact, you can get going in 3 minutes or less! Let's look at how to set this up. 1. In the OCI Console search field, enter str and then select Streaming | Messaging. 1. Select Create Stream. Provide some details on the stream: * its name (e.g., go-on-oci-stream) * the number of partitions (leave at one) * the retention time (24 hours should be fine for our purposes) >NOTE: Leave the radio button selected for Auto-Create a Default Stream Pool. 1. Select Create Stream. At this point, the Stream will be created and should take only a few seconds to complete. Try out the new Stream Once the Stream has been created and is active: 1. Select Produce Test Message. 1. Enter a message and select Produce. The console will indicate that the message was produced successfully. 1. Select the Cancel link to close the popup window. 1. Select Load Messages. All recently published messages (within the last 60 seconds) on the Stream are displayed. You should see the test message that you just published. Go Message Producer Go client The file, producer.go is a Go client for a Stream on OCI. It is located within applications/message-producer in the source code repository of this article series. This file assumes a local $HOME/.oci/config file that provides details for connecting to OCI as a user with permissions on the stream. When the client is first launched, StreamClient is initialized and the function PutMessages is invoked with a request targeted at the streamOCID for the new stream that contains two messages. The code is quite straightforward: go package main import ( \"github.com/oracle/oci-go-sdk/v65/common\" \"github.com/oracle/oci-go-sdk/v65/streaming\" ) const ( streamMessagesEndpoint = \"\" streamOCID = \"\" ) func main() { streamClient, _ := streaming.NewStreamClientWithConfigurationProvider(common.DefaultConfigProvider(), streamMessagesEndpoint) putMsgReq := streaming.PutMessagesRequest{StreamId: common.String(streamOCID), PutMessagesDetails: streaming.PutMessagesDetails{ // two messages are put on the Stream in the single request Messages: []streaming.PutMessagesDetailsEntry{ {Key: []byte(\"key dummy-0\" ), Value: []byte(\"my happy message-\")}, {Key: []byte(\"key dummy-1-\"), Value: []byte(\"hello dolly and others-\")}}}, } streamClient.PutMessages(context.Background(), putMsgReq) } Running the client Before you're able to run producer.go, you'll first need to: 1. Set the appropriate values for streamOCID and streamMessagesEndpoint. 2. In the directory that contains producer.go, run go mod tidy on the command line. With that set, you're all ready to run the client. 1. Execute go run producer.go. 1. Use the OCI Streaming console to inspect the arrival of messages and select Load Messages. In the console, you'll see the messages produced by the Go application. At this point, feel free to change the content/number of messages produced, or run the producer again and check again for the messages in the console. Consume Messages from a Stream in a Go application Really, a Stream isn't much use to us if we're just able to send messages to it. In order for the process to be really useful, messages need to be consumed before they expire. The next application that we'll discuss does exactly this. You can find it in the applications\\message-consumer directory of the source code repository. So, how does it work its magic? First, it creates a client for OCI Streaming using the Go SDK for OCI from Go code. Then it makes a cursor request. A cursor is pointer to a location in a stream and represents a specific consumer. The application uses the cursor to make requests for messages. If more messages are available on the stream beyond the current cursor's offset, then the next batch of messages is delivered (and the offset is moved). When all messages have been retrieved through the cursor, the request will result in a response without any messages. From there, the client can continue polling (with) the cursor for new messages on the stream. Running the application First, run go mod tidy at the command line. Then, run this application using go run consumer.go. go package main import ( \"context\" \"fmt\" \"github.com/oracle/oci-go-sdk/v65/common\" \"github.com/oracle/oci-go-sdk/v65/streaming\" ) const ( streamMessagesEndpoint = \"https://cell-1.streaming.us-ashburn-1.oci.oraclecloud.com\" streamOCID = \"ocid1.stream.oc1.iad.amaaaaaa6sde7caa56brreqvzptc37wytom7pjk7vx3qaflagk2t3syvk67q\" ) func main() { streamClient, _ := streaming.NewStreamClientWithConfigurationProvider(common.DefaultConfigProvider(), streamMessagesEndpoint) partition := \"0\" createCursorRequest := streaming.CreateCursorRequest{ StreamId: common.String(streamOCID), CreateCursorDetails: streaming.CreateCursorDetails{Type: streaming.CreateCursorDetailsTypeTrimHorizon, Partition: &partition, // mandatory: which partition to read from; note: with a GroupCursor, OCI Streaming assigns partitions to consumers (represented by cursors) }} createCursorResponse, _ := streamClient.CreateCursor(context.Background(), createCursorRequest) // using the cursor, go retrieve messages consumeMessagesLoop(streamClient, streamOCID, *createCursorResponse.Value) } func consumeMessagesLoop(streamClient streaming.StreamClient, streamOcid string, cursorValue string) { getMessagesFromCursorRequest := streaming.GetMessagesRequest{Limit: common.Int(5), // optional: how many messages to collect in one request StreamId: common.String(streamOcid), Cursor: common.String(cursorValue)} for i := 0; i NOTE: When AFTEROFFSET or ATOFFSET are defined, the value for offset must be provided. When AT_TIME is set as type, a value for time is mandatory. Example - Where message retrieval should start after offset 5: go partition := \"0\" offset := common.Int64(5) createCursorRequest := streaming.CreateCursorRequest{ StreamId: common.String(streamOCID), CreateCursorDetails: streaming.CreateCursorDetails{Type: streaming.CreateCursorDetailsTypeAfterOffset, Offset: &offset, Partition: &partition, }} Consumer groups Effectively, each consumer is represented by an active cursor. This means that instead of explicitly defining individual, isolated cursors that link up with a specific partition, we can use the concept of consumer groups and leave it to the Streaming Service to associate partitions with specific consumers. In that case, the body of function main becomes: go func main() { streamClient, _ := streaming.NewStreamClientWithConfigurationProvider(common.DefaultConfigProvider(), streamMessagesEndpoint) // Type can be CreateGroupCursorDetailsTypeTrimHorizon, CreateGroupCursorDetailsTypeAtTime, CreateGroupCursorDetailsTypeLatest createGroupCursorRequest := streaming.CreateGroupCursorRequest{ StreamId: common.String(streamOCID), CreateGroupCursorDetails: streaming.CreateGroupCursorDetails{Type: streaming.CreateGroupCursorDetailsTypeTrimHorizon, CommitOnGet: common.Bool(true), // when false, a consumer must manually commit their cursors (to move the offset). GroupName: common.String(\"consumer-group-1\"), InstanceName: common.String(\"go-instance-1\"), // A unique identifier for the instance joining the consumer group. If an instanceName is not provided, a UUID will be generated TimeoutInMs: common.Int(1000), }} createGroupCursorResponse, _ := streamClient.CreateGroupCursor(context.Background(), createGroupCursorRequest) consumeMessagesLoop(streamClient, streamOCID, *createGroupCursorResponse.Value) } As long as there is only a single instance in the group, all messages on all partitions are handed to this consumer's cursor. When multiple instances are added to the group, they will each get one or more partitions assigned to them (if enough partitions are available for the stream) and receive messages from those partitions. Multiple consumers can work in parallel on processing the messages on the stream without any message being processed more than once. Create an OCI Vault and Store Secrets It is common for applications to use configuration settings as part of their operation. These settings can help determine part of the behavior of the application, instructing it on how to deal with endpoints, file system locations, or credentials for making connections. Not unsurprisingly, configuration settings can also be environment dependent. For instance, the same application in a test environment uses different values than it would in the production environment. And as you might imagine, some of the configuration settings are sensitive, such as passwords or private keys. So far, the Go applications discussed in this series haven't worked explicitly with configuration settings. For example purposes, some applications contain hard-coded references to compartment, bucket, and stream, or are committed to the code repository with a database wallet and hard-coded database connection details. This certainly isn't ideal, and in practice you'll want to avoid doing the same for your own applications. Vaults From this point on, we'll improve our way of working and use OCI Vault instead. The Vault service on OCI lets you create vaults in your tenancy as containers for encryption keys and secrets. Vaults are logical entities where the Vault service creates and durably stores keys and secrets. The type of vault you choose determines features and functionality such as degrees of storage isolation, access to management and encryption, and scalability. If needed, a virtual private vault provides you with a dedicated partition in a hardware security module (HSM), offering a level of storage isolation for encryption keys that’s effectively equivalent to a virtual independent HSM. Keys are stored on highly available and durable HSMs that meet Federal Information Processing Standards (FIPS) 140-2 Security Level 3 security certification. The Vault service uses the Advanced Encryption Standard (AES) as its encryption algorithm and uses AES symmetric keys. Note that the virtual private vault is a paid service whereas the default vault is always free. Keys are logical entities that represent one or more key versions that contain the cryptographic material used to encrypt and decrypt data, protecting the data where it's stored. When processed as part of an encryption algorithm, a key specifies how to transform plaintext into ciphertext during encryption and how to transform ciphertext into plaintext during decryption. Plain text in this sense can either be a base64 representation of binary content or a JSON document that contain many configuration settings in a single key. Vaults are first and foremost associated with secrets that contain sensitive information. However, even information that may not be very sensitive can be stored in a key. This means that an application can use keys as configuration settings during deployment or at runtime to help define the environment-specific settings and behavior. And a reference to the Vault Secret is all that the application needs to retrieve the secret that provides the required settings. What this means is that secrets can now be managed independently from an application. You can avoid using hard coding that runs contrary to standard best practices and makes it impossible to use a single code base for all environments. >NOTE: It's important to keep in mind that when the value of secret changes and you want the application to start using the new values, you have to make sure to reinitialize the application using the changed values. Create an OCI Vault In this section, we'll create a Vault and a simple secret that we can read from a Go application. Next, we'll create a secret that contains an Oracle Wallet and then a second secret through a JSON document that contains additional database connection details. Using this secret, we can create an application that works with an Oracle Database and which learns at runtime how to connect to the database when it accesses the secret. All you need to connect to a different database is change the secret and then restart the application. That's it! Let's create the vault. 1. In the OCI console, enter vau in the search field and then select the link Vault | Identity & Security. The overview page with all of vaults in the current compartment is shown, most likely without any entries. 2. Select Create Vault and in the Create Vault form enter the name for the new vault: go-on-oci-vault. The new vault doesn't have to be a Virtual Private Vault, so leave the checkbox unchecked. 3. Select the Create Vault button to create the vault. A list of vaults is returned. This time though, it'll contain your new vault with the status creating. >NOTE: It can take up to one minute or so for the vault to be initialized. 4. When the new vault's status is Active, select the name of your new vault and then navigate to its details page. 5. Create the master encryption key for the vault. We'll need to do this before we can start creating secrets in it. 1. Enter the name for the master key (e.g., go-on-oci-vault-master-key). 2. Accept all default values. 3. Select Create Key. It'll take a little time for this new master key to be produced. Create a Secret We now have a vault and a master key to encrypt any secrets we'll want to store. At this point, we're ready to define secrets in the vault. Let's start with a very simple one. 1. Select the link for the Secrets and select Create Secret. A page is presented where a new secret can be created. 1. Enter the name for the secret (e.g., greeting-of-the-day). 1. Select the master key we set up for the vault in the previous section and leave the selection list Secret Type Template at Plain Text. 1. Enter the value of the secret greeting (e.g., Have a wonderful day!). 1. Select Create Secret to save the new secret to the store. This is the start of what's commonly referred to as a secret bundle. A secret bundle consists of the secret contents, properties of the secret and secret version (such as version number or rotation state), and user-provided contextual metadata for the secret. When you rotate a secret, you provide new secret contents to the Vault service to generate a new secret version. The complete version history of all values that have been assigned to the secret will be retained. Read Secret from Go application The secret that you just defined is now required in our Go application. Let's see how we can use the application to access the values of secrets. Probably the most straightforward example of how to retrieve a secret from an OCI Vault from Go lives in the file secret-reader.go (located in applications/secret-reader of the source code repository). It uses the Go SDK for OCI and the only piece of information it really needs (outside of the $HOME/.oci/config file) is the OCID for the secret to be retrieved. >NOTE: The assumption here is that whichever user's credentials are defined in the config file also has read permissions for the secret. In the code below, make sure to replace secretOCID with the appropriate OCID value. go package main import ( \"context\" b64 \"encoding/base64\" \"fmt\" \"github.com/oracle/oci-go-sdk/v65/common\" \"github.com/oracle/oci-go-sdk/v65/secrets\" ) const ( secretOCID = \"\" ) func main() { secretsClient, _ := secrets.NewSecretsClientWithConfigurationProvider(common.DefaultConfigProvider()) secretReq := secrets.GetSecretBundleRequest{SecretId: common.String(secretOCID)} secretResponse, _ := secretsClient.GetSecretBundle(context.Background(), secretReq) contentDetails := secretResponse.SecretBundleContent.(secrets.Base64SecretBundleContentDetails) decodedSecretContents, _ := b64.StdEncoding.DecodeString(*contentDetails.Content) fmt.Println(\"Secret Contents:\", string(decodedSecretContents)) } Run the application To run the application, execute go run secret-reader.go at the command line. This should echo the content of the secret that you just created to the command line. To see the effect of managing configuration settings separate from the application source code, update the secret in the OCI Console and run the application again. You should now see the changed content since the application will always retrieve the latest version of the secret. So, let's review. Not only does the code not contain the highly sensitive value of the secret, it also remains unchanged until the application has been restarted. In a broader context, refreshing updated values of configuration settings in live applications is an interesting and relevant topic, but one we'll leave for another moment. In the next section, we'll explore how to create secrets that actually contain sensitive information. Store Oracle Database connection details and Wallet in Vault In the previous article, details about the database connection and the Oracle Wallet Information were hard coded into our source code and left completely exposed in our repository. In a real environment, that would be inexcusable. The proper way to handle such sensitive data would be using a vault. When an application runs it knows it has to connect to a database, but unless we tell it, it doesn't know where that database is or which credentials it needs to connect with. That particular information can be retrieved from the vault provided that: * we use the secret identifiers * the application is running as a user or on a host that has permissions for reading the content of these secrets. Create secret Storing the database connection details (e.g., the autonomousDB struct in the file data-service.go in application data-service discussed in the previous article) in a secret isn't difficult at all. 1. Create a JSON-formatted string from the data in the struct: json { \"service\": \"k8j2fvxbaujdcfygoonocidbmedium.adb.oraclecloud.com\", \"username\": \"demo\", \"server\": \"adb.us-ashburn-1.oraclecloud.com\", \"port\": \"1522\", \"password\": \"thePassword1\" } 1. Navigate to the vault page for go-on-oc-vault in the OCI console. 1. In the Secrets tab, select Create Secret and enter the name for the secret: autonomousDB-demo-credentials. 1. Select the master encryption key. 1. In the Secret Contents area, copy and paste the JSON content containing the database connection details from step 1 above. >NOTE: Make sure to keep the type template on Plain-Text. 1. Select Create Secret. Define type In the Go application, define a type to hold the database credentials: go type DatabaseConnectDetails struct { Service string json:service Username string json:username Server string json:server Port string json:port Password string json:password WalletLocation string json:walletLocation } Decode secret Now, create code that decrypts content from the secret and stores it in a variable based on this type: go secretReq := secrets.GetSecretBundleRequest{SecretId: common.String(secretOCID)} secretResponse, _ := secretsClient.GetSecretBundle(context.Background(), secretReq) contentDetails := secretResponse.SecretBundleContent.(secrets.Base64SecretBundleContentDetails) decodedSecretContents, _ := b64.StdEncoding.DecodeString(*contentDetails.Content) var dbCredentials DatabaseConnectDetails json.Unmarshal(decodedSecretContents, &dbCredentials) fmt.Println(\"database connect details for user: \" + dbCredentials.Username) Store the wallet file Next, we need to also store the wallet file cwallet.sso in a secret. Since this file contains binary content, we'll first need to convert it into a string representation before we can store this information as a secret. All we need to do is encode the content in base64. We can easily do this on the Linux command line by running: console base64 -i cwallet.sso > cwallet-sso-base64.txt The resulting file, cwallet-sso-base64.txt contains the content we want to use for the secret. Now that we have the content in the proper format, let's save the wallet file. 1. From the vault page for go-on-oc-vault in the OCI console navigate to the Secrets tab and select Create Secret. 1. Enter the name for the secret: autonomousDB-cwallet-sso. 1. Select the master encryption key and paste the base64 content with database wallet details into the Secret Contents. >Note: Make sure to keep the type template set on Base64. 1. Select Create Secret. Retrieve the wallet file The contents of this secret can be retrieved from the Go code in the same way as before. However, this time the application needs to write a local cwallet.sso file with the contents from the secret: go decodedSecretContents, _ := b64.StdEncoding.DecodeString(*contentDetails.Content) _ = os.WriteFile(\"./cwallet.sso\", decodedSecretContents, 0644) This allows the application, using only the OCID values for the two secrets (the database connection details and the database wallet file), to initiate communications with the database it needs to use in its current environment. Just as we promised, nothing hard coded, nothing exposed! Connecting a Go application to an Oracle Database using Secrets from Vault The code for a simple Oracle Database client can be found in the application Directory (applications/safe-database-client) of this article's source-code repository. If you've been following along with the series, the client code is very similar to what we discussed in article four's applications/go-orcl-db. The main difference here is that this application contains neither a wallet file nor any database connection details. And at this point, you can probably see where this is all going. What the database client does require are references to two OCI Secrets. The first OCID refers to the secret with connection details (in JSON string format) while the second OCID refers to the secret that contains the base64-encoded representation of the cwallet.sso file. Provide the OCIDs In order to run the oracle-database-client-app.go application, you'll need to provide the values for those two OCID references we just mentioned. These references are contained in the variables: autonomousDatabaseConnectDetailsSecretOCID (connection details) and autonomousDatabaseCwalletSsoSecretOCID (wallet file). go const ( autonomousDatabaseConnectDetailsSecretOCID = \"ocid1.vaultsecret.oc1.iad.amaaaaaa6sde7caabn37hbdsu7dczk6wpxvr7euq7j5fmti2zkjcpwzlmowq\" autonomousDatabaseCwalletSsoSecretOCID = \"ocid1.vaultsecret.oc1.iad.amaaaaaa6sde7caazzhfhfsy2v6tqpr3velezxm4r7ld5alifmggjv3le2cq\" ) With these values, the application will be able to retrieve the two secrets from the vault. Once retrieved, the wallet file cwallet.sso is written to the local directory, and the other is used to construct an instance of type DatabaseConnectDetails that contains login details for the database (username, password, host, port, and service). At this point, the application has all the information it needs to establish a connection to the database. This connection process is the same as the one used by godror-based-oracle-database-client.go. Run the database client You can run the application on the command line with: console go run *.go The client will connect to the Autonomous Database and perform some small SQL feats. The nice thing about this example is that the application doesn't require any information whatsoever about the database it's going to interact with. > NOTE: In order to run an application on OCI that needs to read secrets from an OCI Vault, you'll need to make sure that the following policy applies to the host that runs the code (using either Resource Principal or Instance Principal authentication): > > console > allow dynamic-group my-secret-group to read secret-family in compartment go-on=oci where target.secret.name = 'my-secret' > From Streaming Messages to new Database Records So far, we've been working in separate components to achieve what we needed. But, we're really at the point where we can pull everything together into a single application that doesn't require any configuration (except for some OCID values for secrets in the OCI vault). So what should this application be able to do? It will need to subscribe to a Stream in OCI, poll and consume messages, and create records in an Autonomous Database on OCI for every message received from the Stream. Such an application can be visualized in the following image: Code repository All of the code you'll need for this application resides in the directory, applications/from-stream-to-database. The files should be to familiar to you by now: * consumer.go retrieves the secret with details for subscribing to the stream. * oracle-database-client-app.go does the same thing for the database connection details. This file also exposes the function PersistPerson that can be invoked with an instance of type Person and subsequently turned into a new database record. Remember to make same changes in oracle-database-client-app as in the previous section. You'll need add the OCID values for both the database connection details and the database wallet secrets. Creating a secret in the OCI Vault Next, you'll need to create a secret in the OCI vault. The secret will contain the stream details in a JSON string similar to the following: json { \"streamMessagesEndpoint\" : \"https://cell-1.streaming.us-ashburn-1.oci.oraclecloud.com\" , \"streamOCID\" : \"ocid1.stream.oc1.iad.amaaaaaa6sde7caa56brreqvzptc37wytom7pjk7vx3qaflagk2t3syvk67q\" } >NOTE: Make sure to set the values that apply to your environment. 1. Name the secret - For the purposes of this example, you can call the secret stream-connection-details. 1. Use the OCID for this new secret to set the value for const streamDetailsSecretOCID - You will need to set the value in the file consumer.go, located in applications/from-stream-to-database/consumer.go. The main function in this file subscribes to the stream and starts a series of iterations in which it polls the stream for new messages. Whenever a message is consumed, this function holds details for a person in a valid JSON message. This message is decoded into a Person instance and then subsequently passed on to PersistPerson. 1. Publish messages to the stream in the OCI Console 1. Navigate to the page for the stream in the console. 2. Select Produce Test Message and enter a valid JSON message with Person details that looks similar to the following: json { \"name\" : \"John Doe\", \"age\" : 34, \"comment\" : \"Nice chap, good looking; not too bright\" } 3. Select Produce to publish the message on the topic and make it available for processing by the consumer. There's no real need to worry, so feel free to publish the message multiple times. It won't result in multiple database records since the name is used as identifier. The only difference you're likely to see will be in the logging of the application. Of course, if you make changes in the name between the messages, no matter how small, you'll wind up with a lot of additional records in the database. >NOTE: The code is not very robust. It will likely choke on messages with a different format, so make sure you stay consistent with what's been presented. Deploy Person Producer on OKE So far, we've discussed several runtime platforms for our Go applications on OCI. However, there's still one we've yet to talk about: Kubernetes. Or, more specifically, Oracle Container Engine for Kubernetes (OKE). Over the course of the series, we've taken our executable (compiled from the Go source code), deployed it on a compute instance, and then converted the source code into a serverless function. In order to begin our exploration of Kubernetes, we'll containerize a standalone application and deploy it on an OKE cluster instance in this final section of the article. There'll be a lot of ground to cover, so let's review the steps ahead of us: * build and run the enhanced Person Producer application locally * create a lightweight container image for the Person Producer application * run a local container to prove that the image is complete and correct * push the container image to the OCI Container Image Registry * (optional) in order to ascertain that the images were all pushed correctly to the image registry, pull the container image from the registry and run it in one of the following: the Cloud Shell, on the go-app-vm compute instance, or in some other environment * create an OKE cluster instance (using the quick start wizard and consisting of a single node) * run the Person Producer application on the OKE cluster instance (manually deploying from *kubectl and passing the OCID of the publication stream as an environment variable) * create an OCI DevOps deployment pipeline to publish the Person Producer container image to the OKE cluster instance; run the pipeline * create an OCI DevOps build pipeline to build the Person Producer from the source code repository, publish the image to the container image registry, and trigger the deployment pipeline When it all comes together, the end result can be visualized as: Build and Run the Person Producer Application locally The Go application that publishes person messages to the Stream on OCID can be found in directory, applications/person-message-producer, in the source code repository for this article series. The code for the message producer is in file, person-producer.go. Set up the message producer application The code needs to have an environment variable set that indicates which secret in the OCI vault contains the stream details. If you recall, that secret was defined earlier in this article as stream-connection-details. 1. Set the STREAMDETAILSSECRET_OCID: console export STREAMDETAILSSECRET_OCID=\"ocid1.vaultsecret.oc1.iad.amaaaaaa6sde7caa6m5tuweeu3lbz22lf37y2dsbdojnhz2owmgvqgwwnvka\" 1. Replace the value with the OCID of the secret in your environment. >NOTE: You just used this OCID in the previous section to set the value for const streamDetailsSecretOCID in the file consumer.go. 1. Verify that there's an OCI config file In order to run the application locally, there needs to be an OCI configuration file. By default, this config file is assumed to be called config and located in $HOME/.oci. However, you're free to use a different file name or even a different location. All you need to do is specify the fully-qualified path to the configuration file using the environment variable OCICONFIGFILE. As noted above, the default value of OCICONFIGFILE is $HOME/.oci/config. Run the message producer application With the reference to the OCI configuration file in place, we're ready to run the application and start producing random person messages to the OCI Stream (go-on-oci-stream). 1. To start the application from the command line using, run: console export INSTANCEPRINCIPALAUTHENTICATION=\"NO\" go run person-producer.go On the command line this looks like: 1. To check the messages that are published to the stream in the console: Build a lightweight Container Image for the Application In may still seem far off, but the ultimate goal is to run the message producer as a containerized application on an OKE cluster. An essential step to getting there is creating a container image for the application. The Go application is already built into a binary executable, so the resulting container image is just a single binary executable file. Well, almost. Ideally, we want the container image to be small. So, for example, something akin to the lean Alpine Linux base image. We also need to make that the right ca-certificates are installed in the image, otherwise the requests to the HTTPS API endpoints will fail. Lastly, the Go application needs to be compiled for the proper runtime environment. In this case, the specific build command we need to create the smallest possible executable able to run inside the Alpine container is: console CGO_ENABLED=0 GOOS=linux go build -o person-producer -ldflags=\"-s -w\" -tags=containers >NOTE: The result of this command is a standalone binary executable file of about 6MB. Set up the container image The container image is built from a Dockerfile called DockerfileAlpine and is located in the same directory as person-producer.go. It has the following content: docker FROM alpine:latest WORKDIR /app copy the OCI Configuration file and the associated private key file - note: only for testing the container image locally; remove these lines before building the image that is pushed to the container image registry COPY config ./ COPY ociapikey.pem ./ add ca-certificates to allow signed communications RUN apk --no-cache add ca-certificates copy the application's binary executable COPY person-producer ./ CMD [\"./person-producer\"] What this code accomplishes: * The container image is created from the latest Alpine image. * A directory called /app is created and used for copying various files to. * The files that are copied to /app are: * config (the OCI Configuration file) * ociapikey.pem (the private key file referenced from the OCI Configuration file) * person-producer (the binary executable produced using go build). * The RUN apk line adds the required certificates to the image. * CMD [\"./person-producer\"] makes the application start up when a container based on this image starts running Build the container image 1. Copy the file $HOME/.oci/config and the referenced \\.pem file, ociapikey.pem, to the current directory. 1. Update the config file by setting the key_file parameter to: text keyfile=/app/ociapi_key.pem >NOTE: Normally, adding the OCI config and private key file should never be done in a container image that leaves your machine. We're only doing so now to test the image locally. Once we're certain everything is working properly, we'll rebuild the image without these files and only then push it to the container image registry. 1. Build the container image locally using this command: console docker build -t person-producer:1.0.0 -f DockerfileAlpine . >NOTE: The first time through, building the container will take a while since the base image needs to be loaded. In subsequent iterations, it should only take 15 seconds on average. 1. To check the success of the build process, inspect the command line output using docker images | grep person-producer and then run a container based on the image: console docker run --name person-messenger -e OCICONFIGFILE=/app/config -e INSTANCEPRINCIPALAUTHENTICATION=NO -e STREAMDETAILSSECRETOCID=$STREAMDETAILSSECRETOCID person-producer:1.0.0 There will be some positive output from the container: console docker run -e OCICONFIGFILE=/app/config -e INSTANCEPRINCIPALAUTHENTICATION=NO -e STREAMDETAILSSECRETOCID=$STREAMDETAILSSECRETOCID person-producer:1.0.0 Welcome from Container - About to publish some person records to the stream OCICONFIGFILE (only relevant when not doing instance principal authentication): /app/config { RawResponse={200 OK 200 HTTP/1.1 1 1 map[Access-Control-Allow-Credentials:[true] Access-Control-Allow-Methods:[POST,PUT,GET,HEAD,DELETE,OPTIONS] Access-Control-Allow-Origin:[*] Access-Control-Expose-Headers:[access-control-allow-credentials,access-control-allow-methods,access-control-allow-origin,connection,content-length,content-type,opc-client-info,opc-request-id,retry-after] Connection:[keep-alive] Content-Length:[134] Content-Type:[application/json] Opc-Request-Id:[873706f4a .... And if you check in the OCI console for the Stream details, you'll find new Person messages published by the application running in the container. This is just what we're looking for and gives us confidence in the container image. Prepare the container image for non-local deployment We're almost ready to move outside the local environment, but we're not quite there yet. We still need to remove the secrets references in the image. 1. Open the file DockerfileAlpine. 1. Either comment out or remove lines 7 and 8 that copy the OCI Config and private key file. 1. Rebuild the container image: console docker build -t person-producer:1.0.1 -f DockerfileAlpine . The console should echo something similar to the following: console The 1.0.1 image does not have the private parts that we do not want to ship. And now, we're all set! Push the Container Image to the OCI Container Image Registry Now that we have a container image free of secrets, it will need to be pushed to the OCI Container Image Registry before it can be deployed to an OKE Cluster. We'll go through the steps in the current section. Before we dive into the specifics, lets give some context about what we'll be doing in the Container Image Registry. First some definitions. A repository inside the container registry is a collection point for versions of a container image. So far, fairly straightforward. Such a repository is created when an image is pushed, based on the name of the image. The name given to the repository is typically drawn from: ///:. The prefix and image-name together become the name of the repository. It's important to note here that this name is not tied to a compartment. However, in order to organize the Container Image Registry in a way similar to how you've organized compartments, you may consider explicitly creating the repository in a specific compartment. This can be done through the OCI Console, the OCI CLI, or even through an API or Terraform operation. By establishing the repository beforehand, you can create it in the context of a specific compartment. The name of the resulting compartment (or even the names of levels of nested compartments) becomes part of the prefix of the container images. Example: Let's create a repository in the OCI Container Image Registry in the context of compartment go-on-oci and call the repository person-producer. When the container image is pushed as //go-on-oci/person-producer:, this image will be stored in the repository. >NOTE: If you don't create the repository before pushing the image, a repository is created automatically in the context of the root compartment and has the name, go-on-oci/person-producer. Everything will still work fine, but the repository structure is just a little less nice. 1. On the command line of your development environment, docker login to .ocir.io, using your specific region key. You may recall, we did this in the third article in the series. You'll need the login name and the authentication token. console docker login iad.ocir.io 1. Tag the locally-built image for use in the OCI Container Image Registry by running the command: console docker tag person-producer:1.0.1 ///person-producer:1.0.1 Example: console docker tag person-producer:1.0.1 iad.ocir.io/idtwlqf2hanz/go-on-oci/person-producer:1.0.1 1. With the login done and this tag in place, the container image can be pushed. console docker push ///person-producer:1.0.1 Example: console docker push iad.ocir.io/idtwlqf2hanz/go-on-oci/person-producer:1.0.1 1. To check the success of the push, inspect the OCI Container Image Registry through the OCI console. (Optional) Run a Container from the Image in Cloud Shell There's an easy way to verify the existence of the container image. Open the Cloud Shell from the OCI Console and run: console docker run -e INSTANCEPRINCIPALAUTHENTICATION=NO iad.ocir.io/idtwlqf2hanz/go-on-oci/person-producer:1.0.1 The will be download the image. If you run the image, it'll quickly exit because no OCI configuration file can be found. But, this shouldn't be too surprising. However, it does show us that the image was pushed successfully to the image registry. Create an OKE Cluster Instance Creating an Kubernetes cluster on OCI may sound like a daunting task, but it's actually really simple. OCI provides a convenient wizard that will handle the entire process for you with minimal input. Through the wizard, you can specify the number of nodes (either Compute Instances or VMs) in the cluster and what the shape of these VMs should be. You're not locked into this configuration, so you can easily change this later on. After you've made that decision (or accepted the default settings), you can leave it to the wizard to take care of the network configuration, the compute instances, the node pool formed by the instances, and the Kubernetes cluster that sits on top of the node pool itself. It might take a few minutes, but once the OKE instance is available, you're all ready to accept deployments. OCI wizard 1. Enter ok in the search box and then select the link Kubernetes Clusters (OKE) | Containers & Artifacts. 1. Select Create cluster. A popup window appears with two radio buttons: Quick create and Custom create. 1. Accept the first one (Quick create) and then select Launch workflow. >You're now in the wizard. 1. [Wizard] 1. Select Public Endpoint and then Private Workers. 1. You can accept the shape VM.Standard.E3.Flex or pick a different one. 2. You can accept three as the number of nodes, but just one node is enough for our purposes. 3. Select Next. 1. This next part of the wizard provides an overview of what the wizard will do on our behalf. Here, you'll be able to perform a final inspection. If you like the proposed action, select Creat cluster. The wizard will provide all required resources and perform any actions that are needed, including network configuration, creating a new compute instance, and provisioning the OKE master. There's also a progress page available so you can keep track of the status of the actions. Inspecting the Kubernetes cluster Once all actions are complete, you can inspect the details of the new Kubernetes cluster, the node pool, the network resources, and the compute instance. The following images shows the details for the node pool and the node(s) in the pool. >NOTE: The dynamic group go-on-oci-instances that was defined in the first article in this series was created in such a way that it includes all compute instances in the compartment. > > This means that if you're still working in that same compartment and the OKE cluster is now also running in that compartment, the node in the OKE instance is a member of the dynamic group and inherits all OCI IAM permissions granted to the group. Containers running in Pods on the OKE instance and scheduled on this node also inherit these privileges when they use instance principal authentication. Connect to the OKE Cluster instance Interaction with a Kubernetes Cluster is typically done through the kubectl command line tool. We can use either the kubectl installation included in Cloud Shell, or a local installation of kubectl. However, before we can use kubectl to access a cluster, we have to specify the cluster on which to perform operations by setting up the cluster’s kubeconfig file. >NOTE: At the time of writing, to access a cluster using kubectl in Cloud Shell, the Kubernetes API endpoint must have a public IP address. Fortunately for us, our cluster alreadt does, thanks to the wizard! Set up 1. In the Cluster Details window, select Access Cluster to display the Access Your Cluster dialog box. 1. Select Cloud Shell Access and then select Copy to copy the shell statement under (2) to the clipboard. 1. Select Launch Cloud Shell to display the Cloud Shell window if it's not already open. 1. Paste the shell statement into the Cloud Shell and then press enter to execute the statement. Config file Once this is complete, the config file in $HOME/.kube of your cloudshell environment is written (or updated if it already existed) with details for the new OKE cluster. Note that the OCI CLI understands the string PUBLIC_ENDPOINT and writes the proper Public IP Address for the cluster into the configuration file. As long as we don't change the name or location of the config file from its default, we don't have to explicitly set the KUBECONFIG environment variable to refer to it, and we can now run kubectl to interact with our new cluster. Interacting with the new cluster Local interaction: In addition to cluster access in Cloud Shell, we probably want to have local interaction from our laptop. It should have kubectl running already, as well as OCI CLI with configuration settings that provide access to the OCI tenancy. 1. In the OKE Cluster Details window in OCI Console, select Access Cluster again to display the Access Your Cluster dialog box. 1. This time, select Local Access. 1. Copy the first statement (for public IP endpoint) in step 2 and execute it locally. At this point, kubectl should be set up and ready for action. The file $HOME/.kube/config is either created or extended with an extra context. The current-context is set to this newly created one for the OKE cluster. Test interaction: For a quick test, let's lists the node(s) that make up the OKE cluster. If you were to run the following command locally, you'll now get the same result as in cloud shell. console kubectl get nodes Kubernetes Dashboard The Kubernetes Dashboard is a well-known UI for monitoring and managing the Kubernetes Cluster. While many other tools are available to make life easier, the dashboard remains as a comfortable and familiar fallback option. Perhaps not unsurprisingly, the Dashboard itself is a Kubernetes application, since it's just a collection of resources that needs to be created on the cluster. This means that there's a predefined collection of yaml files available that can be applied with a single command: console kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.0/aio/deploy/recommended.yaml Example - check pods: 1. Let's take a look at the pods in the newly-created namespace kubernetes-dashboard: console kubectl -n kubernetes-dashboard get pods 2. Access the dashboard in the browser, using: http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/workloads?namespace=default 3. When the GUI appears in the browser, you'll be prompted for either a kubeconfig file or a token. Let's go with the token: 1. Create the service account and clusterbinding - Use the YAML file, oke-admin-service-account.yaml, to create both the service account and the clusterrolebinding in the cluster (as described in the [OCI Documentation for Accessing an OKE Cluster Instance]): console kubectl apply -f oke-admin-service-account.yaml 2. Generate token - Run the following command: console kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep oke-admin | awk '{print $1}') This will result in a token be written to the console output: console Name: oke-admin-token-4dg6k Namespace: kube-system Labels: Annotations: kubernetes.io/service-account.name: oke-admin kubernetes.io/service-account.uid: 1e68cd7b-d362-4de4-8ce8-293ff9afecb4 Type: kubernetes.io/service-account-token Data ==== token: eyJhbGciOiJSUzI1NiIsImtpZCI6InZ1WFprUDRfTjRLYWpNNTVmYmFIMkNpY24yamxhN21IUmdpdjAyZzlVUVkifQ.ZWFjY291bnQvc2VjcmV0L...Weiw-zWpS1bG9GWlSVQxTQa1fVGOqWEQRtKHtA0YEVn1bise-RINwKmDkNQ7nbA1jmZycUOOgAePtIFqbjtvlY6QkA2lAkCQz0-YshIc01XCT7yieymzbyxhBWedbr9bIlHZW3qRb0IeEs_taAghkLHf23S71GxIbF558UUpE9w ca.crt: 1285 bytes namespace: 11 bytes >NOTE: The token is the section of output that starts with eyJh and ends with pE9w. 3. Apply token - Copy and paste this value into the window that prompts you for a token. Run the Person Producer Application on the OKE Cluster instance The file, person-producer-deployment.yaml, contains the specification for the Kubernetes resources that we'd like to deploy on the cluster. Ultimately, we're looking to deploy with a Pod based on the container image (.ocir.io//go-on-oci/person-producer:1.0.1) that we pushed a little while back. Here are the contents of the deployment specification: yaml apiVersion: apps/v1 kind: Deployment metadata: name: personproducer-deployment spec: selector: matchLabels: app: personproducer replicas: 1 template: metadata: labels: app: personproducer spec: containers: - name: personproducer # enter the path to your image, be sure to include the correct region prefix image: iad.ocir.io/idtwlqf2hanz/go-on-oci/person-producer:1.0.1 env: - name: STREAMDETAILSSECRET_OCID value: \"ocid1.vaultsecret.oc1.iad.amaaaaaa6sde7caa6m5tuweeu3lbz22lf37y2dsbdojnhz2owmgvqgwwnvka\" imagePullSecrets: # enter the name of the secret you created - name: ocirsecret 1. Update values in person-producer-deployment.yaml: * STREAMDETAILSSECRET_OCID - Change the value of this env key to the value that applies in your environment. * image - Update the value to correspond with the image you pushed to the OCI Container Image Registry. 1. Authentication - The OKE Cluster can't just start pulling container images from the registry. It needs to use proper authentication details to log in to the registry. These details are represented in this deployment specification by the secret ocirsecret. However, the one wrinkle is that this secret doesn't exist yet on the OKE cluster. Create ocirsecret - Run the following command in kubectl: console kubectl create secret docker-registry --docker-server=.ocir.io --docker-username='/' --docker-password='' --docker-email='' This creates a secret in Kubernetes that can be used when needed to pull container images. The command should resolve to something similar to: console kubectl create secret docker-registry ocirsecret --docker-server=iad.ocir.io --docker-username='idtwlqf2hanz/jellema@chimney.nl' --docker-password='y&aya1PCjJFW8xk1.7o' --docker-email='jellema@chimney.nl' 1. Apply the deployment specification - After creating the secret, you can apply the deployment specification using kubectl, creating the deployment and the pod and pulling the image to run the container: console kubectl apply -f person-producer-deployment.yaml The somewhat underwhelming output of this command is just: console deployment.apps/personproducer-deployment created Here a little means a lot. And this is telling us that the Pod is starting, the container image is being pulled, and the application will soon run and start publishing messages. 1. Check on the deployment - There are multiple ways you can do this: * You can check the logs from the Pod in the Kubernetes Dashboard. * You can also verify in the OCI Console if the expected messages arrive on the stream. * You can even run application applications/from-stream-to-database and see if new database records get created. > NOTE: Logging may reveal problems with an application that has permissions for OCI resources. If the resource issue involves either retrieving the secret with stream details or producing messages to the stream itself, you may need to check if policy statements have been defined for the dynamic group go-on-oci-instances that allow all compute instances. At the compartment level, if the errors relate to the ability of the OKE cluster node (and by extension any application running in a container on this VM) to read secrets and work with streams, you'll need to do the following: console allow dynamic-group go-on-oci-instances to read secret-family in compartment go-on-oci allow dynamic-group go-on-oci-instances to manage streams in compartment go-on-oci Create and run an OCI DevOps Deployment Pipeline to publish the Application to the OKE cluster A Deployment Pipeline will take a Kubernetes manifest that manipulates K8S resources (e.g., creating a Pod or a Deployment) and apply it to a designated OKE cluster. The manifest contains a reference to a container image, most likely located in the OCI Container Image Registry (although it's also possible for to be in a public or other private registry). The manifest can also contain value placeholders of the form, ${parameterName}. If present, these are replaced by the actual values of the parameters when the pipeline is executed. How would this work in practice? The pipeline can be used to deploy specific, varying versions of images and also to set values in the manifest that translate to environment variables inside the container. For example, the value for STREAMDETAILSSECRET_OCID is provided by a pipeline parameter. This value is replaced before the deployment starts in the manifest and that value is available as environment variable to be read by the person-producer application. Create an Inline Artifact with Kubernetes Manifest for a Deployment 1. Create an artifact in the DevOps Project,person-producer-deployment-yaml. * Type: Kubernetes manifest * Artifact source: inline. 1. Paste in this Kubernetes manifest definition for creating a deployment based on a container image: yaml apiVersion: apps/v1 kind: Deployment metadata: name: personproducer-deployment spec: selector: matchLabels: app: personproducer replicas: 1 template: metadata: labels: app: personproducer spec: containers: - name: personproducer # enter the path to your image, be sure to include the correct region prefix image: iad.ocir.io/idtwlqf2hanz/go-on-oci/person-producer:${imageVersion} env: - name: STREAMDETAILSSECRET_OCID value: ${STREAMDETAILSSECRET_OCID} imagePullSecrets: # enter the name of the secret you created - name: ocirsecret * NOTE: You have to replace part of the image name to make it fit your environment: iad.ocir.io/idtwlqf2hanz/go-on-oci/ => // * Also keep in mind that the manifest contains two placeholders: \\${STREAMDETAILSSECRET_OCID} and \\${imageVersion}. These are replaced when the artifact is used by values from parameters set at deployment time. Make sure to set the field Replace parameters used in this artifact to Yes, substitute placeholders. 1. Select Save. Define a DevOps Environment for the OKE Cluster 1. Define an environment in the DevOps Project: * Type: Oracle Kubernetes Engine * Name: go-on-oci-oke 2. Select Next. 3. Select the cluster instance that you just created and then select Create environment. Define a Deployment Pipeline 1. Create a new Deployment Pipeline in the DevOps Project: 1. Give it a name: deploy-person-producer-to-oke. 1. Add a stage of type Apply manifest to your Kubernetes cluster. 1. Select Next to define details. 1. Assign a name: apply-person-producer-deployment 1. Select the environment go-on-oci-oke. 1. Select artifact person-producer-deployment-yaml as an artifact to apply. Note: Multiple Kubernetes manifest artifacts can be included in this stage. 1. You can accept or change the Kubernetes namespace. Select Save to create the stage. 2. Define two Pipeline Parameters: * STREAMDETAILSSECRET_OCID - Its default value should be the OCID of the secret that contains the Stream details (message endpoint and stream OCID). * imageVersion - Its default value can be 1.0.1. This parameter determines the version of the container image person-producer to get from the image registry and deploy onto the OKE cluster. Run the Deployment Pipeline to Deploy Person-Producer on OKE Cluster Finally, the pipeline is ready to run! You can kick it off and have the Deployment created on Kubernetes. If you wish, you can provide overrides for the parameters, but more than likely, these aren't needed. As you might imagine, running the pipeline is a little more impressive if you first removed the Pod and Deployment for person-producer from the OKE cluster. It should automatically be built by the pipeline, so if it makes a reappearance, you know it was the pipeline that did it. The output from running the pipeline is definitely reassuring, green checkmarks all the way: Check status: At this point, there are a lot of different ways you can inspect the health of the system. You can: * check on the Stream to find new messages being published, * check in the Kubernetes Dashboard on the state of the Deployment and the Pod, or * check in kubectl with kubectl get pods to find a very recently kicked-off Pod for personproducer-deployment. Create an OCI DevOps Build Pipeline As a final step, we'll create a new DevOps Build Pipeline that has more of a real-world application. This pipeline will build the Container Image for the person-producer application from the Go sources in the code repository, publish the container image to the registry, and then trigger the deployment pipeline. This means that whenever we commit a code change, we can run a pipeline that takes care of the end-to-end redeployment on the Kubernetes cluster for the changed application code. The are quite a few steps, but they's are all fairly straightforward: 1. Create a DevOps artifact for the container image, person-producer. 1. Create the build pipeline. 1. Add the parameter imageVersion* - this determines the version for the container image to produce. 1. Add a managed build stage - this builds and tags the container image locally (on the build server). 1. Add a stage to publish the freshly built image to the container image registry. 1. Add a final stage to trigger the deployment pipeline, deploy-person-producer-to-oke. Now, let's test it out! Make a change to the application source code and commit the change to the code repository to trigger the build pipeline. This might take a few minutes, but afterwards, inspect the Stream for messages that carry the change made in the source code of the application. Define DevOps Artifact 1. Add a new artifact in the DevOps project: * Name: PersonProducerImage * Type: Docker image * Artifact repository path: iad.ocir.io/idtwlqf2hanz/go-on-oci/person-producer:${imageVersion} 1. Replace the region key, namespace, and repository prefix with values that conform to your environment. 1. Set Replace parameters used in this artifact to Yes, substitute placeholders. Create Build Pipeline and Add Parameter imageVersion 1. Create a new Build Pipeline in the DevOps Project: * Any name will do, but a good example might be: build-person-message-producer. * Optionally provide a description. 1. Define a parameter for the build pipeline: imageVersion: 1.0.5 (for this example) This parameter contains a string that defines the version label assigned to the container image produced by the build pipeline. The value of that parameter is also made available to the deployment pipeline to determine the container image to fetch from the container image registry and use in the Pod created on the OKE cluster instance. Add Managed Build Stage - to build the Container Image 1. Add a stage to the build pipeline. This stage has a lot to do. It will first gather the input sources from the source repository and build them into a linted, tested, vetted, and formatted Go application executable. From there, it will be converted into a lean container image. Name: build-container-image-for-go-application Type: Managed Build build spec file path: /applications/person-message-producer/build_spec.yaml This refers to the file buildspec.yaml in the root of the application directory for application person-message-producer. This file contains the build steps required to take the Go sources, perform the automated build steps on them, and finally build a lightweight, stand-alone executable suitable for the Alpine Linux container image. The last step performs the docker build into a container image, with the local tag fresh-person-producer-container-image. The output from the stage of type DOCKERIMAGE is labeled person-producer-image and references the fresh-person-producer-container-image. This output is used in the next stage that publishes the container image to the container image registry. 2. Select the source code repository go-on-oci-sources and set the Build source name to go-on-oci-sources. 3. Select Save to complete the stage definition. Add a Stage to Publish the freshly built image to the container image registry 1. Add a second stage in the build pipeline. This stage will take care of taking the output of the managed build stage and pushing the container image to the registry, with the right version tag based on the build pipeline's parameter. * Name: push-container-image * Type: Deliver artifacts 1. Select the artifact to publish. * PersonProducerImage: the Docker image * Path: iad.ocir.io/idtwlqf2hanz/go-on-oci/person-producer:${imageVersion} This was defined as an artifact in the DevOps Project earlier. 1. Select Save to complete the stage definition. Add a stage to Trigger the Deployment Pipeline 1. Add a stage to the build pipeline. * Name: trigger-person-producer-deployment-pipeline * Type: Trigger deployment * Deployment pipeline to be triggered by this stage: deploy-person-producer-to-oke. 1. Select Save to complete the stage definition. Run Build Pipeline, Create Container Image, and Run Deployment Pipeline We're done! The Build Pipeline is complete. You can run it, set the value for parameter imageVersion and wait for the source code to be converted into a running Pod on OKE. In a typical workflow, you'll make a change to the application source code and commit that change to the code repository to trigger the build pipeline. After a few minutes, when the two pipelines are done, the changed application will be running. For reference, the end-to-end flow (triggering the build pipeline to completion of the deployment on the OKE instance) for the example presented in this article should take around three minutes. Some things to keep an eye out for during the execution of the build pipelie: * Midway through, the console should look similar to: * The output from the deployed application can be seen as the Pod logs in the Kubernetes Dashboard: * You can also check out the logs from the Pod using kubectl: console kubectl logs -l app=personproducer Conclusion This article demonstrated how a Go application, through the Go SDK for OCI, can easily publish messages to OCI Streaming topics as well as consume such messages. This application is varsatile and can run either on OCI or outside of it. It's credentials and other secrets are ideally managed using OCI Key Vault, something which the article also introduced and showed how it can be used from Go applications. Finally, a third type of application runtime platform was introduced, the managed OCI Kubernetes Engine (OKE). This application exists next to the VM and the serverless function. Once a Build Pipeline has created a container image for the application, OCI DevOps Deployment Pipelines can deploy our Go applications to OKE in an automated fashion. The five articles that make up the series \"Way to Go on OCI\" have provided Go developers (Gophers) with a overview of how OCI provides a valuable platform both for engineering and running Go based applications as well as for leveraging relevant platform services from Go applications. The series demonstrates automated build and deployment of Go applications as stand alone executables on Compute Instances, as serverless functions and as containers on a Kubernetes cluster. Throughout the articles, introductions are given of these OCI services used from Go applications: Object Storage, Functions, API Gateway, Autonomous Database, Streaming and Key Vault. Additional, platform services used for engineering and operations were discussed, including DevOps Build and Deployment Pipelines, Code Repositories, Artifact Registry, Container Image Registry, IAM and Logging. Resources Source code repository for the sources discussed in this article series Oracle Cloud Infrastructure Blog - Automating a pod identity solution with Oracle Container Engine for Kubernetes (OKE) - by Ed Shnekendorf - this article describes the use of instance principals for the nodes (OCI Compute Instances) in the OKE cluster to provide permissions for the Pods running on the node to access OCI services Oracle Functions - Connecting To An ATP Database With A Wallet Stored As Secrets - article by Todd Sharp on retrieving Oracle Wallet from OCI Vault from Functions Protect Your Sensitive Data With Secrets In The Oracle Cloud - article by Todd Sharp on use of OCI Vault OCI Streaming — create producer/consumer use case using ATP, OIC and Kafka connect Oracle Cloud Streaming Service – Scalable, Reliable, Kafka-like Event service on OCI SDK for Go Streaming Quickstart Getting started (again) with Kubernetes on Oracle Cloud Compiling Your Go Application for Containers By Lucas Jellema","categories": ["clouddev","cloudapps"],
        "tags": ["open-source","devops","get-started","automation","iac"],
        "url": "/tutorials/way-to-go-on-oci/way-to-go-on-oci-article5",
        "teaser": ""
      },{
        "title": "field-notes",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/field-notes",
        "teaser": ""
      },{
        "title": "always-free",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/always-free",
        "teaser": ""
      },{
        "title": "analytics",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/analytics",
        "teaser": ""
      },{
        "title": "apache",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/apache",
        "teaser": ""
      },{
        "title": "apex",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/apex",
        "teaser": ""
      },{
        "title": "api",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/api",
        "teaser": ""
      },{
        "title": "automation",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/automation",
        "teaser": ""
      },{
        "title": "data-visualization",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/data-visualization",
        "teaser": ""
      },{
        "title": "get-started",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/get-started",
        "teaser": ""
      },{
        "title": "hardware",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/hardware",
        "teaser": ""
      },{
        "title": "iac",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/iac",
        "teaser": ""
      },{
        "title": "iot",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/iot",
        "teaser": ""
      },{
        "title": "jupyter",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/jupyter",
        "teaser": ""
      },{
        "title": "machine-learning",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/machine-learning",
        "teaser": ""
      },{
        "title": "oci",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/oci",
        "teaser": ""
      },{
        "title": "oke",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/oke",
        "teaser": ""
      },{
        "title": "open-source",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/open-source",
        "teaser": ""
      },{
        "title": "orm",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/orm",
        "teaser": ""
      },{
        "title": "pytorch",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/pytorch",
        "teaser": ""
      },{
        "title": "rpi",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/rpi",
        "teaser": ""
      },{
        "title": "serverless",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/serverless",
        "teaser": ""
      },{
        "title": "streaming",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/streaming",
        "teaser": ""
      },{
        "title": "ubuntu",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/ubuntu",
        "teaser": ""
      },{
        "title": "verrazzano",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/verrazzano",
        "teaser": ""
      },{
        "title": "gaming",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/gaming",
        "teaser": ""
      },{
        "title": "docker",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/docker",
        "teaser": ""
      },{
        "title": "postman",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/postman",
        "teaser": ""
      },{
        "title": "scripting",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/scripting",
        "teaser": ""
      },{
        "title": "etl",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/etl",
        "teaser": ""
      },{
        "title": "architect",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/architect",
        "teaser": ""
      },{
        "title": "arvr",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/arvr",
        "teaser": ""
      },{
        "title": "back-end",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/back-end",
        "teaser": ""
      },{
        "title": "community",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/community",
        "teaser": ""
      },{
        "title": "data-management",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/data-management",
        "teaser": ""
      },{
        "title": "data-science",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/data-science",
        "teaser": ""
      },{
        "title": "dbre",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/dbre",
        "teaser": ""
      },{
        "title": "devops",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/devops",
        "teaser": ""
      },{
        "title": "front-end",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/front-end",
        "teaser": ""
      },{
        "title": "full-stack",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/full-stack",
        "teaser": ""
      },{
        "title": "game-dev",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/game-dev",
        "teaser": ""
      },{
        "title": "robotics",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/robotics",
        "teaser": ""
      },{
        "title": "secops",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/secops",
        "teaser": ""
      },{
        "title": "go",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/go",
        "teaser": ""
      },{
        "title": "java",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/java",
        "teaser": ""
      },{
        "title": "javascript",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/javascript",
        "teaser": ""
      },{
        "title": "mysql",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/mysql",
        "teaser": ""
      },{
        "title": "nodejs",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/nodejs",
        "teaser": ""
      },{
        "title": "php",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/php",
        "teaser": ""
      },{
        "title": "python",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/python",
        "teaser": ""
      },{
        "title": "ruby",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/ruby",
        "teaser": ""
      },{
        "title": "terraform",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/terraform",
        "teaser": ""
      },{
        "title": "typescript",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/typescript",
        "teaser": ""
      },{
        "title": "ansible",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/ansible",
        "teaser": ""
      },{
        "title": "dotnet",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/dotnet",
        "teaser": ""
      },{
        "title": "express",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/express",
        "teaser": ""
      },{
        "title": "flask",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/flask",
        "teaser": ""
      },{
        "title": "graalvm",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/graalvm",
        "teaser": ""
      },{
        "title": "kubernetes",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/kubernetes",
        "teaser": ""
      },{
        "title": "micronaut",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/micronaut",
        "teaser": ""
      },{
        "title": "pitorch",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/pitorch",
        "teaser": ""
      },{
        "title": "spark",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/spark",
        "teaser": ""
      },{
        "title": "spring",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/spring",
        "teaser": ""
      },{
        "title": "tensorflow",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/topics/tensorflow",
        "teaser": ""
      },{
        "title": "Oracle Developer Tutorials",
        "excerpt":" By ","categories": null,
        "tags": null,
        "url": "/index.html",
        "teaser": ""
      }]
