<div class="initial-content">
  <div id="main" role="main">

  <article class="page has-sidebar" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Deploying A Multi-Cluster Verrazzano On Oracle Container Engine for Kubernetes (OKE) Part 1">
    
    <meta itemprop="datePublished" content="2021-12-03T09:11:00+00:00">
    


    <header>
      <h1 id="page-title" class="page__title" itemprop="headline">Deploying A Multi-Cluster Verrazzano On Oracle Container Engine for Kubernetes (OKE) Part 1
</h1>
      

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2021-12-03T09:11:00+00:00">December 3, 2021</time>
      </span>
    

    

    
  </p>


    </header>

    <section class="page__content" itemprop="text">
      

        <picture class="alignright">
                <source srcset="assets/verrazzano-logo.png 1x" />
                <img loading="lazy" width="400" height="400" src="assets/verrazzano-logo.png" data-original="assets/verrazzano-logo.png" alt="Verrazzano Logo" title="Verrazzano Logo" />
            </picture>

<p>In the <a href="1-deploying-verrazzano-on-oke">previous article</a>, we were introduced to Verrazzano and took it for a quick spin on an Oracle Container Engine for Kubernetes (OKE). As promised, in this article, we’re going to deploy a multi-cluster Verrazzano on OKE. And just to make things a little more interesting, we’ll also do that using different Oracle Cloud Infrastructure (OCI) regions.</p>

<p>But first, we’ll make a small digression into WebLogic and Kubernetes to set the stage for how we’ll be handling this in each of the next two tutorials.</p>

<p>Key topics covered in this tutorial:</p>

<ul>
  <li>An introduction to WebLogic and Kubernetes</li>
  <li>A discussion about infrastructure</li>
  <li>Creating Verrazzano clusters</li>
</ul>

<p>For additional information, see:</p>

<ul>
  <li><a href="https://docs.oracle.com/iaas/Content/GSG/Tasks/signingup.htm">Signing Up for Oracle Cloud Infrastructure</a></li>
  <li><a href="https://docs.oracle.com/en-us/iaas/Content/API/SDKDocs/terraformgettingstarted.htm">Getting started with Terraform</a></li>
</ul>

<h2 id="getting-started">Getting started</h2>

<h3 id="from-weblogic-to-kubernetes-to-verrazzano">From WebLogic to Kubernetes to Verrazzano</h3>

<p>To better frame our understanding of what Kubernetes is and how it fits in with Verrazzano, let’s take a step back and apply some simple analogies to its components.</p>

<p>A good touchstone for us is the WebLogic space, and we can draw from a lot of familiar concepts to help with our understanding here.</p>

<figure class="aligncenter">
              <picture>
                  <source srcset="assets/SumRnmK8ZrOCVzWwaAETC3Q.png 1x" />
                  <img loading="lazy" width="1200" height="401" src="assets/SumRnmK8ZrOCVzWwaAETC3Q.png" data-original="assets/SumRnmK8ZrOCVzWwaAETC3Q.png" title="WebLogic and Kubernetes analogy" alt="WebLogic and Kubernetes analogy" />
              </picture>
              <figcaption>WebLogic and Kubernetes analogy</figcaption>
            </figure>

<p>In WebLogic, a cluster consists of an Admin Server and a group of Managed Servers. In this set up, the Admin Server handles the administration, deployment, and other less silky but nevertheless important tasks, while Managed Servers are utilized for deploying and running the applications, as well as responding to requests.  This allows you to run your applications either across your entire cluster or on specific Managed Servers.</p>

<blockquote class="alert">
  <p><strong>NOTE:</strong> You could always run your applications on the single Admin Server (in a way that’s somewhat equivalent to <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">taints and tolerations</a> of the master nodes), but it’s not recommended.</p>
</blockquote>

<p>Under this set up, if your application is deployed to the cluster and a Managed Server in the cluster fails (JVM, host, reboot, etc.), other Managed Servers in the cluster will automatically handle the job.</p>

<p>But, what if the Managed Server where your singleton service is running fails? WebLogic has you covered with Automatic Service Migration (ASM). For a more detailed read on ASM, check out the <a href="https://www.oracle.com/technetwork/middleware/weblogic/weblogic-automatic-service-migratio-133948.pdf">WebLogic ASM guide</a>.</p>

<p>Now that we have a better sense of the basic cluster infrastructure, let’s start connecting this back to Kubernetes. What’s the best equivalent in Kubernetes? Essentially, ASM is a bit like a <code class="language-plaintext highlighter-rouge">ReplicaSet</code>. Initially, applications on Kubernetes were stateless until the addition of StatefulSets, but now you can also run stateful applications across the entire cluster.</p>

<h3 id="geographically-distributed-clusters">Geographically distributed clusters</h3>

<p>What if, for the purpose of high availability, you needed to run your Kubernetes applications in geographically distributed clusters. You could try your luck with <a href="https://github.com/kubernetes-sigs/kubefed">kubefed</a>, although it’s currently still in beta and has admittedly been experiencing some growing pains. Or, you could try deploying the same applications to different clusters, implement a kind of global health check, and then use an <a href="https://docs.oracle.com/en-us/iaas/Content/TrafficManagement/Concepts/overview.htm">intelligent load balancer</a> to switch the traffic from one cluster to another. All these approaches are valid, but still fairly limited, error-prone, and risky.</p>

<p>Enter Verrazzano multi-clustering.</p>

<p>How did Verrazzano make our lives a whole lot better? It took the concept of Admin and Managed Servers in WebLogic and applied it to Kubernetes clusters:</p>

<figure class="aligncenter">
              <picture>
                  <source srcset="assets/5i_215fK15AiaYSz.png 1x" />
                  <img loading="lazy" width="535" height="413" src="assets/5i_215fK15AiaYSz.png" data-original="assets/5i_215fK15AiaYSz.png" title="Verrazzano multi-cluster" alt="Verrazzano multi-cluster" />
              </picture>
              <figcaption>Verrazzano multi-cluster</figcaption>
            </figure>

<p>Where you previously had a single Admin Server for WebLogic, you now have a single Admin <em>cluster</em> based on Kubernetes for Verrazzano. And where your applications were deployed on managed servers, your Verrazzano workloads are deployed on managed Kubernetes clusters, possibly closer to your users.</p>

<h3 id="infrastructure-planning">Infrastructure Planning</h3>

<p>In order to achieve this, Verrazzano-managed clusters (i.e., Kubernetes clusters administered and managed by the Verrazzano container platform) need to be able to communicate with the Verrazzano Admin cluster and vice-versa. In WebLogic, the Managed Servers would usually be part of the same network (unless you were using <a href="https://docs.oracle.com/en/middleware/standalone/weblogic-server/14.1.1.0/wlcag/active-active-stretch-cluster-active-passive-database-tier.html#GUID-66D13F44-200A-45AB-9676-2BF18610554D">stretch clusters</a>) and this administration would be fairly straightforward.</p>

<p>Our ultimate goal though, is to deploy the different Verrazzano clusters in different cloud regions on OCI, so we need to start thinking about our plan for networking and security.</p>

<blockquote class="alert">
  <p><strong>NOTE:</strong>  You can also use Verrazzano to manage clusters deployed in other clouds or on-premises, but the networking and security configurations would vary (VPN/FastConnect etc).</p>
</blockquote>

<p>Below is a map of OCI regions to help us pick a set of regions:</p>

<figure class="aligncenter">
              <picture>
                  <source srcset="assets/syLSX57E1bT7_EzYZU_qLGg.png 1x" />
                  <img loading="lazy" width="1200" height="508" src="assets/syLSX57E1bT7_EzYZU_qLGg.png" data-original="assets/syLSX57E1bT7_EzYZU_qLGg.png" title="Map of OCI regions" alt="Map of OCI regions" />
              </picture>
              <figcaption>Map of OCI regions</figcaption>
            </figure>

<p>We’ll use our newly-minted Singapore region for the Admin cluster and then Mumbai, Tokyo, and Sydney as managed clusters in a star architecture:</p>

<figure class="aligncenter">
              <picture>
                  <source srcset="assets/rnXSnetqM6oAOJk6bfkyQ.png 1x" />
                  <img loading="lazy" width="671" height="549" src="assets/rnXSnetqM6oAOJk6bfkyQ.png" data-original="assets/rnXSnetqM6oAOJk6bfkyQ.png" title="Verrazzano Clusters spread across OCI Asia Pacific regions" alt="Verrazzano Clusters spread across OCI Asia Pacific regions" />
              </picture>
              <figcaption>Verrazzano Clusters spread across OCI Asia Pacific regions</figcaption>
            </figure>

<h3 id="networking-infrastructure">Networking Infrastructure</h3>

<figure class="aligncenter">
              <picture>
                  <source srcset="assets/bF77x66gHN42zsW_2_9Dxw.png 1x" />
                  <img loading="lazy" width="695" height="554" src="assets/bF77x66gHN42zsW_2_9Dxw.png" data-original="assets/bF77x66gHN42zsW_2_9Dxw.png" title="Remote Peering with different regions" alt="Remote Peering with different regions" />
              </picture>
              <figcaption>Remote Peering with different regions</figcaption>
            </figure>

<p>We need the clusters to communicate securely using the OCI Backbone, so this means we need to set up DRGs in each region, attach them to their respective VCNs, and then use remote peering. Since the VCNs and the clusters will eventually be connected, we also need to ensure that their respective IP address ranges (VCN, pod, and service) do not overlap.</p>

<h2 id="creating-the-verrazzano-clusters">Creating the Verrazzano clusters</h2>

<p>We’re going to the use <a href="https://github.com/oracle-terraform-modules/terraform-oci-oke">terraform-oci-oke module</a> to create our clusters. We could create them individually by cloning the module 4 times and then changing the region parameters, but there’s now a much simpler way to do it. You’ll be pleased to know that one of the things we improved in the 4.0 release of the module is <em>reusability</em>. We’ll take advantage of this!</p>

<h3 id="create-a-new-terraform-project">Create a new Terraform project</h3>

<ol>
  <li>
    <p><strong>Create project</strong>
Let’s create a new Terraform project and define our variables as follows:</p>

    <pre><code class="language-JSON">   # Copyright 2017, 2021 Oracle Corporation and/or affiliates.  All rights reserved.
   # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl

   # OCI Provider parameters
   variable "api_fingerprint" {
     default     = ""
     description = "Fingerprint of the API private key to use with OCI API."
     type        = string
   }

   variable "api_private_key_path" {
     default     = ""
     description = "The path to the OCI API private key."
     type        = string
   }

   variable "verrazzano_regions" {
     # List of regions: https://docs.cloud.oracle.com/iaas/Content/General/Concepts/regions.htm#ServiceAvailabilityAcrossRegions
     description = "A map Verrazzano regions."
     type        = map(string)
   }

   variable "tenancy_id" {
     description = "The tenancy id of the OCI Cloud Account in which to create the resources."
     type        = string
   }

   variable "user_id" {
     description = "The id of the user that terraform will use to create the resources."
     type        = string
     default     = ""
   }

   # General OCI parameters
   variable "compartment_id" {
     description = "The compartment id where to create all resources."
     type        = string
   }

   variable "label_prefix" {
     default     = "none"
     description = "A string that will be prepended to all resources."
     type        = string
   }
</code></pre>
  </li>
  <li>
    <p><strong>Define regions</strong>
In <code class="language-plaintext highlighter-rouge">terraform.tfvars</code>, along with the identity parameters, let’s define our regions:</p>

    <pre><code class="language-JSON">   verrazzano_regions = {  
     home  = "your-tenancy-home-region" #replace with your tenancy's home region  
     admin = "ap-singapore-1"  
     syd   = "ap-sydney-1"  
     mum   = "ap-mumbai-1"  
     tok   = "ap-tokyo-1"  
   }
</code></pre>
  </li>
  <li>
    <p><strong>Define providers</strong>
In <code class="language-plaintext highlighter-rouge">provider.tf</code>, let’s define the providers for the different regions using aliases:</p>

    <pre><code class="language-JSON">   provider "oci" {
     fingerprint      = var.api_fingerprint
     private_key_path = var.api_private_key_path
     region           = var.verrazzano_regions["admin"]
     tenancy_ocid     = var.tenancy_id
     user_ocid        = var.user_id
     alias            = "admin"
   }

   provider "oci" {
     fingerprint      = var.api_fingerprint
     private_key_path = var.api_private_key_path
     region           = var.verrazzano_regions["home"]
     tenancy_ocid     = var.tenancy_id
     user_ocid        = var.user_id
     alias            = "home"
   }

   provider "oci" {
     fingerprint      = var.api_fingerprint
     private_key_path = var.api_private_key_path
     region           = var.verrazzano_regions["syd"]
     tenancy_ocid     = var.tenancy_id
     user_ocid        = var.user_id
     alias            = "syd"
   }

   provider "oci" {
     fingerprint      = var.api_fingerprint
     private_key_path = var.api_private_key_path
     region           = var.verrazzano_regions["mum"]
     tenancy_ocid     = var.tenancy_id
     user_ocid        = var.user_id
     alias            = "mum"
   }

   provider "oci" {
     fingerprint      = var.api_fingerprint
     private_key_path = var.api_private_key_path
     region           = var.verrazzano_regions["tok"]
     tenancy_ocid     = var.tenancy_id
     user_ocid        = var.user_id
     alias            = "tok"
   }
</code></pre>
  </li>
  <li>
    <p><strong>Create clusters</strong>
In <code class="language-plaintext highlighter-rouge">main.tf</code>, we’ll create the different clusters (note that some of the parameters here have the same values, and you could use the default ones, but it’s definitely possible to configure these by regions too):</p>

    <pre><code class="language-JSON">   module "vadmin" {
     source  = "oracle-terraform-modules/oke/oci"
     version = "4.0.1"

     home_region = var.verrazzano_regions["home"]
     region      = var.verrazzano_regions["admin"]

     tenancy_id = var.tenancy_id

     # general oci parameters
     compartment_id = var.compartment_id
     label_prefix   = "v8o"

     # ssh keys
     ssh_private_key_path = "~/.ssh/id_rsa"
     ssh_public_key_path  = "~/.ssh/id_rsa.pub"

     # networking
     create_drg                   = true
     internet_gateway_route_rules = []
     nat_gateway_route_rules = [
       {
         destination       = "10.1.0.0/16"
         destination_type  = "CIDR_BLOCK"
         network_entity_id = "drg"
         description       = "To Sydney"
       },
       {
         destination       = "10.2.0.0/16"
         destination_type  = "CIDR_BLOCK"
         network_entity_id = "drg"
         description       = "To Mumbai"
       },
       {
         destination       = "10.3.0.0/16"
         destination_type  = "CIDR_BLOCK"
         network_entity_id = "drg"
         description       = "To Tokyo"
       },
     ]

     vcn_cidrs     = ["10.0.0.0/16"]
     vcn_dns_label = "admin"
     vcn_name      = "admin"

     # bastion host
     create_bastion_host = true
     upgrade_bastion     = false

     # operator host
     create_operator                    = true
     enable_operator_instance_principal = true
     upgrade_operator                   = false

     # oke cluster options
     cluster_name                = "admin"
     control_plane_type          = "private"
     control_plane_allowed_cidrs = ["0.0.0.0/0"]
     kubernetes_version          = "v1.20.11"
     pods_cidr                   = "10.244.0.0/16"
     services_cidr               = "10.96.0.0/16"

     # node pools
     node_pools = {
       np1 = { shape = "VM.Standard.E4.Flex", ocpus = 2, memory = 32, node_pool_size = 2, boot_volume_size = 150, label = { app = "frontend", pool = "np1" } }
     }
     node_pool_name_prefix = "np-admin"

     # oke load balancers
     load_balancers          = "both"
     preferred_load_balancer = "public"
     public_lb_allowed_cidrs = ["0.0.0.0/0"]
     public_lb_allowed_ports = [80, 443]

     # freeform_tags
     freeform_tags = {
       vcn = {
         verrazzano = "admin"
       }
       bastion = {
         access     = "public",
         role       = "bastion",
         security   = "high"
         verrazzano = "admin"
       }
       operator = {
         access     = "restricted",
         role       = "operator",
         security   = "high"
         verrazzano = "admin"
       }
     }

     providers = {
       oci      = oci.admin
       oci.home = oci.home
     }
   }

   module "vsyd" {
     source  = "oracle-terraform-modules/oke/oci"
     version = "4.0.1"

     home_region = var.verrazzano_regions["home"]
     region      = var.verrazzano_regions["syd"]

     tenancy_id = var.tenancy_id

     # general oci parameters
     compartment_id = var.compartment_id
     label_prefix   = "v8o"

     # ssh keys
     ssh_private_key_path = "~/.ssh/id_rsa"
     ssh_public_key_path  = "~/.ssh/id_rsa.pub"

     # networking
     create_drg                   = true
     internet_gateway_route_rules = []
     nat_gateway_route_rules = [
       {
         destination       = "10.0.0.0/16"
         destination_type  = "CIDR_BLOCK"
         network_entity_id = "drg"
         description       = "To Admin"
       }
     ]

     vcn_cidrs     = ["10.1.0.0/16"]
     vcn_dns_label = "syd"
     vcn_name      = "syd"

     # bastion host
     create_bastion_host = false
     upgrade_bastion     = false

     # operator host
     create_operator                    = false
     enable_operator_instance_principal = true
     upgrade_operator                   = false

     # oke cluster options
     cluster_name                = "syd"
     control_plane_type          = "private"
     control_plane_allowed_cidrs = ["0.0.0.0/0"]
     kubernetes_version          = "v1.20.11"
     pods_cidr                   = "10.245.0.0/16"
     services_cidr               = "10.97.0.0/16"

     # node pools
     node_pools = {
       np1 = { shape = "VM.Standard.E4.Flex", ocpus = 2, memory = 32, node_pool_size = 2, boot_volume_size = 150 }
     }

     # oke load balancers
     load_balancers          = "both"
     preferred_load_balancer = "public"
     public_lb_allowed_cidrs = ["0.0.0.0/0"]
     public_lb_allowed_ports = [80, 443]

     # freeform_tags
     freeform_tags = {
       vcn = {
         verrazzano = "syd"
       }
       bastion = {
         access     = "public",
         role       = "bastion",
         security   = "high"
         verrazzano = "syd"
       }
       operator = {
         access     = "restricted",
         role       = "operator",
         security   = "high"
         verrazzano = "syd"
       }
     }

     providers = {
       oci      = oci.syd
       oci.home = oci.home
     }
   }

   module "vmum" {
     source  = "oracle-terraform-modules/oke/oci"
     version = "4.0.1"

     home_region = var.verrazzano_regions["home"]
     region      = var.verrazzano_regions["mum"]

     tenancy_id = var.tenancy_id

     # general oci parameters
     compartment_id = var.compartment_id
     label_prefix   = "v8o"

     # ssh keys
     ssh_private_key_path = "~/.ssh/id_rsa"
     ssh_public_key_path  = "~/.ssh/id_rsa.pub"

     # networking
     create_drg                   = true
     internet_gateway_route_rules = []
     nat_gateway_route_rules = [
       {
         destination       = "10.0.0.0/16"
         destination_type  = "CIDR_BLOCK"
         network_entity_id = "drg"
         description       = "To Admin"
       }
     ]

     vcn_cidrs     = ["10.2.0.0/16"]
     vcn_dns_label = "mum"
     vcn_name      = "mum"

     # bastion host
     create_bastion_host = false
     upgrade_bastion     = false

     # operator host
     create_operator                    = false
     enable_operator_instance_principal = true
     upgrade_operator                   = false

     # oke cluster options
     cluster_name                = "mum"
     control_plane_type          = "private"
     control_plane_allowed_cidrs = ["0.0.0.0/0"]
     kubernetes_version          = "v1.20.11"
     pods_cidr                   = "10.246.0.0/16"
     services_cidr               = "10.98.0.0/16"

     # node pools
     node_pools = {
       np1 = { shape = "VM.Standard.E4.Flex", ocpus = 2, memory = 32, node_pool_size = 2, boot_volume_size = 150 }
     }

     # oke load balancers
     load_balancers          = "both"
     preferred_load_balancer = "public"
     public_lb_allowed_cidrs = ["0.0.0.0/0"]
     public_lb_allowed_ports = [80, 443]

     # freeform_tags
     freeform_tags = {
       vcn = {
         verrazzano = "mum"
       }
       bastion = {
         access     = "public",
         role       = "bastion",
         security   = "high"
         verrazzano = "mum"
       }
       operator = {
         access     = "restricted",
         role       = "operator",
         security   = "high"
         verrazzano = "mum"
       }
     }

     providers = {
       oci      = oci.mum
       oci.home = oci.home
     }
   }

   module "vtok" {
     source  = "oracle-terraform-modules/oke/oci"
     version = "4.0.1"

     home_region = var.verrazzano_regions["home"]
     region      = var.verrazzano_regions["tok"]

     tenancy_id = var.tenancy_id

     # general oci parameters
     compartment_id = var.compartment_id
     label_prefix   = "v8o"

     # ssh keys
     ssh_private_key_path = "~/.ssh/id_rsa"
     ssh_public_key_path  = "~/.ssh/id_rsa.pub"

     # networking
     create_drg                   = true
     internet_gateway_route_rules = []
     nat_gateway_route_rules = [
       {
         destination       = "10.0.0.0/16"
         destination_type  = "CIDR_BLOCK"
         network_entity_id = "drg"
         description       = "To Admin"
       }
     ]

     vcn_cidrs     = ["10.3.0.0/16"]
     vcn_dns_label = "tok"
     vcn_name      = "tok"

     # bastion host
     create_bastion_host = false
     upgrade_bastion     = false

     # operator host
     create_operator                    = false
     enable_operator_instance_principal = true
     upgrade_operator                   = false

     # oke cluster options
     cluster_name                = "tok"
     control_plane_type          = "private"
     control_plane_allowed_cidrs = ["0.0.0.0/0"]
     kubernetes_version          = "v1.20.11"
     pods_cidr                   = "10.247.0.0/16"
     services_cidr               = "10.99.0.0/16"

     # node pools
     node_pools = {
       np1 = { shape = "VM.Standard.E4.Flex", ocpus = 2, memory = 32, node_pool_size = 2, boot_volume_size = 150 }
     }

     # oke load balancers
     load_balancers          = "both"
     preferred_load_balancer = "public"
     public_lb_allowed_cidrs = ["0.0.0.0/0"]
     public_lb_allowed_ports = [80, 443]

     # freeform_tags
     freeform_tags = {
       vcn = {
         verrazzano = "tok"
       }
       bastion = {
         access     = "public",
         role       = "bastion",
         security   = "high"
         verrazzano = "tok"
       }
       operator = {
         access     = "restricted",
         role       = "operator",
         security   = "high"
         verrazzano = "tok"
       }
     }

     providers = {
       oci      = oci.tok
       oci.home = oci.home
     }
   }
</code></pre>
  </li>
  <li>
    <p><strong>Display operators</strong>
For convenience, let’s display the operator host in each region:</p>

    <pre><code class="language-JSON">   output "ssh_to_admin_operator" {
     description = "convenient command to ssh to the Admin operator host"
     value       = module.vadmin.ssh_to_operator
   }

   output "ssh_to_au_operator" {
     description = "convenient command to ssh to the Sydney operator host"
     value       = module.vsyd.ssh_to_operator
   }

   output "ssh_to_in_operator" {
     description = "convenient command to ssh to the Mumbai operator host"
     value       = module.vmum.ssh_to_operator
   }

   output "ssh_to_jp_operator" {
     description = "convenient command to ssh to the Tokyo operator host"
     value       = module.vtok.ssh_to_operator
   }
</code></pre>
  </li>
</ol>

<h3 id="create-oke-clusters">Create OKE clusters</h3>

<ol>
  <li>
    <p>Run <code class="language-plaintext highlighter-rouge">terraform init</code> and then <code class="language-plaintext highlighter-rouge">terraform plan</code>.
The output of <em>plan</em> should indicate the following:</p>

    <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">   Plan: 292 to add, 0 to change, 0 to destroy.Changes to Outputs:  
   + ssh_to_admin_operator = (known after apply)  
   + ssh_to_au_operator    = "ssh -i ~/.ssh/id_rsa -J opc@ opc@"  
   + ssh_to_in_operator    = "ssh -i ~/.ssh/id_rsa -J opc@ opc@"  
   + ssh_to_jp_operator    = "ssh -i ~/.ssh/id_rsa -J opc@ opc@"
</span></code></pre></div>    </div>
  </li>
  <li>
    <p>Run <code class="language-plaintext highlighter-rouge">terraform apply</code> and then <code class="language-plaintext highlighter-rouge">terraform relax</code>.
Shortly after, you should see the following:</p>

    <figure class="aligncenter">
           <picture>
               <source srcset="assets/Eeiu_1isUl47wVZAAd1eoA.png 1x" />
               <img loading="lazy" width="975" height="88" src="assets/Eeiu_1isUl47wVZAAd1eoA.png" data-original="assets/Eeiu_1isUl47wVZAAd1eoA.png" title="Simultaneous creation of 4 OKE clusters in different regions" alt="Simultaneous creation of 4 OKE clusters in different regions" />
           </picture>
           <figcaption>Simultaneous creation of 4 OKE clusters in different regions</figcaption>
         </figure>

    <p>This means that our four OKE Clusters are being simultaneously created in 4 different OCI regions. In about 15 minutes, you’ll have all four clusters created:</p>

    <picture class="aligncenter">
             <source srcset="assets/1vdOhprGm48QCzDjYBkUQQ.png 1x" />
             <img loading="lazy" width="855" height="183" src="assets/1vdOhprGm48QCzDjYBkUQQ.png" data-original="assets/1vdOhprGm48QCzDjYBkUQQ.png" alt="Showing outputs after creating clusters" title="Showing outputs after creating clusters" />
         </picture>

    <blockquote class="notice">
      <p><strong>NOTE:</strong> The ssh commands to the various operator hosts will also be printed.</p>
    </blockquote>
  </li>
</ol>

<h3 id="establish-connections">Establish connections</h3>

<ol>
  <li><strong>Create remote peering connections</strong>
    <ol>
      <li>Navigate to the DRGs in <strong><em>each</em></strong> <strong>managed cluster’s</strong> region (Mumbai, Tokyo, and Sydney).</li>
      <li>Select <strong>Remote Peering Attachment</strong> and create a <strong>Remote Peering Connection</strong> (call it <em>rpc_to_admin</em>).</li>
      <li>
        <p>In the <strong>Admin region</strong> (<em>Singapore</em> in our selected region), create 3 <strong>Remote Peering Connections</strong>:</p>

        <figure class="aligncenter">
        <picture>
            <source srcset="assets/sub6pYSaRFEQumzQLQdDxwg.png 1x" />
            <img loading="lazy" width="1200" height="339" src="assets/sub6pYSaRFEQumzQLQdDxwg.png" data-original="assets/sub6pYSaRFEQumzQLQdDxwg.png" title="3 RPCs in the Admin region" alt="3 RPCs in the Admin region" />
        </picture>
        <figcaption>3 RPCs in the Admin region</figcaption>
      </figure>

        <p>Now, we need to peer them.</p>
      </li>
    </ol>
  </li>
  <li><strong>Peer the connections</strong>
    <ol>
      <li>Select <strong>rpc_to_syd</strong>.</li>
      <li>Open a new tab in your browser and access the <em>OCI Console</em>.</li>
      <li>Change the region to <em>Sydney</em>.</li>
      <li>Navigate to the DRG and then to <strong>rpc_to_syd</strong> page.</li>
      <li>
        <p>Copy the RPC’s OCID (not the DRG), switch to the <strong>Admin</strong> tab, and then select <strong>Establish Connection</strong>:</p>

        <figure class="aligncenter">
        <picture>
            <source srcset="assets/2g_Oih2j9NBRy_cUoUW9Eg.png 1x" />
            <img loading="lazy" width="619" height="216" src="assets/2g_Oih2j9NBRy_cUoUW9Eg.png" data-original="assets/2g_Oih2j9NBRy_cUoUW9Eg.png" title="Establishing RPC" alt="Establishing RPC" />
        </picture>
        <figcaption>Establishing RPC</figcaption>
      </figure>
      </li>
    </ol>
  </li>
  <li><strong>Establish connection</strong>
    <ol>
      <li>Once you’ve provided the <em>RPC ID</em> and the <em>region</em> as above, select <strong>Establish Connection</strong> to perform the peering.</li>
      <li>
        <p>Repeat the same procedure for the Tokyo and Mumbai regions until all the managed cluster regions are peered with the Admin region. When the peering is performed and completed, you will see its status will change to “Pending” and eventually “Peered”:</p>

        <figure class="aligncenter">
       <picture>
           <source srcset="assets/DX7Nv3MRwczRYbmc5aXzlA.png 1x" />
           <img loading="lazy" width="1200" height="405" src="assets/DX7Nv3MRwczRYbmc5aXzlA.png" data-original="assets/DX7Nv3MRwczRYbmc5aXzlA.png" title="RPCs in Pending state" alt="RPCs in Pending state" />
       </picture>
       <figcaption>RPCs in Pending state</figcaption>
     </figure>

        <figure class="aligncenter">
       <picture>
           <source srcset="assets/TSN09Afrj1KwHSEjFeYQ.png 1x" />
           <img loading="lazy" width="1110" height="471" src="assets/TSN09Afrj1KwHSEjFeYQ.png" data-original="assets/TSN09Afrj1KwHSEjFeYQ.png" title="RPCs in Peered state" alt="RPCs in Peered state" />
       </picture>
       <figcaption>RPCs in Peered state</figcaption>
     </figure>
      </li>
    </ol>
  </li>
</ol>

<h3 id="configure">Configure</h3>

<p>At this point, our VCNs are peered but there are three more things we need to do:</p>

<ol>
  <li>Configure routing tables so that the Verrazzano managed clusters can communicate to the Admin cluster and vice-versa</li>
  <li>Configure NSGs for the control plane CIDRs to accept requests from Admin VCN</li>
  <li>Merge the <code class="language-plaintext highlighter-rouge">kubeconfigs</code></li>
</ol>

<p>In the first step, we’re asked to configure the routing table. Previously, you would have had to manually configure rules, but now, they’re automagically done for you! “How,” you ask? Well, one of the <a href="https://github.com/oracle-terraform-modules/terraform-oci-oke/releases">features</a> we’ve added is the <a href="https://github.com/oracle-terraform-modules/terraform-oci-oke/issues/279">ability to configure and update routing tables</a>.</p>

<p>Let’s explore this in a little more detail. In your <code class="language-plaintext highlighter-rouge">main.tf</code>, take a look in the the Admin cluster module. Usually, the <code class="language-plaintext highlighter-rouge">nat_gateway_route_rules</code>  parameter is an empty list:</p>

<pre><code class="language-JSON">nat_gateway_route_rules = []
</code></pre>

<p>However, in our Admin module definition, notice that we had already changed this to:</p>

<pre><code class="language-JSON">nat_gateway_route_rules = [
{
  destination       = "10.1.0.0/16"
  destination_type  = "CIDR_BLOCK"
  network_entity_id = "drg"       
  description       = "To Sydney"
},
{
  destination       = "10.2.0.0/16"
  destination_type  = "CIDR_BLOCK"
  network_entity_id = "drg"       
  description       = "To Mumbai"
},
{
  destination       = "10.3.0.0/16"
  destination_type  = "CIDR_BLOCK"
  network_entity_id = "drg"       
  description       = "To Tokyo"
},
]
</code></pre>

<p>Similarly, in the managed cluster definitions, we had also set the routing rules to reach the Admin cluster in Singapore:</p>

<pre><code class="language-JSON">nat_gateway_route_rules = [  
  {  
    destination       = "10.0.0.0/16"  
    destination_type  = "CIDR_BLOCK"  
    network_entity_id = "drg"  
    description       = "To Admin"  
  }  
]
</code></pre>

<blockquote class="notice">
  <p><strong>NOTE:</strong> You can always update these rules later.</p>
</blockquote>

<p><strong>Example - updating routing rules:</strong></p>

<p>Let’s say you add another managed region in Hyderabad (VCN CIDR: 10.4.0.0). There’s a couple of things you’ll need to do to have this region recognized.</p>

<ol>
  <li>
    <p><strong>Add new rule</strong>
In the routing rules for Admin, you’ll need to add an additional entry to route traffic to this new region:</p>

    <pre><code class="language-JSON">   nat_gateway_route_rules = [  
   {  
     destination       = "10.4.0.0/16"  
     destination_type  = "CIDR_BLOCK"  
     network_entity_id = "drg"  
     description       = "To Hyderabad"  
   }  
   ]
</code></pre>
  </li>
  <li>
    <p><strong>Update</strong>
After updating the custom rules, run <code class="language-plaintext highlighter-rouge">terraform apply</code> again and the routing rules in the Admin region will be updated.</p>
  </li>
  <li>
    <p><strong>Check rules</strong>
Navigate to the <strong>Network Visualizer</strong> page to check your connectivity and routing rules:</p>

    <figure class="aligncenter">
           <picture>
               <source srcset="assets/oS_bZ4lnAounnM_lgBGELg.png 1x" />
               <img loading="lazy" width="1200" height="390" src="assets/oS_bZ4lnAounnM_lgBGELg.png" data-original="assets/oS_bZ4lnAounnM_lgBGELg.png" title="Network connectivity across regions" alt="Network connectivity across regions" />
           </picture>
           <figcaption>Network connectivity across regions</figcaption>
         </figure>
  </li>
  <li>
    <p><strong>TCP requests</strong>
Next, in <em>each</em> region-managed VCN’s control plane NSG, add an ingress to accept TCP requests from source CIDR 10.0.0.0/16 (Admin) and destination port 6443. This is so that the Admin cluster can be able to communicate with the Managed Cluster’s control plane.</p>

    <figure class="aligncenter">
           <picture>
               <source srcset="assets/UbwGSoe2twNSHayNM4YfRg.png 1x" />
               <img loading="lazy" width="1200" height="359" src="assets/UbwGSoe2twNSHayNM4YfRg.png" data-original="assets/UbwGSoe2twNSHayNM4YfRg.png" title="Additional ingress security rule in each managed cluster's control plane NSG" alt="Additional ingress security rule in each managed cluster's control plane NSG" />
           </picture>
           <figcaption>Additional ingress security rule in each managed cluster's control plane NSG</figcaption>
         </figure>
  </li>
</ol>

<h2 id="operational-convenience">Operational Convenience</h2>

<p>For convenience, we’d like to be able to execute most of our operations from the Admin operator host. To do this, we first need to obtain the <code class="language-plaintext highlighter-rouge">kubeconfig</code> of each cluster and then merge them together on the Admin operator. At the moment, this step isn’t quite so convenient itself, since you’ll have to perform it manually, but we’re working to improve this in the future.</p>

<h3 id="obtain-kubeconfigs">Obtain <code class="language-plaintext highlighter-rouge">kubeconfigs</code></h3>

<ol>
  <li>Navigate to <em>each</em> managed cluster’s page and select <strong>Access cluster</strong>.</li>
  <li>
    <p>Copy the second command which allows you get the <code class="language-plaintext highlighter-rouge">kubeconfig</code> for that cluster:</p>

    <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">   oci ce cluster create-kubeconfig --cluster-id ocid1.cluster.... --file $</span>HOME/.kube/configsyd <span class="nt">--region</span> ap-sydney-1 <span class="nt">--token-version</span> 2.0.0  <span class="nt">--kube-endpoint</span> PRIVATE_ENDPOINT
<span class="go">
</span><span class="gp">   oci ce cluster create-kubeconfig --cluster-id ocid1.cluster.... --file $</span>HOME/.kube/configmum <span class="nt">--region</span> ap-mumbai-1 <span class="nt">--token-version</span> 2.0.0  <span class="nt">--kube-endpoint</span> PRIVATE_ENDPOINT
<span class="go">
</span><span class="gp">   oci ce cluster create-kubeconfig --cluster-id ocid1.cluster.... --file $</span>HOME/.kube/configtok <span class="nt">--region</span> ap-tokyo-1 <span class="nt">--token-version</span> 2.0.0  <span class="nt">--kube-endpoint</span> PRIVATE_ENDPOINT
</code></pre></div>    </div>

    <blockquote class="alert">
      <p><strong>NOTE:</strong> You also have to rename the file so it won’t overwrite the existing config for the Admin region. In our example above, that would be configsyd, configmum, and configtok.</p>
    </blockquote>
  </li>
  <li>
    <p>Run the commands to get the managed cluster’s respective <code class="language-plaintext highlighter-rouge">kubeconfigs</code>. You should have four <code class="language-plaintext highlighter-rouge">kubeconfigs</code>:</p>

    <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">   $</span><span class="w"> </span><span class="nb">ls</span> <span class="nt">-al</span> .kube  
<span class="go">   total 16  
   drwxrwxr-x. 2 opc opc   71 Nov 10 11:40 .  
   drwx------. 4 opc opc  159 Nov 10 11:15 ..  
   -rw--w----. 1 opc opc 2398 Nov 10 11:15 config  
   -rw-rw-r--. 1 opc opc 2364 Nov 10 11:40 configmum  
   -rw-rw-r--. 1 opc opc 2364 Nov 10 11:40 configsyd  
   -rw-rw-r--. 1 opc opc 2362 Nov 10 11:40 configtok
</span></code></pre></div>    </div>

    <p>We can check access to the clusters from the Admin operator host:</p>

    <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">   cd .kubefor cluster in config configsyd configmum configtok;</span><span class="w"> </span><span class="k">do</span>  
<span class="gp">     KUBECONFIG=$</span>CLUSTER kubectl get nodes  
<span class="go">   done
</span></code></pre></div>    </div>

    <p>This will return us the list of nodes in each cluster:</p>

    <figure class="aligncenter">
           <picture>
               <source srcset="assets/Wbt9jmrz8pJxxZliPssYBw.png 1x" />
               <img loading="lazy" width="818" height="310" src="assets/Wbt9jmrz8pJxxZliPssYBw.png" data-original="assets/Wbt9jmrz8pJxxZliPssYBw.png" title="List of nodes in each cluster" alt="List of nodes in each cluster" />
           </picture>
           <figcaption>List of nodes in each cluster</figcaption>
         </figure>
  </li>
</ol>

<h3 id="rename-cluster-content">Rename cluster content</h3>

<p>Again for convenience, another thing we’ll want to do is rename each cluster’s context. That way, we’ll know which region we’re dealing with.</p>

<p>In this exercise, we want 1 context to equate to a Verrazzano cluster.</p>

<ol>
  <li>
    <p>Let’s rename all the <code class="language-plaintext highlighter-rouge">kubeconfig</code> files first:</p>

    <ul>
      <li>config -&gt; admin</li>
      <li>configmum -&gt; mumbai</li>
      <li>configsyd -&gt; sydney</li>
      <li>configtok -&gt; tokyo</li>
    </ul>
  </li>
  <li>
    <p>Now, let’s rename their respective contexts:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="k">for </span>cluster <span class="k">in </span>admin sydney mumbai tokyo<span class="p">;</span> <span class="k">do  
     </span><span class="nv">current</span><span class="o">=</span><span class="si">$(</span><span class="nv">KUBECONFIG</span><span class="o">=</span><span class="nv">$cluster</span> kubectl config current-context<span class="si">)</span>  
     <span class="nv">KUBECONFIG</span><span class="o">=</span><span class="nv">$cluster</span> kubectl config rename-context <span class="nv">$current</span> <span class="nv">$cluster</span>  
   <span class="k">done</span>
</code></pre></div>    </div>
  </li>
</ol>

<h3 id="merge">Merge</h3>

<p>We’re now ready to merge!</p>

<p>To merge the files, run:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">KUBECONFIG</span><span class="o">=</span>./admin:./sydney:./mumbai:./tokyo kubectl config view <span class="nt">--flatten</span> <span class="o">&gt;</span> ./config
</code></pre></div></div>

<h4 id="list-contexts">List contexts</h4>

<p>To get a list of the contexts, run:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">kubectl config get-contexts
</span></code></pre></div></div>

<p>This will return the following:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">CURRENT   NAME     CLUSTER               AUTHINFO           NAMESPACE
*         admin    cluster-cillzxw34tq   user-cillzxw34tqmumbai
mumbai    cluster-cuvo2ifxe2a   user-cuvo2ifxe2asydney   
sydney    cluster-cmgb37morjq   user-cmgb37morjqtokyo    
tokyo     cluster-coxskjynjra   user-coxskjynjra
</span></code></pre></div></div>

<p>Now, this is all rather verbose. Fortunately, there’s a way to get nice, succinct output through <a href="https://github.com/ahmetb/kubectx">kubectx</a>. But generating clean output isn’t all this tool can do. It can also be used to rename the contexts as well. In the next section, we’ll explore this alternative a little further.</p>

<h4 id="install-and-run-kubectx">Install and run <code class="language-plaintext highlighter-rouge">kubectx</code></h4>

<p>First, let’s install <code class="language-plaintext highlighter-rouge">kubectx</code>:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">wget https://github.com/ahmetb/kubectx/releases/download/v0.9.4/kubectx
chmod +x kubectx  
sudo mv kubectx /usr/local/bin
</span></code></pre></div></div>

<p>Now, run <code class="language-plaintext highlighter-rouge">kubectx</code>:</p>

<figure class="aligncenter">
              <picture>
                  <source srcset="assets/DPsupx6c_ADhdwE0TwS-Zw.png 1x" />
                  <img loading="lazy" width="397" height="113" src="assets/DPsupx6c_ADhdwE0TwS-Zw.png" data-original="assets/DPsupx6c_ADhdwE0TwS-Zw.png" title="Using kubectx" alt="Using kubectx" />
              </picture>
              <figcaption>Using kubectx</figcaption>
            </figure>

<p>The current context (i.e., the current Verrazzano cluster), is highlighted in yellow. We can also easily change contexts in order to perform Verrazzano installations and other operations. For example:</p>

<figure class="aligncenter">
              <picture>
                  <source srcset="assets/nCSAOCAmUaD-AYTnUDhDpA.png 1x" />
                  <img loading="lazy" width="390" height="160" src="assets/nCSAOCAmUaD-AYTnUDhDpA.png" data-original="assets/nCSAOCAmUaD-AYTnUDhDpA.png" title="Changing context to Sydney" alt="Changing context to Sydney" />
              </picture>
              <figcaption>Changing context to Sydney</figcaption>
            </figure>

<h2 id="whats-next">What’s next</h2>

<p>We made it through a lot of material in this article. Let’s review everything we covered:</p>

<ul>
  <li>Learned how to set up OKE, networking connectivity, and routing.</li>
  <li>Discovered operational convenience to run multi-cluster Verrazzano in different regions.</li>
  <li>Got started with <code class="language-plaintext highlighter-rouge">kubectx</code></li>
</ul>

<p>With this, I would like to thank my colleague and friend Shaun Levey for his ever perceptive insights into the intricacies of OCI Networking.</p>

<p>Our exploration continues in <a href="3-deploy-multi-cluster-verrazzano-oke">Part 2</a>, where we’ll take a look at how to install Verrazzano in a multi-cluster configuration.</p>

<p>To explore more information about development with Oracle products:</p>

<ul>
  <li><a href="https://developer.oracle.com/">Oracle Developers Portal</a></li>
  <li><a href="https://www.oracle.com/cloud/">Oracle Cloud Infrastructure</a></li>
</ul>


          <div class="sidebar sticky">
    <!-- <p><strong>Tags:</strong> <span class="tags">

            
            <a class="animated-link tag" href="/topics/open-source">open-source</a>
            <a class="animated-link tag" href="/topics/oke">oke</a>
            <a class="animated-link tag" href="/topics/kubernetes">kubernetes</a>
            <a class="animated-link tag" href="/topics/verrazzano">verrazzano</a>
            <a class="animated-link tag" href="/topics/terraform">terraform</a>
            <a class="animated-link tag" href="/topics/devops">devops</a>
            </span>
    </p> -->
  


<div itemscope itemtype="https://schema.org/Person">

  

  <div class="author__content">
    
      <a href="https:/lmukadam.medium.com"><h3 class="author__name" itemprop="name">Ali Mukadam</h3></a>
    
    
      <div class="author__bio" itemprop="description">
        <p>Technical Director, Asia Pacific Center of Excellence.</p>

<p>For the past 16 years, Ali has held technical presales, architect and industry consulting roles in BEA Systems and Oracle across Asia Pacific, focusing on middleware and application development. Although he pretends to be Thor, his real areas of expertise are Application Development, Integration, SOA (Service Oriented Architecture) and BPM (Business Process Management). An early and worthy Docker and Kubernetes adopter, Ali also leads a few open source projects (namely <a href="https://github.com/oracle-terraform-modules/terraform-oci-oke">terraform-oci-oke</a>) aimed at facilitating the adoption of Kubernetes and other cloud native technologies on Oracle Cloud Infrastructure.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <ul class="author__urls social-icons">
      

      

      

      

      

      

      

      
        <li>
          <a href="https://www.linkedin.com/in/alimukadam" itemprop="sameAs" rel="nofollow noopener noreferrer">
            <i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">LinkedIn</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      
    </ul>
  </div>
</div>

  
  
  

  </div>


      </section>

      <footer class="page__meta">
        
        


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2021-12-03T09:11:00+00:00">December 3, 2021</time></p>


      </footer>
    </div>

  </article>
</div>

</div>

